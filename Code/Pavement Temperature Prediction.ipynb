{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Loaded\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xlwt \n",
    "from xlwt import Workbook \n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "print('Libraries Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities \n",
    "\n",
    "def read_file(path):\n",
    "    df= pd.read_excel(path)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "def create_dataset(X, y, time_steps, ts_range):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps - ts_range):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.values[(i + time_steps):(i + time_steps + ts_range),0])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def splitter(df,output,lag,duration,ts):\n",
    "    assert (0. <= ts <= 1.)\n",
    "    train_size = int(len(df) * ts)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df[train_size:]\n",
    "    print(train.shape, test.shape)\n",
    "    scaler,scaler_single = MinMaxScaler(feature_range=(0, 1)),MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaler.fit(train)\n",
    "    scaler_single.fit(train[output])\n",
    "\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train), columns=[df.columns])\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test), columns=[df.columns])\n",
    "\n",
    "    df_train = train_scaled.copy(deep=True)\n",
    "    df_test = test_scaled.copy(deep=True)\n",
    "\n",
    "    x_train,y_train = create_dataset(df_train,df_train[[output]],lag,duration)\n",
    "    x_test, y_test = create_dataset(df_test, df_test[[output]], lag, duration)\n",
    "\n",
    "    return x_train,x_test,y_train,y_test,scaler_single\n",
    "\n",
    "class attention(keras.layers.Layer):\n",
    "    '''\n",
    "    if return_sequences=True, it will give 3D vector and if false it will give 2D vector. It is same as LSTMs.\n",
    "\n",
    "    https://stackoverflow.com/questions/62948332/how-to-add-attention-layer-to-a-bi-lstm/62949137#62949137\n",
    "    the  following code is being copied from the above link.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, return_sequences=True, **kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention, self).__init__()\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10896, 7)\n",
      "   Year  Month  Day  Hour  Temp  Solar  Pavement\n",
      "0  2009     11    1     1   8.4    0.0  9.333333\n",
      "1  2009     11    1     2   8.3    0.0  8.933333\n",
      "2  2009     11    1     3   7.9    0.0  8.700000\n",
      "3  2009     11    1     4   7.6    0.0  8.533333\n",
      "4  2009     11    1     5   6.9    0.0  8.533333\n"
     ]
    }
   ],
   "source": [
    "## Loading the file \n",
    "\n",
    "src = r'C:\\Users\\Saad.LAKES\\Desktop\\Pavement-Temperature-Prediction\\Data'\n",
    "filename = r'Pave_data_cleaned.xlsx'\n",
    "\n",
    "dest = r'C:\\Users\\Saad.LAKES\\Desktop\\Pavement-Temperature-Prediction\\Solutions\\Six Hours Lag'\n",
    "\n",
    "df = read_file(os.path.join(src,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8716, 2) (2180, 2)\n",
      "The shape of x_train is (8704, 6, 2) and x_test is (2168, 6, 2)\n",
      "The shape of y_train is (8704, 6) and y_test is (2168, 6)\n"
     ]
    }
   ],
   "source": [
    "## Training the training and testing data\n",
    "\n",
    "x_train,x_test,y_train,y_test,scaler = splitter(df[['Temp','Pavement']],['Pavement'],6,6,0.8)\n",
    "print(f'The shape of x_train is {x_train.shape} and x_test is {x_test.shape}')\n",
    "print(f'The shape of y_train is {y_train.shape} and y_test is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_lstm.hdf5'\n",
    "filepath_attention = 'attention_lstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple LSTM\n",
    "K.clear_session()\n",
    "simple_lstm = keras.Sequential()\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.Dropout(0.3))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.Flatten())\n",
    "simple_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dropout(0.3))\n",
    "simple_lstm.add(keras.layers.Dense(32))\n",
    "simple_lstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new directory......\n",
      "New Directory Created\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0146 - mae: 0.0852 - val_loss: 0.0023 - val_mae: 0.0370\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0069 - mae: 0.0611 - val_loss: 0.0022 - val_mae: 0.0365\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0056 - mae: 0.0552 - val_loss: 0.0022 - val_mae: 0.0367\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0046 - mae: 0.0497 - val_loss: 0.0014 - val_mae: 0.0284\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0041 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0272\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0036 - mae: 0.0431 - val_loss: 0.0015 - val_mae: 0.0295\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0035 - mae: 0.0426 - val_loss: 0.0012 - val_mae: 0.0260\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0032 - mae: 0.0407 - val_loss: 0.0020 - val_mae: 0.0357\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0030 - mae: 0.0402 - val_loss: 0.0010 - val_mae: 0.0242\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0029 - mae: 0.0391 - val_loss: 0.0012 - val_mae: 0.0281\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0027 - mae: 0.0373 - val_loss: 0.0013 - val_mae: 0.0279\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0026 - mae: 0.0367 - val_loss: 9.7834e-04 - val_mae: 0.0230\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0027 - mae: 0.0370 - val_loss: 0.0010 - val_mae: 0.0250\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0026 - mae: 0.0365 - val_loss: 9.7266e-04 - val_mae: 0.0236\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0025 - mae: 0.0355 - val_loss: 0.0011 - val_mae: 0.0263\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0351 - val_loss: 0.0014 - val_mae: 0.0290\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0352 - val_loss: 0.0011 - val_mae: 0.0259\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0348 - val_loss: 9.2194e-04 - val_mae: 0.0231\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0340 - val_loss: 8.6621e-04 - val_mae: 0.0220\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0345 - val_loss: 0.0011 - val_mae: 0.0256\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0340 - val_loss: 9.2318e-04 - val_mae: 0.0226\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0335 - val_loss: 8.9590e-04 - val_mae: 0.0223\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0335 - val_loss: 8.9534e-04 - val_mae: 0.0221\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0335 - val_loss: 8.1338e-04 - val_mae: 0.0207\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0333 - val_loss: 8.4413e-04 - val_mae: 0.0209\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0329 - val_loss: 8.8885e-04 - val_mae: 0.0226\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0329 - val_loss: 9.0732e-04 - val_mae: 0.0230\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0327 - val_loss: 0.0011 - val_mae: 0.0257\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0325 - val_loss: 8.5654e-04 - val_mae: 0.0220\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0321 - val_loss: 9.1475e-04 - val_mae: 0.0233\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0324 - val_loss: 7.9062e-04 - val_mae: 0.0209\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0319 - val_loss: 7.4094e-04 - val_mae: 0.0195\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0326 - val_loss: 0.0013 - val_mae: 0.0293\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0326 - val_loss: 7.8354e-04 - val_mae: 0.0205\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0318 - val_loss: 8.3236e-04 - val_mae: 0.0206\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 8.0783e-04 - val_mae: 0.0211\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0313 - val_loss: 8.2644e-04 - val_mae: 0.0207\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0321 - val_loss: 0.0010 - val_mae: 0.0238\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0315 - val_loss: 8.4793e-04 - val_mae: 0.0216\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0319 - val_loss: 8.2382e-04 - val_mae: 0.0197\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0320 - val_loss: 7.4777e-04 - val_mae: 0.0199\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0317 - val_loss: 7.4088e-04 - val_mae: 0.0198\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0314 - val_loss: 8.7723e-04 - val_mae: 0.0217\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0304 - val_loss: 8.6330e-04 - val_mae: 0.0223\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0314 - val_loss: 7.7653e-04 - val_mae: 0.0201\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 7.3451e-04 - val_mae: 0.0196\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0308 - val_loss: 7.7454e-04 - val_mae: 0.0192\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0312 - val_loss: 8.3103e-04 - val_mae: 0.0209\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 8.3696e-04 - val_mae: 0.0208\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0309 - val_loss: 7.2785e-04 - val_mae: 0.0195\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 9.0795e-04 - val_mae: 0.0209\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 8.6529e-04 - val_mae: 0.0220\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0019 - mae: 0.0305 - val_loss: 0.0011 - val_mae: 0.0253\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0298 - val_loss: 8.4696e-04 - val_mae: 0.0216\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 7.0835e-04 - val_mae: 0.0194\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 7.3532e-04 - val_mae: 0.0192\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0300 - val_loss: 8.3972e-04 - val_mae: 0.0221\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0298 - val_loss: 7.6693e-04 - val_mae: 0.0206\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0300 - val_loss: 7.2297e-04 - val_mae: 0.0200\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0300 - val_loss: 7.7074e-04 - val_mae: 0.0206\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 9.6617e-04 - val_mae: 0.0243\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0310 - val_loss: 8.5748e-04 - val_mae: 0.0224\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 7.1669e-04 - val_mae: 0.0200\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0296 - val_loss: 7.6433e-04 - val_mae: 0.0207\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0299 - val_loss: 7.6293e-04 - val_mae: 0.0203\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 6.8059e-04 - val_mae: 0.0185\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 7.8179e-04 - val_mae: 0.0203\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0297 - val_loss: 6.9912e-04 - val_mae: 0.0194\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0299 - val_loss: 7.3879e-04 - val_mae: 0.0192\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 7.4835e-04 - val_mae: 0.0196\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 9.2646e-04 - val_mae: 0.0221\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0302 - val_loss: 0.0012 - val_mae: 0.0270\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0296 - val_loss: 7.2881e-04 - val_mae: 0.0201\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 8.3453e-04 - val_mae: 0.0222\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 8.6633e-04 - val_mae: 0.0222\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0289 - val_loss: 7.5719e-04 - val_mae: 0.0194\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0287 - val_loss: 8.8940e-04 - val_mae: 0.0218\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0297 - val_loss: 6.6091e-04 - val_mae: 0.0188\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 8.2296e-04 - val_mae: 0.0215\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 8.4799e-04 - val_mae: 0.0221\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0297 - val_loss: 6.9173e-04 - val_mae: 0.0190\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 8.0499e-04 - val_mae: 0.0212\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 9.0890e-04 - val_mae: 0.0233\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0286 - val_loss: 8.9849e-04 - val_mae: 0.0230\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 0.0010 - val_mae: 0.0238\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 7.9315e-04 - val_mae: 0.0201\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 6.7464e-04 - val_mae: 0.0188\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 8.4237e-04 - val_mae: 0.0220\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 7.9438e-04 - val_mae: 0.0202\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 8.7215e-04 - val_mae: 0.0220\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 7.8217e-04 - val_mae: 0.0211\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0288 - val_loss: 7.5208e-04 - val_mae: 0.0202\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0286 - val_loss: 7.7745e-04 - val_mae: 0.0204\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 7.6917e-04 - val_mae: 0.0205\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 8.5075e-04 - val_mae: 0.0223\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 7.8915e-04 - val_mae: 0.0203\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 8.1809e-04 - val_mae: 0.0216\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0286 - val_loss: 0.0010 - val_mae: 0.0240\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 7.1980e-04 - val_mae: 0.0191\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 7.7559e-04 - val_mae: 0.0195\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 9.1498e-04 - val_mae: 0.0216\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 7.6044e-04 - val_mae: 0.0196\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 8.5384e-04 - val_mae: 0.0205\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 8.5643e-04 - val_mae: 0.0210\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 0.0010 - val_mae: 0.0239\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.3890e-04 - val_mae: 0.0192\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 7.3880e-04 - val_mae: 0.0193\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 7.9752e-04 - val_mae: 0.0204\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 7.1612e-04 - val_mae: 0.0193\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0279 - val_loss: 9.0503e-04 - val_mae: 0.0218\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 7.8252e-04 - val_mae: 0.0206\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 8.5672e-04 - val_mae: 0.0217\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 8.5172e-04 - val_mae: 0.0206\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 7.6126e-04 - val_mae: 0.0202\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0281 - val_loss: 7.0811e-04 - val_mae: 0.0190\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 7.4392e-04 - val_mae: 0.0192\n",
      "Epoch 117/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.2853e-04 - val_mae: 0.0196\n",
      "Epoch 118/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 9.5897e-04 - val_mae: 0.0234\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 8.0539e-04 - val_mae: 0.0210\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0278 - val_loss: 8.6833e-04 - val_mae: 0.0213\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 9.8666e-04 - val_mae: 0.0240\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.7944e-04 - val_mae: 0.0201\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 7.9892e-04 - val_mae: 0.0199\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 7.6120e-04 - val_mae: 0.0196\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 0.0013 - val_mae: 0.0271\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 8.0362e-04 - val_mae: 0.0205\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 7.6504e-04 - val_mae: 0.0193\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0276 - val_loss: 8.2691e-04 - val_mae: 0.0207\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 7.9200e-04 - val_mae: 0.0202\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 8.6818e-04 - val_mae: 0.0207\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 0.0010 - val_mae: 0.0254\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 9.3438e-04 - val_mae: 0.0225\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 7.5784e-04 - val_mae: 0.0202\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 8.0414e-04 - val_mae: 0.0202\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 0.0010 - val_mae: 0.0230\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.6841e-04 - val_mae: 0.0202\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 8.8575e-04 - val_mae: 0.0211\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.9197e-04 - val_mae: 0.0202\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 8.5646e-04 - val_mae: 0.0210\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 7.9322e-04 - val_mae: 0.0204\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 7.7534e-04 - val_mae: 0.0202\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.9249e-04 - val_mae: 0.0207\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 8.3167e-04 - val_mae: 0.0206\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 7.3123e-04 - val_mae: 0.0193\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 8.0289e-04 - val_mae: 0.0198\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 8.0347e-04 - val_mae: 0.0199\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.4373e-04 - val_mae: 0.0191\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.2872e-04 - val_mae: 0.0209\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 0.0010 - val_mae: 0.0230\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 7.5154e-04 - val_mae: 0.0197\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 8.7847e-04 - val_mae: 0.0219\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 7.6216e-04 - val_mae: 0.0202\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 8.8959e-04 - val_mae: 0.0227\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 0.0011 - val_mae: 0.0258\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.4885e-04 - val_mae: 0.0214\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 0.0014 - val_mae: 0.0290\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 8.5135e-04 - val_mae: 0.0214\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 8.1426e-04 - val_mae: 0.0206\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.9252e-04 - val_mae: 0.0204\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 7.6319e-04 - val_mae: 0.0200\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 8.6093e-04 - val_mae: 0.0212\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 7.4374e-04 - val_mae: 0.0193\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 9.2227e-04 - val_mae: 0.0219\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 9.5011e-04 - val_mae: 0.0223\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.9101e-04 - val_mae: 0.0204\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 7.8269e-04 - val_mae: 0.0198\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 7.7988e-04 - val_mae: 0.0204\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.1635e-04 - val_mae: 0.0203\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 8.0689e-04 - val_mae: 0.0207\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0273 - val_loss: 8.6265e-04 - val_mae: 0.0206\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 0.0012 - val_mae: 0.0263\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 8.1575e-04 - val_mae: 0.0211\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0263 - val_loss: 7.8355e-04 - val_mae: 0.0192\n",
      "Epoch 174/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.3257e-04 - val_mae: 0.0191\n",
      "Epoch 175/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.6884e-04 - val_mae: 0.0196\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 7.5175e-04 - val_mae: 0.0197\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 7.7034e-04 - val_mae: 0.0192\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 7.4060e-04 - val_mae: 0.0190\n",
      "Epoch 179/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 7.5415e-04 - val_mae: 0.0196\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 7.6827e-04 - val_mae: 0.0198\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 8.2486e-04 - val_mae: 0.0200\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 9.3326e-04 - val_mae: 0.0221\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 0.0010 - val_mae: 0.0242\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 7.8020e-04 - val_mae: 0.0203\n",
      "Epoch 185/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 7.9389e-04 - val_mae: 0.0210\n",
      "Epoch 186/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 8.1494e-04 - val_mae: 0.0208\n",
      "Epoch 187/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 8.3797e-04 - val_mae: 0.0209\n",
      "Epoch 188/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 8.0297e-04 - val_mae: 0.0202\n",
      "Epoch 189/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 9.0414e-04 - val_mae: 0.0214\n",
      "Epoch 190/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 7.8015e-04 - val_mae: 0.0199\n",
      "Epoch 191/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 7.7391e-04 - val_mae: 0.0194\n",
      "Epoch 192/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 9.6838e-04 - val_mae: 0.0226\n",
      "Epoch 193/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0257 - val_loss: 8.2800e-04 - val_mae: 0.0212\n",
      "Epoch 194/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 8.4145e-04 - val_mae: 0.0207\n",
      "Epoch 195/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 7.4491e-04 - val_mae: 0.0190\n",
      "Epoch 196/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 7.7399e-04 - val_mae: 0.0197\n",
      "Epoch 197/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 7.9242e-04 - val_mae: 0.0201\n",
      "Epoch 198/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 7.4055e-04 - val_mae: 0.0194\n",
      "Epoch 199/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 8.1176e-04 - val_mae: 0.0194\n",
      "Epoch 200/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 9.1598e-04 - val_mae: 0.0212\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA7wUlEQVR4nO3deXgUVfbw8e8hYd9XlUUIiiDIHlFRFMYNV1xAQVQYHREU13FBHZdBfV3GGRnGbVBRBxVQHBURxZ+IoOJCQEBQ0AiMBFQ2ZQ+Q5Lx/nGq6k3SWDul0AufzPP10963tVnV1nbr3Vt0SVcU555wrrkqJzoBzzrmKxQOHc865mHjgcM45FxMPHM4552LigcM551xMPHA455yLiQcO55xzMfHA4VyMRGSViJyS6Hw4lygeOJxzzsXEA4dzpUBEqorIGBFZG7zGiEjVYFgjEZkmIr+LyCYR+UREKgXDbheRNSKyVUSWi8jJQXolERklIj+KyEYReU1EGgTDqonIy0H67yIyT0QOStzauwONBw7nSsddwLFAF6Az0AP4SzDsz0AG0Bg4CLgTUBFpC4wEjlbV2sDpwKpgmuuA84CTgKbAb8CTwbAhQF2gBdAQGA7sjNeKOZeXBw7nSsdgYLSqrlPV9cBfgcuCYXuAQ4CWqrpHVT9R6yQuG6gKtBeRyqq6SlV/DKYZDtylqhmqugu4D+gvIsnB/BoCh6tqtqrOV9UtZbam7oDngcO50tEU+F/E9/8FaQB/A9KBD0RkhYiMAlDVdOBGLCisE5FJIhKapiXwZlAV9TvwHRZoDgImADOASUG12KMiUjmeK+dcJA8czpWOtdjBPuTQIA1V3aqqf1bV1sC5wM2htgxVfVVVTwimVeCRYPrVwBmqWi/iVU1V1wSllr+qanugJ3A2cHmZrKVzeOBwrqQqB43U1USkGjAR+IuINBaRRsA9wMsAInK2iBwuIgJsxkoOOSLSVkT+EDSiZ2LtFDnB/J8BHhSRlsE8GotIv+BzHxHpKCJJwBas6ioH58qIBw7nSmY6dqAPvaoBacBi4BtgAfBAMG4b4ENgG/A58JSqzsLaNx4GNgC/AE2AO4Jp/glMxaq3tgJfAMcEww4GpmBB4ztgNlZ95VyZEH+Qk3POuVh4icM551xMPHA455yLiQcO55xzMfHA4ZxzLibJic5AWWjUqJG2atUq0dlwzrkKZf78+RtUtXHe9AMicLRq1Yq0tLREZ8M55yoUEflftHSvqnLOORcTDxzOOedi4oHDOedcTA6INg7nXNnYs2cPGRkZZGZmJjorLgbVqlWjefPmVK5cvE6WPXA450pNRkYGtWvXplWrVlifjq68U1U2btxIRkYGKSkpxZrGq6qcc6UmMzOThg0betCoQESEhg0bxlRK9MDhnCtVHjQqnlh/Mw8chfnXv2Dy5ETnwjnnyhUPHIV55hmYMiXRuXDOFdPGjRvp0qULXbp04eCDD6ZZs2Z7v+/evbvQadPS0rj++uuLXEbPnj1LJa8ff/wxZ599dqnMq6x543hhkpIgKyvRuXDOFVPDhg1ZuHAhAPfddx+1atXilltu2Ts8KyuL5OToh73U1FRSU1OLXMbcuXNLJa8VmZc4CpOc7IHDuQpu6NChDB8+nGOOOYbbbruNr776iuOOO46uXbvSs2dPli9fDuQuAdx3331cccUV9O7dm9atWzN27Ni986tVq9be8Xv37k3//v1p164dgwcPJvRgvOnTp9OuXTu6d+/O9ddfH1PJYuLEiXTs2JGjjjqK22+/HYDs7GyGDh3KUUcdRceOHXn88ccBGDt2LO3bt6dTp04MHDhw3zdWMXmJozDJyZCdnehcOFcx3XgjBGf/paZLFxgzJubJMjIymDt3LklJSWzZsoVPPvmE5ORkPvzwQ+68807eeOONfNMsW7aMWbNmsXXrVtq2bcuIESPy3efw9ddfs3TpUpo2bcrxxx/PZ599RmpqKldffTVz5swhJSWFQYMGFTufa9eu5fbbb2f+/PnUr1+f0047jbfeeosWLVqwZs0alixZAsDvv/8OwMMPP8zKlSupWrXq3rSy4CWOwnhVlXP7hQEDBpCUlATA5s2bGTBgAEcddRQ33XQTS5cujTrNWWedRdWqVWnUqBFNmjTh119/zTdOjx49aN68OZUqVaJLly6sWrWKZcuW0bp16733RMQSOObNm0fv3r1p3LgxycnJDB48mDlz5tC6dWtWrFjBddddx/vvv0+dOnUA6NSpE4MHD+bll18usAouHrzEURgvcThXciUoGcRLzZo1936+++676dOnD2+++SarVq2id+/eUaepWrXq3s9JSUlkRTmJLM44paF+/fosWrSIGTNm8Mwzz/Daa68xfvx43n33XebMmcM777zDgw8+yDfffFMmASSuJQ4R6Ssiy0UkXURGRRleVUQmB8O/FJFWQXpDEZklIttE5IkC5j1VRJbEM//exuHc/mfz5s00a9YMgBdffLHU59+2bVtWrFjBqlWrAJgcwyX9PXr0YPbs2WzYsIHs7GwmTpzISSedxIYNG8jJyeHCCy/kgQceYMGCBeTk5LB69Wr69OnDI488wubNm9m2bVupr080cQtNIpIEPAmcCmQA80Rkqqp+GzHalcBvqnq4iAwEHgEuBjKBu4GjglfeeV8AxH8LJSVBEZfwOecqlttuu40hQ4bwwAMPcNZZZ5X6/KtXr85TTz1F3759qVmzJkcffXSB486cOZPmzZvv/f7666/z8MMP06dPH1SVs846i379+rFo0SL++Mc/kpOTA8BDDz1EdnY2l156KZs3b0ZVuf7666lXr16pr080EroKoNRnLHIccJ+qnh58vwNAVR+KGGdGMM7nIpIM/AI01iBTIjIUSFXVkRHT1ALeB4YBr6lqvsCSV2pqqpboQU59+8Lvv8MXX8Q+rXMHoO+++44jjzwy0dlIuG3btlGrVi1UlWuvvZY2bdpw0003JTpbhYr224nIfFXNd41yPKuqmgGrI75nBGlRx1HVLGAz0LCI+d4P/B3YUdhIIjJMRNJEJG39+vWx5DvMG8edcyXw7LPP0qVLFzp06MDmzZu5+uqrE52lUlWhGsdFpAtwmKreFGoPKYiqjgPGgZU4SrRAbxx3zpXATTfdVO5LGPsiniWONUCLiO/Ng7So4wRVVXWBjYXM8zggVURWAZ8CR4jIx6WU3/y8cdw55/KJZ+CYB7QRkRQRqQIMBKbmGWcqMCT43B/4SAtpdFHVp1W1qaq2Ak4AvlfV3qWe8xCvqnLOuXziVlWlqlkiMhKYASQB41V1qYiMBtJUdSrwPDBBRNKBTVhwASAoVdQBqojIecBpea7Iij+vqnLOuXzi2sahqtOB6XnS7on4nAkMKGDaVkXMexVRLtUtVV7icM65fLzLkcJ4icO5CqVPnz7MmDEjV9qYMWMYMWJEgdP07t2b0OX6Z555ZtQ+n+677z4ee+yxQpf91ltv8e234UqRe+65hw8//DCG3EdXHrtf98BRGG8cd65CGTRoEJMmTcqVNmnSpGL3FzV9+vQS30SXN3CMHj2aU045pUTzKu88cBTGq6qcq1D69+/Pu+++u/ehTatWrWLt2rX06tWLESNGkJqaSocOHbj33nujTt+qVSs2bNgAwIMPPsgRRxzBCSecsLfrdbB7NI4++mg6d+7MhRdeyI4dO5g7dy5Tp07l1ltvpUuXLvz4448MHTqUKcGD4GbOnEnXrl3p2LEjV1xxBbt27dq7vHvvvZdu3brRsWNHli1bVux1TWT36xXqPo4y51VVzpVYInpVb9CgAT169OC9996jX79+TJo0iYsuuggR4cEHH6RBgwZkZ2dz8skns3jxYjp16hR1PvPnz2fSpEksXLiQrKwsunXrRvfu3QG44IILuOqqqwD4y1/+wvPPP891113Hueeey9lnn03//v1zzSszM5OhQ4cyc+ZMjjjiCC6//HKefvppbrzxRgAaNWrEggULeOqpp3jsscd47rnnitwOie5+3UschfESh3MVTmR1VWQ11WuvvUa3bt3o2rUrS5cuzVWtlNcnn3zC+eefT40aNahTpw7nnnvu3mFLliyhV69edOzYkVdeeaXAbtlDli9fTkpKCkcccQQAQ4YMYc6cOXuHX3DBBQB07959b8eIRUl09+te4iiMlzicK7FE9arer18/brrpJhYsWMCOHTvo3r07K1eu5LHHHmPevHnUr1+foUOHkpmZWaL5Dx06lLfeeovOnTvz4osv8vHHH+9TfkNds5dGt+xl1f26lzgK443jzlU4tWrVok+fPlxxxRV7SxtbtmyhZs2a1K1bl19//ZX33nuv0HmceOKJvPXWW+zcuZOtW7fyzjvv7B22detWDjnkEPbs2cMrr7yyN7127dps3bo137zatm3LqlWrSE9PB2DChAmcdNJJ+7SOie5+3UschfGqKucqpEGDBnH++efvrbLq3LkzXbt2pV27drRo0YLjjz++0Om7devGxRdfTOfOnWnSpEmurtHvv/9+jjnmGBo3bswxxxyzN1gMHDiQq666irFjx+5tFAeoVq0aL7zwAgMGDCArK4ujjz6a4cOHx7Q+5a379bh1q16elLhb9XvugQcegOBHcM4VzrtVr7jKS7fqFV9SEqh64HDOuQgeOAoTajzyBnLnnNvLA0dhQoHD2zmcK7YDofp7fxPrb+aBozBJSfbuJQ7niqVatWps3LjRg0cFoqps3LiRatWqFXsav6qqMF7icC4mzZs3JyMjgxI/rtklRLVq1XJdtVUUDxyFCZU4PHA4VyyVK1cmJSUl0dlwceZVVYXxxnHnnMvHA0dhvKrKOefy8cBRGG8cd865fDxwFMZLHM45l09cA4eI9BWR5SKSLiKjogyvKiKTg+FfikirIL2hiMwSkW0i8kTE+DVE5F0RWSYiS0Xk4Xjm3xvHnXMuv7gFDhFJAp4EzgDaA4NEpH2e0a4EflPVw4HHgUeC9EzgbuCWKLN+TFXbAV2B40XkjHjkH/DGceeciyKeJY4eQLqqrlDV3cAkoF+ecfoBLwWfpwAni4io6nZV/RQLIHup6g5VnRV83g0sAIp/8XGsvKrKOefyiWfgaAasjvieEaRFHUdVs4DNQMPizFxE6gHnADMLGD5MRNJEJK3ENyN547hzzuVTIRvHRSQZmAiMVdUV0cZR1XGqmqqqqY0bNy7ZgrzE4Zxz+cQzcKwBWkR8bx6kRR0nCAZ1gY3FmPc44AdVHbPv2SyEN44751w+8Qwc84A2IpIiIlWAgcDUPONMBYYEn/sDH2kRvaOJyANYgLmxdLMbhTeOO+dcPnHrq0pVs0RkJDADSALGq+pSERkNpKnqVOB5YIKIpAObsOACgIisAuoAVUTkPOA0YAtwF7AMWCAiAE+o6nNxWQmvqnLOuXzi2smhqk4HpudJuyficyYwoIBpWxUwWymt/BXJG8edcy6fCtk4Xma8xOGcc/l44CiMBw7nnMvHA0dhvKrKOefy8cBRGC9xOOdcPh44CuMlDuecy8cDR2G8xOGcc/l44CiMBw7nnMvHA0dhvKrKOefy8cBRGC9xOOdcPh44CuMlDuecy8cDR2G8xOGcc/l44CiMBw7nnMvHA0dhvKrKOefy8cBRGC9xOOdcPh44CuMlDuecy8cDR2G8xOGcc/l44CiMP3PcOefy8cBRGBGoVMmrqpxzLoIHjqIkJ3uJwznnIsQ1cIhIXxFZLiLpIjIqyvCqIjI5GP6liLQK0huKyCwR2SYiT+SZpruIfBNMM1ZE4vsM8qQkL3E451yEuAUOEUkCngTOANoDg0SkfZ7RrgR+U9XDgceBR4L0TOBu4JYos34auApoE7z6ln7uI3iJwznncolniaMHkK6qK1R1NzAJ6JdnnH7AS8HnKcDJIiKqul1VP8UCyF4icghQR1W/UFUF/gOcF8d18MDhnHN5xDNwNANWR3zPCNKijqOqWcBmoGER88woYp4AiMgwEUkTkbT169fHmPUIXlXlnHO57LeN46o6TlVTVTW1cePGJZ+Rlziccy6XeAaONUCLiO/Ng7So44hIMlAX2FjEPJsXMc/S5SUO55zLJZ6BYx7QRkRSRKQKMBCYmmecqcCQ4HN/4KOg7SIqVf0Z2CIixwZXU10OvF36WY/gJQ7nnMslOV4zVtUsERkJzACSgPGqulRERgNpqjoVeB6YICLpwCYsuAAgIquAOkAVETkPOE1VvwWuAV4EqgPvBa/48cDhnHO5xC1wAKjqdGB6nrR7Ij5nAgMKmLZVAelpwFGll8sieFWVc87lst82jpcaL3E451wuHjiK4iUO55zLxQNHUbzE4ZxzuXjgKIoHDuecy8UDR1G8qso553LxwFEUL3E451wuHjiK4iUO55zLxQNHUbzE4ZxzuXjgKIoHDuecy8UDR1G8qso553LxwFEUL3E451wuHjiK4iUO55zLxQNHUbzE4ZxzuXjgKIoHDuecy8UDR1G8qso553LxwFEUL3E451wuHjiKkpzsJQ7nnIvggaMoSUle4nDOuQgeOIriVVXOOZeLB46ieOO4c87lUqzAISI1RaRS8PkIETlXRCoXY7q+IrJcRNJFZFSU4VVFZHIw/EsRaRUx7I4gfbmInB6RfpOILBWRJSIyUUSqFWtNS8pLHM45l0txSxxzgGoi0gz4ALgMeLGwCUQkCXgSOANoDwwSkfZ5RrsS+E1VDwceBx4Jpm0PDAQ6AH2Bp0QkKVj+9UCqqh4FJAXjxY83jjvnXC7FDRyiqjuAC4CnVHUAdlAvTA8gXVVXqOpuYBLQL884/YCXgs9TgJNFRIL0Saq6S1VXAunB/ACSgeoikgzUANYWcx1KxhvHnXMul2IHDhE5DhgMvBukJRUxTTNgdcT3jCAt6jiqmgVsBhoWNK2qrgEeA34CfgY2q+oHBWR4mIikiUja+vXri8hqIbyqyjnncilu4LgRuAN4U1WXikhrYFbcclUAEamPlUZSgKZATRG5NNq4qjpOVVNVNbVx48YlX2hSEqhCTk7J5+Gcc/uR5OKMpKqzgdkAQSP5BlW9vojJ1gAtIr43D9KijZMRVD3VBTYWMu0pwEpVXR/k5b9AT+Dl4qxHiSQHmyg7Gyr5RWjOOVfcq6peFZE6IlITWAJ8KyK3FjHZPKCNiKSISBWsEXtqnnGmAkOCz/2Bj1RVg/SBwVVXKUAb4CusiupYEakRtIWcDHxXnHUoscjA4ZxzrthVVe1VdQtwHvAeVlV0WWETBG0WI4EZ2MH9taCaa7SInBuM9jzQUETSgZuBUcG0S4HXgG+B94FrVTVbVb/EGtEXAN8E+R9XzHUomaSgKcfbOZxzDihmVRVQObhv4zzgCVXdIyJa1ESqOh2YniftnojPmcCAAqZ9EHgwSvq9wL3FzPe+C5U4PHA45xxQ/BLHv4FVQE1gjoi0BLbEK1PlSqjE4VVVzjkHFL9xfCwwNiLpfyLSJz5ZKme8xOGcc7kUt3G8roj8I3RfhIj8HSt97P+8cdw553IpblXVeGArcFHw2gK8EK9MlSveOO6cc7kUt3H8MFW9MOL7X0VkYRzyU/54icM553Ipboljp4icEPoiIscDO+OTpXLGSxzOOZdLcUscw4H/iEjd4PtvhG/c279547hzzuVS3KuqFgGdRaRO8H2LiNwILI5j3soHr6pyzrlcYup8SVW3BHeQg93pvf/zqirnnMtlX3rtk1LLRXlWM7jqeOvWxObDOefKiX0JHEV2ObJfaNLE3vflmR7OObcfKbSNQ0S2Ej1ACFA9Ljkqb0LP8vDA4ZxzQBGBQ1Vrl1VGyq1Gjex93brE5sM558oJfzJRUapUgXr1vMThnHMBDxzF0bixlziccy7ggaM4mjTxEodzzgU8cBSHlzicc24vDxzF4SUO55zbywNHcTRuDBs2QE5OonPinHMJF9fAISJ9RWS5iKSLyKgow6uKyORg+Jci0ipi2B1B+nIROT0ivZ6ITBGRZSLynYgcF891AKzEkZ0Nv/0W90U551x5F7fAISJJwJPAGUB7YJCItM8z2pXAb6p6OPA48EgwbXtgINAB6As8FcwP4J/A+6raDugMfBevddjLbwJ0zrm94lni6AGkq+oKVd0NTAL65RmnH/BS8HkKcLKISJA+SVV3qepKIB3oEXTrfiLwPICq7lbV3+O4DibU7Yg3kDvnXFwDRzNgdcT3jCAt6jiqmgVsBhoWMm0KsB54QUS+FpHnRCTqs89FZFjoGenr97Wk4CUO55zbq6I1jicD3YCnVbUrsB3I13YCoKrjVDVVVVMbhw78JRWa3ksczjkX18CxBmgR8b15kBZ1HBFJBuoCGwuZNgPIUNUvg/QpWCCJr1B/VV7icM65uAaOeUAbEUkRkSpYY/fUPONMJfwI2v7AR6qqQfrA4KqrFKAN8JWq/gKsFpG2wTQnA9/GcR1M5cpQv76XOJxzjuI/czxmqpolIiOBGUASMF5Vl4rIaCBNVadijdwTRCQd2IQFF4LxXsOCQhZwraqGnt16HfBKEIxWAH+M1zrk4jcBOuccAGIn+Pu31NRUTUtL27eZnHgiiMDs2aWTKeecK+dEZL6qpuZNr2iN44nTvDmsydtE45xzBx4PHMXVvDlkZMABUEJzzrnCeOAorubNYdcu2Lgx0TlxzrmE8sBRXM2CexczMhKbD+ecSzAPHMXVvLm9ezuHc+4A54GjuEKBw0sczrkDnAeO4jr4YEhK8sDhnDvgeeAorqQkOOQQDxzOuQOeB45YhC7Jdc65A5gHjlg0a+aBwzl3wPPAEQu/e9w55zxwxKR5c9i6FbZsSXROnHMuYTxwxMIvyXXOOQ8cMTn0UHtfuTKx+XDOuQTywBGLDh3sffHixObDOecSyANHLOrWhVatPHA45w5oHjhi1akTLFqU6Fw451zCeOCIVefOsHw5ZGYmOifOOZcQHjhi1akT5OTA0qWJzolzziVEXAOHiPQVkeUiki4io6IMryoik4PhX4pIq4hhdwTpy0Xk9DzTJYnI1yIyLZ75j6pzZ3v3dg7n3AEqboFDRJKAJ4EzgPbAIBFpn2e0K4HfVPVw4HHgkWDa9sBAoAPQF3gqmF/IDcB38cp7oVq3hho1vJ3DOXfAimeJoweQrqorVHU3MAnol2ecfsBLwecpwMkiIkH6JFXdpaorgfRgfohIc+As4Lk45r1gSUnQsSMsXJiQxTvnXKLFM3A0A1ZHfM8I0qKOo6pZwGagYRHTjgFuA3JKPcfF1aMHzJsHe/YkLAvOOZcoFapxXETOBtap6vxijDtMRNJEJG39+vWlm5FevWDHDvj669Kdr3POVQDxDBxrgBYR35sHaVHHEZFkoC6wsZBpjwfOFZFVWNXXH0Tk5WgLV9VxqpqqqqmNGzfe97WJdMIJ9v7pp6U7X+ecqwDiGTjmAW1EJEVEqmCN3VPzjDMVGBJ87g98pKoapA8MrrpKAdoAX6nqHaraXFVbBfP7SFUvjeM6RHfIIXDYYfDJJ2W+aOecS7TkeM1YVbNEZCQwA0gCxqvqUhEZDaSp6lTgeWCCiKQDm7BgQDDea8C3QBZwrapmxyuvJdKrF0ybBqogkujcOOdcmRE7wd+/paamalpaWunO9Pnn4U9/shsB2+e9ytg55yo+EZmvqql50ytU43i5cuqpUKUKXH017NyZ6Nw451yZ8cBRUoceChMmwGefWfBwzrkDhAeOfXHRRfDnP8Mrr/izyJ1zBwwPHPtq2DDr9PDVVxOdE+ecKxMeOPZVmzZw3HHw0kt2hZVzzu3nPHCUhssvt6ur5hd5Q7tzzlV4HjhKw8UXQ/36MHw47N6d6Nw451xceeAoDfXr230d8+fDXXclOjfOORdXHjhKy/nnw4gR8Nhjdpmuc87tpzxwlKYxY+APf4Arr4Qvvkh0bpxzLi48cJSmKlXgjTegcWO4445E58Y55+LCA0dpq1cPbr0VPv7Yu113zu2XPHDEw1VXWanjL3+BrVvh55/tiYF+n4dzbj/ggSMeataE0aNh9mxo2dL6terRA/r2hZUrE50755zbJx444mX4cPjySzj5ZLjhBrvaKvT9118TnTvnnCuxuD3IyWGljNdfD3/v1Qt694azzoL//tdKIs45V8F4iaMs9egBr70Gy5ZBhw7QsSM0bQoPP+zP9HDOVRj+BMBC5ORApXiE1lWrYNQoazjPyoIPPoAjjoB//AO+/x6SkqBfP2sfcc65BCnoCYAeOApx9tnWzj1ihNUwxc3//Z/dNLh6de70M86AK66A44+HQw6JYwaccy4/f3RsjHJyoG1bO6b36WP9GG7YEKeFnXoqfPMNTJwIP/5opY777oMFC2DAAKvOatnS2khSU+HRRyEjw+5O37IlTplyzrno4lriEJG+wD+BJOA5VX04z/CqwH+A7sBG4GJVXRUMuwO4EsgGrlfVGSLSIhj/IECBcar6z6LyUdISB1jTw+OP23H8nHPsxvAys3u3BY8vvoDPP4d162DXLvscUq2aXeZ70klwySXW4eLYsdCtmxWT3n7bqsSOOcaqw5xzrpjKvKpKRJKA74FTgQxgHjBIVb+NGOcaoJOqDheRgcD5qnqxiLQHJgI9gKbAh8ARQBPgEFVdICK1gfnAeZHzjGZfAkfIJZfY8bpc3IbxyScWUFq0gFmzYNo0azepVw8OPxzS0kAEOnWCRYvC051+OjzyCHTubN9Xr4Ynn7RnpqekWKnnX/+yGxb79rViVqNGiVhD51w5UFDgQFXj8gKOA2ZEfL8DuCPPODOA44LPycAGQPKOGzlenunfBk4tKi/du3fXfXXPPaqVKqlmZuYftn27at++qosX7/NiSm7JEtUTT1StUUP1hRdU//Qn1Xr1VJ980oY99JBqo0aqtWqpfvCB6n//q9qkiSrYeL172+fq1VUPO8w+V62qesMNqllZqgsXqj7+uOrmzYXnIzu7DFbWOVcWgDSNdnyPllgaL6A/Vj0V+n4Z8ESecZYAzSO+/wg0Ap4ALo1Ifx7on2faVsBPQJ0Clj8MSAPSDj300H3egBMm2Nb69tv8w774wob9/e/7vJh9k5OjumNH+Hveg3hGhmq7dpZZUD3iCNX33lPt2VO1ZUvVhx9W3bjRxv3mG9UrrrDxBgxQrVvXPjdsqDpsmOqoUapdu6oedJC9+vRRbdNGtXZt1ddft7yE5hWL339XXb26pFvAOVeKCgocFfIGQBGpBbwB3KiqUVuHVXUcMA6sqmpfl9mmjb3/8AMceWTuYaHqq7wXRZU5EahePfw977XEzZpZNyj/+Y/dR9KnT7iNJK+jjrKHU9WvD3//u1VlvfwyjB8Pkydbu8nxx9tlw3v22KNz27Sx6rKLLrIG/TVrYOBAu+ExPd16Dz70UOjZ0+YnAr/9BpmZsGQJvPii3Ri5a5ddyvbggza/4srOhs8+s/knV8hd27kKIZ7/rjVAi4jvzYO0aONkiEgyUBdrJC9wWhGpjAWNV1T1v/HJen6RgSOvVavsPSOjrHKzD5o0gVtuKf74jz5qbSJ/+IMFnrPPtkvOdu60a5Xz2rEDbr7ZGvIPPRSeeQYmTco/3sEH2/Q//hhOq1/fLj8WgaefhilTYMgQuzjgsMPgj3+0gDN7Nrz/vgWc7t3hmmugTh247jqb7rTT4IUXLChmZ0Pduha0wL4/95x1/XL44Za2e7cFuZSU4m+XuN3k41wFEK0YUhovLCitAFKAKsAioEOeca4Fngk+DwReCz53CMavGky/ArsyS7CrqsbEkpfSaONQVW3QQPXqq/OnDxtmtTjHHFMqi9m/rF1r1V67d6vu3Km6aJHqU0+pDh6sesEFqo88ovr009bmsnNneLr581V79LAN27Wrtc2EqthA9ZBDVNu2tc+1atk4oHrmmarJybnHbdJE9cUXrRrv6qstrW5d1YkTVRcsUO3e3dIuuMBe3burzphh+cjMVJ0+XbV/f1vm//t/qnfcoVqzpuo//1l62yknp/Tm5VwpoYCqqnhfjnsmMCY46I9X1QdFZHSQmakiUg2YAHQFNgEDVXVFMO1dwBVAFlYl9Z6InAB8AnwD5ASLuVNVpxeWj9K4qgrguOOgRg2YOTN3+mmn2f0ezZpVkFJHRaEK27ZB7dp2v8rHH8PatdCuHZx4op3xz58Pzz5rV4/16mXdtyxYAHPm2B34lSrBq69aqaVSJSspjBgBc+eGrzirWxcuvRReeglq1QqXhFq0gI0brRTVoIF1ETN7tk3Trl2465hNm6B5cyv1/Pyzdanfvr3lp3Jluxzv1VftJs7TT7er3dq3tyq+J56wK+M2b4Zx4+yenjlzbNwmTazq8aCDit5OIvnTMzNt+UlJpfqzuAOH3zleCoHj8svtP563LaNNG6vCr1TJque9er2cycmB6dMteBx0EIwcaT/UnDlWz3jaadCqlR3Ik5Js2GOPwYoV1sZy6qlWVVetGsyYYdVeJ54I999v82zaFH76CbZvtyq49eth8WJrBwLbIc45x6rvvvjCqstCatWyNqBVq6z35GrV7IAfqV07OOUUe1eFX36xhrWDD7bPb75p83/6aZv3ggVWzTdhggWxyy6z6saePS0gTZtmbUrZ2TB4cLjKLtKKFdZelJlpZ0wdOuQPTmlpFoCHDw9XBRZl7VrLQ7RA58odDxylEDjuvx/uuceOD5Uq2f/1vPPshLV2bTvx/OknO1F1B7isLDs4JyXZDhFq5N+1yxrKvv3WShkDBtiwzExre8rKgkGDbNimTfZ67z0LONu22TySkqyEs26dlShOP912xqys8PKrVbOgsHq1FYdD//P69e2ChJBKlSwItmkTLi43bGgXQOzZEx7v0EOha1e44AIrMY0bB//+twXlnj2tfSkry/Jbu7a1F6WkWGBbssQC2IsvWj4HD4Zrr7X0o4+2YZ98YnkLdfy5erVNm5pqQfizz+Dcc229i5Kdbety6KH5A9T69damtXu3Pd6ga1eoWjWGH/bA4oGjFALHpEn2nx471k7o5syxp8T+7W92cdHbb4cv6inK0KFwwgnwpz/tc7bcgSBU0khOtkBTubIdtHNyLG3+fCtJ1KtnV8T16GEHcLCqtu+/twsKFi+2Gzv79rUA8s9/wkcfhavmcnLs8yWX2HNkqle3Utbs2VYiCl0JkpwMw4bZcq691s6milK9up1pTZ5syylISootR9UuYVy+PHwxQvXqlt6hg5Vcdu2ykk/jxnDmmWxetIpKX31B7a1rLXCcfLIFm6ZNLThNnJj7SZw9e8Irr1gg7949XC24bJlVLzZqZNt28WK7UbZtWwv8/ftb9SVYoMrOtt9kxw4rfVWuHNPPW1554CiFwPHTT3YFakaG7Rv169t+snWr1Wzccov9Jy66qPD5fPedVXHnvbHbuXJN1epqf/zRzpSaNLH0TZvs7D052f4UW7ZYVdfKlRbIunWz0lHLlnYA//prO/h27mwH882brXuc7dvhq6/g00+hSxc7iL/wgpVKrroK3nkHfv/dSjZLl1rncZUq2bjp6fDpp5xa4zOqNqzFtD/Psl6n58+3vKla0Ln2WquaS062Ut7NN1vpA2x5jz9u873jjnAwrFfP8h75Z23QwPL/9deWp0j161ub2aJFtt7nnmvjffWVlRZ79LBt8tlnlu/sbDuDHDLEpn3pJataXbbMqkkbNLDpW7WyS+jPPddKb6tWWUeoK1dannv0sN9E1drmFi607fT88yWuGvTAUQqBA+w3/vxzOxF54w17rDjYydgxx9gtD1ddZftMtWpW0s7r7rvhgQfs85o19l+Kl+nTbX9r3z5+y3D7l19/Lbo9vjzKysyiToNkKle2wtTeq6WzsmylatbMf1/QF19YSaxTJ7jzTivdgJVEJk+2M8RQCW/WLDtTbNTI/sC//GJB7aCDbPju3Xb1zFdfwVtvWd9wTZvadC1bWilP1Zb3009WTda5sx3033knd746dbLqww8+sPkedZRNs359+CKPypVzVydGk5IC8+ZZ9WMJeOAopcAR6YcfbN8QsdsaGjaEM8+0kv/GjbbPrVmTu7snVTvhUbUThfHj7faEeAjdwnDKKbYf7+9UrfrwxBO97bWkQheKvfOO7csVyZIl1kQCVjMXuveq2LZvt4NskyZ2IcK+3KezY4eVcESsSqJmzfD8VG1ZtWqFx09PD195M2BAeEV27bL3qlVtupkzrU78vPPsTHXOHDugHHSQVdmFesuuV8+q+Q4+uOTrQAL6qipPr9K6jyOarl1VmzWzz6HePCpXVn3iCfs8Zkzu8T//3NLHj1dt2tR689gX27er/vhj9GFLl4ZvYyjr2wRycgrOV7xMmmTrO21a2S53f/L007YNr7su0TkpvvHjVd99127VCd26M3Fi6S/npptK99adioAC7uPwW1/30TPP2AvCF3xcdZVVpaamWoki1DaZk2ON6XXr2sUpffvaBS+RF8NMmGDtd8V19dV2crJxY/5hoULWunXhNs28iirpltTEiXaz95w58Zl/NE8/be+hWy0qsk2b8ledl4VQj/1F/W6RVxQn0i+/2H9g5Ejb32vUsJL+ggWlu5yMDBgzxm4TKqxd/4ARLZrsb694ljgiDRumWq2a9SWoajdIg+rBB9t7z572/uKLNnzaNPt+1132fdUqK60kJal+9lnRy/vmG1URm8ff/pZ/+HXXhc/AXn01//CPP7YbqN9/v0SrW6CcnPCN3BddVPzpNm8ueckoVLoKbefSMHOm6vnnl6yvxmi+/FL1xhuL7kB41y67Kf6kk0pnuar5l7ltm+obb1jHx5GOOMK2oYjqb79Fn9eaNXYT/eDB+XuL/te/bLuVps8/Vx05Mvp2++tfw797vXqqxx+vmpqq+oc/lG4eHn44vJzPPy/5fHJyrBbiwgvtd47Fzp32u5Ulyrp33PL0KqvAsXat9ZQR8ttvtjN37Kg6YoR1y3722eGDY06O6pVX2q/w7LOqQ4daT+YtW9rr9detR/QVK3IfUHNyVNPTVc84Q7VOHesho3Vr1eXLVd9+OzzeccfZq0aN/FUPOTnhHj3atIl9J45m1y7VDz+0XtvB1iE5WfWXX4qe9vvvLZ+XXJL/YFYc111nQXfoUHuP7CQ4UlHz3rbNtuErr1h+QPXuu2PPT167d4erMmfNCqd/8onqypW5x3300fBBasWK6PP79lvVPXuKt+w9e2wf+dOf7PuuXaqnnmrzf/rp8HgbNlja6adrgVV+OTmqZ51l2xhUTz45nI8vv7S01q0trTSqR3fuVD388Oj52bXLAthJJ4V7pLn+ejuBq1ev8OXn5NjJ1nHHWVdBW7cWPm779qqdOtn+fOutJVuXHTtUzzsv/Nu+8ELB42Zl2WMaIo8VJ5yg2qqV/U7F8b//2dMU9uV38MCRIL//Hj5YrVyZuzsmVft+wgnhnemmm6yb9vr1w2lgZ3eLF9vO3qZNOP2hh8J1+6HSx9ix9setXt3md9JJqkcfbW0Oy5bZmdvkyTbuJZfY+7XXWgkmcid75BE7Az3lFAsGeS1fbnXLodLRrbeG81W/vnUDBaoPPhieJjvbSjh5z5wuuCDcxdTll8e2s//wg2qVKhY03n7b5jF7th1orrrKDvx79qhedpnqUUfZASc7284cx461kp6qlXiOPTa8Dm3a2AG2Tp3w2feiRaqffmoHmmuuscC/fXv+PGVm2ry//tq+jxlj80xOtvVTtRJfUpIdaLduVX3pJTvBqFXLDmig+sAD+ef94Ye6tzS3YYPqnXfaGfmTT0Y/K584MbxO//iHHfjB2uaaNg0H2XfftfTp0y0w3Habpe/cacE/O9t+S7BHs4wbF/6ck6Paq5f9DqGAdPTRFoTWrVP9979Vn3suf7Bbu1b1p58K/m3vvdfmV7u2zeuVV6xE8f334QD77rv224dK8//+t32eOjX39tiwwR4v8803qv/5j43TrZv9b4YPz7/sPXts+hdesHGfeUb1tNMskMV6MN6506YVsd+gc2fVI48M5y8ry/I1bZrq+vWqF1+c+/cP/V9DgX3KFNV58wpeXnq6nbjVrWsBpKQ8cJRjWVn2vI+LL7adRtXOUOfOtR3mzjuttBLacY4/3nbiefNsB961y3bKa65RPeccG/f6623cl19Wvf32cFAJNd6HHsexZ084eIDl4ddfbccEK5WkpNg8//pX1U2b7MxywIDcgW3kSDsonnOOHaDHj7f1OP10OxB+952t55AhNv5hh6n+3/9Z/mfMsLTRo8NVD3//u1URTZtmjwwJVWO99JIdcEOys+0hWnXq2EFo/XqbPnSWWrOmvR95ZDivL7wQ7uswdPDYuNEO1snJlvePPrJg8fXXNs6oUXbwDAX0mjVtm4rY9FOmWL4ee8wCRmpqOJh362bzPf10C2Q1athvd9BBqi1a2DihZ2fVr29ntunpdiBu187We9o027bTp1vwC61XjRoWfOrVs+/9+oWftZWTY6/OnW0+oarSqlWtSmnWLPt+0kn2HK/jj7ffeds2G7dZM9X77rN3UG3e3N4vvNB+y5wcK/XWqhV+dMtTT4X7nqxSxfa10P4W+h1uvFH1llvsES6VKtm6vPOO6s032z7Ztq3t8489Zus2cKDtG5H77sEH23a74ALLx1dfWRD83//sBC20PRo2tG3y6KPhEt/BB9sFI8ccY/vPn/8cDsRXX23569bN0qpXt/dOnewk8Jln7HuvXhYct2+3E5CpU+3E7p137LV5s/2PvvzSAlnr1pbf0P/i5ZdtPn37qnbpYlXckf8nCFf33nCDBYFOncIX3YBtu3/9ywLn3XerDhpky3rzTVvHhg1z14CUhAeOCm7uXNspirpSadu2cFUDWAnjq6/sD3vvvXbW9+c/2x98zRqbJlT19de/5u5Y9thj7cx527bwGVAogNWsaTvr55/bDhuqY163Lnd+Vq9WbdzYgk+XLjbe8OFW5I48GLVsacvJybHifFJS+E8bGv7HP4YPxkOH2gE0lJ/Iq9dCB4iRIy2oPvCAfb/0UjuINmpk36+5Jnw22bChLXPKlPzb9PLLwwePKlWsvrt/fwsU06bZtHn/9HXr2tnxzTfbQfj22+1AErqqDizYLVliQQmsxBZ5hhw6c65dO/dBEyyfo0ZZgAqdQIwda9ujQQPLX/364QPo+PF2UB01Knf114ABdpDp2tW267HHWvrMmVY9Ewqso0fbQe7f/859tp2ebge9KlUsKO7ZY+tdubJVtc6aZaW2t96yDpB79LDftWpV25533WW/Y2i9zjrLlhP6ft55dhD++WdbRrt2VuKqXduq3wqq89+61dr1hg4Nn0TUqWP7f8OGtp1CB9WdOy0AHXaY7avVq1tp6c477aD98svhWoNdu+x/FMpz3o6YC3p17py7LXHPHvvtWrWy4HvzzVYK+vBD+41ef93+e2efHf7fffihbfv581XT0qwj6MggEmpLBaseX7Ik+raJhQeOA8zcuarPPx97kXrhQquieuCB/G0T8+fbTv3ss3b2FbJnj/3J3nkn+jw/+sgOYj172h9X1c7UnnvODhQPPZS7Afr3363u/Iorwgfnli3DweCyy3Rvyeuuu+wPF9l28cYb4eqTyPXavTtcbXPkkeFqw6FDLWi8/nr0/GdmWmMrqN5/f/7hu3dbW8W0aRY4160r+ICWk2PLu+EGOxiG0r7/Pv+427eHD16hbT5ihJXaCvpd582zg02TJhYor77aDujFacNasyZc4g3lKyOj6Mb85cstKObNe0Gys3P/Xhs3WhtVZNvPvHlWCo9c9tdfh/P3yy8Ft2NFs3ZtuLrxu++s5LYvcnIsv9dea/mcO9f2rc8+s/199Gg7mXn7bauC2pcnKmdl5a/iVrX/3YQJVo28fXu49P7cc6XTZqlacODwGwBdhbBpk92bddppdk9VZqbdmR+r7Gy76XfAgPDd9NnZdllns2YFT7d1q3UF1b//ftMNkXNF8jvHPXA451xMCgocfgOgc865mHjgcM45FxMPHM4552LigcM551xM4ho4RKSviCwXkXQRGRVleFURmRwM/1JEWkUMuyNIXy4ipxd3ns455+IrboFDRJKAJ4EzgPbAIBHJ+zihK4HfVPVw4HHgkWDa9sBAoAPQF3hKRJKKOU/nnHNxFM8SRw8gXVVXqOpuYBLQL884/YCXgs9TgJNFRIL0Saq6S1VXAunB/IozT+ecc3EUz8DRDFgd8T0jSIs6jqpmAZuBhoVMW5x5AiAiw0QkTUTS1q9fvw+r4ZxzLlJyojMQL6o6DhgHICLrReR/JZxVI2BDqWWs9Hi+Ylde8+b5ik15zReU37yVNF8toyXGM3CsAVpEfG8epEUbJ0NEkoG6wMYipi1qnvmoauOYch5BRNKi3TmZaJ6v2JXXvHm+YlNe8wXlN2+lna94VlXNA9qISIqIVMEau6fmGWcqMCT43B/4KOhYayowMLjqKgVoA3xVzHk655yLo7iVOFQ1S0RGAjOAJGC8qi4VkdFYj4tTgeeBCSKSDmzCAgHBeK8B3wJZwLWqmg0QbZ7xWgfnnHP5xbWNQ1WnA9PzpN0T8TkTGFDAtA8CDxZnnnE2rgyXFQvPV+zKa948X7Epr/mC8pu3Us3XAdE7rnPOudLjXY4455yLiQcO55xzMfHAUYDy1CeWiLQQkVki8q2ILBWRG4L0+0RkjYgsDF5nJiBvq0Tkm2D5aUFaAxH5PxH5IXivX8Z5ahuxTRaKyBYRuTFR20tExovIOhFZEpEWdRuJGRvsd4tFpFsZ5+tvIrIsWPabIlIvSG8lIjsjtt0zZZyvAn+7gvq1K6N8TY7I0yoRWRikl+X2Kuj4EL99LNrzZA/0F3bF1o9Aa6AKsAhon8D8HAJ0Cz7XBr7H+uq6D7glwdtqFdAoT9qjwKjg8yjgkQT/lr9gNzIlZHsBJwLdgCVFbSPgTOA9QIBjgS/LOF+nAcnB50ci8tUqcrwEbK+ov13wP1gEVAVSgv9tUlnlK8/wvwP3JGB7FXR8iNs+5iWO6MpVn1iq+rOqLgg+bwW+o4CuVsqJyD7IXgLOS1xWOBn4UVVL2nPAPlPVOdjl5pEK2kb9gP+o+QKoJyKHlFW+VPUDte5/AL7AbrItUwVsr4IU1K9dmeZLRAS4CJgYj2UXppDjQ9z2MQ8c0RW7T6yyJtb1fFfgyyBpZFDcHF/WVUIBBT4QkfkiMixIO0hVfw4+/wIclIB8hQwk95850dsrpKBtVJ72vSuwM9OQFBH5WkRmi0ivBOQn2m9XXrZXL+BXVf0hIq3Mt1ee40Pc9jEPHBWIiNQC3gBuVNUtwNPAYUAX4GesqFzWTlDVblhX99eKyImRA9XKxgm55lusd4FzgdeDpPKwvfJJ5DYqiIjchd18+0qQ9DNwqKp2BW4GXhWROmWYpXL520UYRO4TlDLfXlGOD3uV9j7mgSO64vSzVaZEpDK2U7yiqv8FUNVfVTVbVXOAZ4lTEb0wqromeF8HvBnk4ddQ0Td4X1fW+QqcASxQ1V+DPCZ8e0UoaBslfN8TkaHA2cDg4IBDUBW0Mfg8H2tLOKKs8lTIb1cetlcycAEwOZRW1tsr2vGBOO5jHjiiK1d9YgX1p88D36nqPyLSI+slzweW5J02zvmqKSK1Q5+xhtUl5O6DbAjwdlnmK0Kus8BEb688CtpGU4HLgytfjgU2R1Q3xJ2I9AVuA85V1R0R6Y3FHqSGiLTG+o9bUYb5Kui3K6hfu7J0CrBMVTNCCWW5vQo6PhDPfawsWv0r4gu78uB77EzhrgTn5QSsmLkYWBi8zgQmAN8E6VOBQ8o4X62xK1oWAUtD2wl7pspM4AfgQ6BBArZZTayn5boRaQnZXljw+hnYg9UnX1nQNsKudHky2O++AVLLOF/pWP13aD97Jhj3wuA3XggsAM4p43wV+NsBdwXbazlwRlnmK0h/ERieZ9yy3F4FHR/ito95lyPOOedi4lVVzjnnYuKBwznnXEw8cDjnnIuJBw7nnHMx8cDhnHMuJh44nCshEcmW3L3wllovykHvqom8z8S5AsX10bHO7ed2qmqXRGfCubLmJQ7nSlnwXIZHxZ5T8pWIHB6ktxKRj4KO+maKyKFB+kFiz75YFLx6BrNKEpFng2csfCAi1YPxrw+evbBYRCYlaDXdAcwDh3MlVz1PVdXFEcM2q2pH4AlgTJD2L+AlVe2EdR44NkgfC8xW1c7Y8x6WBultgCdVtQPwO3Y3MtizFboG8xken1VzrmB+57hzJSQi21S1VpT0VcAfVHVF0PncL6raUEQ2YF1l7AnSf1bVRiKyHmiuqrsi5tEK+D9VbRN8vx2orKoPiMj7wDbgLeAtVd0W51V1LhcvcTgXH1rA51jsivicTbhN8iysr6FuwLygd1bnyowHDufi4+KI98+Dz3OxnpYBBgOfBJ9nAiMARCRJROoWNFMRqQS0UNVZwO1AXSBfqce5ePIzFedKrrqILIz4/r6qhi7JrS8ii7FSw6Ag7TrgBRG5FVgP/DFIvwEYJyJXYiWLEVgvrNEkAS8HwUWAsar6eymtj3PF4m0czpWyoI0jVVU3JDovzsWDV1U555yLiZc4nHPOxcRLHM4552LigcM551xMPHA455yLiQcO55xzMfHA4ZxzLib/Hzo9NkxO3D2IAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error is: 12.577021916561455\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('LSTM')\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_lstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_lstm.load_weights(filepath_simple)\n",
    "preds = simple_lstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention model\n",
    "\n",
    "K.clear_session()\n",
    "atten_lstm = keras.Sequential()\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "# atten_lstm.add(attention(return_sequences=True))\n",
    "atten_lstm.add(keras.layers.Dropout(0.3))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_lstm.add(attention(return_sequences=True))\n",
    "atten_lstm.add(keras.layers.Flatten())\n",
    "atten_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dropout(0.3))\n",
    "atten_lstm.add(keras.layers.Dense(32))\n",
    "atten_lstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.0169 - mae: 0.0914 - val_loss: 0.0028 - val_mae: 0.0430\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0071 - mae: 0.0621 - val_loss: 0.0024 - val_mae: 0.0375\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0063 - mae: 0.0581 - val_loss: 0.0021 - val_mae: 0.0366\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0053 - mae: 0.0531 - val_loss: 0.0016 - val_mae: 0.0307\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0044 - mae: 0.0484 - val_loss: 0.0015 - val_mae: 0.0302\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0038 - mae: 0.0447 - val_loss: 0.0013 - val_mae: 0.0271\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0037 - mae: 0.0439 - val_loss: 0.0015 - val_mae: 0.0307\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0034 - mae: 0.0421 - val_loss: 0.0017 - val_mae: 0.0323\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - ETA: 0s - loss: 0.0031 - mae: 0.040 - 1s 5ms/step - loss: 0.0031 - mae: 0.0406 - val_loss: 0.0014 - val_mae: 0.0283\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0030 - mae: 0.0394 - val_loss: 0.0011 - val_mae: 0.0256\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0029 - mae: 0.0386 - val_loss: 0.0011 - val_mae: 0.0244\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0015 - val_mae: 0.0294\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0027 - mae: 0.0370 - val_loss: 0.0018 - val_mae: 0.0338\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0027 - mae: 0.0372 - val_loss: 0.0011 - val_mae: 0.0244\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0027 - mae: 0.0376 - val_loss: 0.0011 - val_mae: 0.0253\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0027 - mae: 0.0376 - val_loss: 8.9347e-04 - val_mae: 0.0215\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0027 - mae: 0.0370 - val_loss: 9.2033e-04 - val_mae: 0.0228\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0026 - mae: 0.0360 - val_loss: 0.0013 - val_mae: 0.0284\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0026 - mae: 0.0362 - val_loss: 8.9940e-04 - val_mae: 0.0220\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0026 - mae: 0.0362 - val_loss: 9.6491e-04 - val_mae: 0.0232\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0025 - mae: 0.0357 - val_loss: 8.8832e-04 - val_mae: 0.0217\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0348 - val_loss: 9.4178e-04 - val_mae: 0.0229\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0349 - val_loss: 9.2025e-04 - val_mae: 0.0226\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0023 - mae: 0.0337 - val_loss: 8.2059e-04 - val_mae: 0.0211\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0344 - val_loss: 9.9881e-04 - val_mae: 0.0244\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0346 - val_loss: 0.0012 - val_mae: 0.0272\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0340 - val_loss: 8.4872e-04 - val_mae: 0.0214\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0022 - mae: 0.0329 - val_loss: 7.9549e-04 - val_mae: 0.0205\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0335 - val_loss: 0.0015 - val_mae: 0.0295\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0325 - val_loss: 8.7829e-04 - val_mae: 0.0209\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0323 - val_loss: 8.9556e-04 - val_mae: 0.0227\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0022 - mae: 0.0330 - val_loss: 7.8043e-04 - val_mae: 0.0207\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0321 - val_loss: 0.0012 - val_mae: 0.0265\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0021 - mae: 0.0325 - val_loss: 7.5122e-04 - val_mae: 0.0197\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0327 - val_loss: 9.4840e-04 - val_mae: 0.0225\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0322 - val_loss: 8.1564e-04 - val_mae: 0.0209\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0321 - val_loss: 8.6964e-04 - val_mae: 0.0225\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0318 - val_loss: 9.5667e-04 - val_mae: 0.0242\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0314 - val_loss: 7.5720e-04 - val_mae: 0.0203\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0321 - val_loss: 7.6363e-04 - val_mae: 0.0203\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0315 - val_loss: 8.6772e-04 - val_mae: 0.0225\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 7.5282e-04 - val_mae: 0.0194\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0312 - val_loss: 8.6436e-04 - val_mae: 0.0214\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0316 - val_loss: 7.5114e-04 - val_mae: 0.0194\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0315 - val_loss: 9.7814e-04 - val_mae: 0.0239\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0311 - val_loss: 7.2606e-04 - val_mae: 0.0185\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0320 - val_loss: 8.0594e-04 - val_mae: 0.0210\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0322 - val_loss: 8.0358e-04 - val_mae: 0.0196\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0309 - val_loss: 7.4059e-04 - val_mae: 0.0194\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 7.5571e-04 - val_mae: 0.0207\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 8.0491e-04 - val_mae: 0.0203\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0316 - val_loss: 8.6829e-04 - val_mae: 0.0224\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 6.9629e-04 - val_mae: 0.0185\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 8.7870e-04 - val_mae: 0.0215\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0306 - val_loss: 8.6077e-04 - val_mae: 0.0216\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 7.3772e-04 - val_mae: 0.0192\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0312 - val_loss: 6.9988e-04 - val_mae: 0.0185\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0019 - mae: 0.0308 - val_loss: 8.6720e-04 - val_mae: 0.0218\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 7.2425e-04 - val_mae: 0.0190\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0303 - val_loss: 8.0739e-04 - val_mae: 0.0209\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0309 - val_loss: 8.0182e-04 - val_mae: 0.0209\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0310 - val_loss: 7.8321e-04 - val_mae: 0.0204\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0308 - val_loss: 8.3131e-04 - val_mae: 0.0213\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 8.1192e-04 - val_mae: 0.0209\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 0.0010 - val_mae: 0.0235\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 6.9679e-04 - val_mae: 0.0190\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 7.1078e-04 - val_mae: 0.0188\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0018 - mae: 0.0299 - val_loss: 7.7962e-04 - val_mae: 0.0208\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0306 - val_loss: 7.4561e-04 - val_mae: 0.0195\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0295 - val_loss: 7.7086e-04 - val_mae: 0.0203\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0018 - mae: 0.0299 - val_loss: 6.6565e-04 - val_mae: 0.0186\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0293 - val_loss: 8.2684e-04 - val_mae: 0.0220\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0018 - mae: 0.0298 - val_loss: 0.0013 - val_mae: 0.0285\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 9.3685e-04 - val_mae: 0.0237\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 7.1773e-04 - val_mae: 0.0199\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0296 - val_loss: 8.9911e-04 - val_mae: 0.0214\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0294 - val_loss: 7.6854e-04 - val_mae: 0.0209\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0300 - val_loss: 8.5287e-04 - val_mae: 0.0214\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0302 - val_loss: 8.4699e-04 - val_mae: 0.0222\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 7.1390e-04 - val_mae: 0.0198\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0298 - val_loss: 7.5513e-04 - val_mae: 0.0202\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0296 - val_loss: 7.9904e-04 - val_mae: 0.0205\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 8.5854e-04 - val_mae: 0.0205\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 7.4370e-04 - val_mae: 0.0196\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0293 - val_loss: 7.4370e-04 - val_mae: 0.0193\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 7.2264e-04 - val_mae: 0.0191\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 9.8388e-04 - val_mae: 0.0235\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 7.8953e-04 - val_mae: 0.0198\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0289 - val_loss: 8.0544e-04 - val_mae: 0.0204\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 7.9083e-04 - val_mae: 0.0202\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 7.6312e-04 - val_mae: 0.0202\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0289 - val_loss: 7.5814e-04 - val_mae: 0.0201\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 8.5744e-04 - val_mae: 0.0214\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 7.3611e-04 - val_mae: 0.0198\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 8.7179e-04 - val_mae: 0.0229\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 7.9605e-04 - val_mae: 0.0206\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 8.6844e-04 - val_mae: 0.0222\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0289 - val_loss: 7.6976e-04 - val_mae: 0.0196\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0289 - val_loss: 7.5846e-04 - val_mae: 0.0195\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 7.6989e-04 - val_mae: 0.0201\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 7.1824e-04 - val_mae: 0.0186\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 7.5634e-04 - val_mae: 0.0196\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 7.2341e-04 - val_mae: 0.0197\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0287 - val_loss: 8.0563e-04 - val_mae: 0.0204\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 8.4059e-04 - val_mae: 0.0209\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 7.2786e-04 - val_mae: 0.0196\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 7.1600e-04 - val_mae: 0.0195\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 8.2106e-04 - val_mae: 0.0205\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.6741e-04 - val_mae: 0.0202\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 7.8090e-04 - val_mae: 0.0206\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 7.0475e-04 - val_mae: 0.0187\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 0.0010 - val_mae: 0.0250\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 7.0807e-04 - val_mae: 0.0190\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 7.3636e-04 - val_mae: 0.0195\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.0542e-04 - val_mae: 0.0187\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 7.7008e-04 - val_mae: 0.0195\n",
      "Epoch 117/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.9860e-04 - val_mae: 0.0202\n",
      "Epoch 118/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 7.9580e-04 - val_mae: 0.0205\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 7.7175e-04 - val_mae: 0.0203\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.6365e-04 - val_mae: 0.0198\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 8.8329e-04 - val_mae: 0.0218\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 7.3877e-04 - val_mae: 0.0194\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 8.1677e-04 - val_mae: 0.0204\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 8.8620e-04 - val_mae: 0.0230\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 7.8148e-04 - val_mae: 0.0212\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 8.1870e-04 - val_mae: 0.0215\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 7.5653e-04 - val_mae: 0.0193\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 7.5188e-04 - val_mae: 0.0193\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 7.6331e-04 - val_mae: 0.0203\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0279 - val_loss: 8.8493e-04 - val_mae: 0.0215\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 0.0010 - val_mae: 0.0241\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 8.9763e-04 - val_mae: 0.0210\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 8.2356e-04 - val_mae: 0.0199\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0286 - val_loss: 6.8807e-04 - val_mae: 0.0187\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 7.9138e-04 - val_mae: 0.0200\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 7.8524e-04 - val_mae: 0.0193\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 8.1196e-04 - val_mae: 0.0212\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0276 - val_loss: 7.8225e-04 - val_mae: 0.0207\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 7.3939e-04 - val_mae: 0.0198\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 7.8387e-04 - val_mae: 0.0201\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 8.7912e-04 - val_mae: 0.0213\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 7.5780e-04 - val_mae: 0.0198\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.9792e-04 - val_mae: 0.0204\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 8.2278e-04 - val_mae: 0.0202\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 7.6322e-04 - val_mae: 0.0194\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 8.8231e-04 - val_mae: 0.0205\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.6386e-04 - val_mae: 0.0194\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 8.0544e-04 - val_mae: 0.0200\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 7.0468e-04 - val_mae: 0.0183\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 9.3577e-04 - val_mae: 0.0227\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 9.2337e-04 - val_mae: 0.0214\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 9.5924e-04 - val_mae: 0.0234\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 0.0014 - val_mae: 0.0293\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 7.5074e-04 - val_mae: 0.0193\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 6.9919e-04 - val_mae: 0.0183\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 7.0424e-04 - val_mae: 0.0189\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 8.2517e-04 - val_mae: 0.0204\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 8.0714e-04 - val_mae: 0.0205\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.2420e-04 - val_mae: 0.0186\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 8.0537e-04 - val_mae: 0.0206\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0270 - val_loss: 8.5042e-04 - val_mae: 0.0217\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.4208e-04 - val_mae: 0.0201\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 7.1223e-04 - val_mae: 0.0192\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 7.6394e-04 - val_mae: 0.0194\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 7.6218e-04 - val_mae: 0.0189\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 7.6643e-04 - val_mae: 0.0198\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 7.2839e-04 - val_mae: 0.0190\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 7.7493e-04 - val_mae: 0.0205\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 7.6375e-04 - val_mae: 0.0189\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 7.3289e-04 - val_mae: 0.0196\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 7.7810e-04 - val_mae: 0.0195\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.3367e-04 - val_mae: 0.0208\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 9.5072e-04 - val_mae: 0.0221\n",
      "Epoch 174/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 7.6632e-04 - val_mae: 0.0199\n",
      "Epoch 175/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.1473e-04 - val_mae: 0.0205\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 9.7365e-04 - val_mae: 0.0237\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 7.8817e-04 - val_mae: 0.0200\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 8.1443e-04 - val_mae: 0.0210\n",
      "Epoch 179/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 8.0422e-04 - val_mae: 0.0199\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.3764e-04 - val_mae: 0.0212\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 7.2208e-04 - val_mae: 0.0189\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 8.6961e-04 - val_mae: 0.0212\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 7.8018e-04 - val_mae: 0.0203\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 7.4548e-04 - val_mae: 0.0201\n",
      "Epoch 185/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 7.7088e-04 - val_mae: 0.0207\n",
      "Epoch 186/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 7.8265e-04 - val_mae: 0.0201\n",
      "Epoch 187/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 7.5644e-04 - val_mae: 0.0192\n",
      "Epoch 188/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 7.7397e-04 - val_mae: 0.0193\n",
      "Epoch 189/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 9.3574e-04 - val_mae: 0.0228\n",
      "Epoch 190/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0262 - val_loss: 7.8691e-04 - val_mae: 0.0199\n",
      "Epoch 191/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 7.9892e-04 - val_mae: 0.0202\n",
      "Epoch 192/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 8.2565e-04 - val_mae: 0.0209\n",
      "Epoch 193/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 7.5711e-04 - val_mae: 0.0193\n",
      "Epoch 194/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 8.1702e-04 - val_mae: 0.0200\n",
      "Epoch 195/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 7.8006e-04 - val_mae: 0.0201\n",
      "Epoch 196/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 9.0326e-04 - val_mae: 0.0222\n",
      "Epoch 197/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0262 - val_loss: 8.3971e-04 - val_mae: 0.0213\n",
      "Epoch 198/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 0.0011 - val_mae: 0.0233\n",
      "Epoch 199/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 8.0593e-04 - val_mae: 0.0204\n",
      "Epoch 200/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 7.7159e-04 - val_mae: 0.0198\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error is: 12.074794783720996\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('LSTM')\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_lstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_lstm.load_weights(filepath_attention)\n",
    "preds = atten_lstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "\n",
    "wk.save(f'LSTM Result.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_cnnlstm.hdf5'\n",
    "filepath_attention = 'attention_cnnlstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "simple_cnnlstm = keras.Sequential()\n",
    "simple_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "simple_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3))\n",
    "simple_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_cnnlstm.add(keras.layers.Flatten())\n",
    "simple_cnnlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(32))\n",
    "simple_cnnlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_cnnlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0290 - mae: 0.1144 - val_loss: 0.0026 - val_mae: 0.0431\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0050 - mae: 0.0514 - val_loss: 0.0018 - val_mae: 0.0327\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0043 - mae: 0.0472 - val_loss: 0.0015 - val_mae: 0.0293\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0040 - mae: 0.0450 - val_loss: 0.0014 - val_mae: 0.0283\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0036 - mae: 0.0425 - val_loss: 0.0014 - val_mae: 0.0290\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0032 - mae: 0.0401 - val_loss: 0.0013 - val_mae: 0.0267\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0030 - mae: 0.0383 - val_loss: 0.0011 - val_mae: 0.0248\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0027 - mae: 0.0359 - val_loss: 0.0011 - val_mae: 0.0250\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0025 - mae: 0.0349 - val_loss: 0.0011 - val_mae: 0.0242\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0342 - val_loss: 0.0010 - val_mae: 0.0237\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0023 - mae: 0.0328 - val_loss: 0.0010 - val_mae: 0.0240\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0022 - mae: 0.0324 - val_loss: 0.0011 - val_mae: 0.0243\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0022 - mae: 0.0318 - val_loss: 9.3729e-04 - val_mae: 0.0222\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0021 - mae: 0.0313 - val_loss: 9.5543e-04 - val_mae: 0.0225\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0020 - mae: 0.0310 - val_loss: 8.9605e-04 - val_mae: 0.0217\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0020 - mae: 0.0299 - val_loss: 9.3509e-04 - val_mae: 0.0227\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0019 - mae: 0.0295 - val_loss: 8.9650e-04 - val_mae: 0.0222\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0019 - mae: 0.0293 - val_loss: 8.6948e-04 - val_mae: 0.0216\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0018 - mae: 0.0288 - val_loss: 8.4570e-04 - val_mae: 0.0210\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0018 - mae: 0.0289 - val_loss: 8.2495e-04 - val_mae: 0.0211\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0281 - val_loss: 8.2581e-04 - val_mae: 0.0213\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0272 - val_loss: 8.6144e-04 - val_mae: 0.0220\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0277 - val_loss: 8.8174e-04 - val_mae: 0.0213\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0268 - val_loss: 9.1643e-04 - val_mae: 0.0218\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0276 - val_loss: 7.4636e-04 - val_mae: 0.0195\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0268 - val_loss: 7.6162e-04 - val_mae: 0.0198\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0268 - val_loss: 7.5151e-04 - val_mae: 0.0199\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0267 - val_loss: 7.7743e-04 - val_mae: 0.0199\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0268 - val_loss: 8.0337e-04 - val_mae: 0.0201\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0261 - val_loss: 7.4538e-04 - val_mae: 0.0199\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 7.1038e-04 - val_mae: 0.0190\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0267 - val_loss: 7.1519e-04 - val_mae: 0.0193\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0258 - val_loss: 7.5964e-04 - val_mae: 0.0208\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0260 - val_loss: 7.7995e-04 - val_mae: 0.0207\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0264 - val_loss: 7.1891e-04 - val_mae: 0.0192\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0254 - val_loss: 7.3507e-04 - val_mae: 0.0195\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0256 - val_loss: 8.4812e-04 - val_mae: 0.0215\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0254 - val_loss: 7.7600e-04 - val_mae: 0.0197\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0254 - val_loss: 7.4934e-04 - val_mae: 0.0193\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0255 - val_loss: 7.2135e-04 - val_mae: 0.0195\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 7.4160e-04 - val_mae: 0.0199\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0255 - val_loss: 7.9849e-04 - val_mae: 0.0203\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0256 - val_loss: 7.2718e-04 - val_mae: 0.0191\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0251 - val_loss: 7.2685e-04 - val_mae: 0.0191\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0251 - val_loss: 7.0194e-04 - val_mae: 0.0189\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0255 - val_loss: 7.3679e-04 - val_mae: 0.0195\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0251 - val_loss: 7.5157e-04 - val_mae: 0.0193\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0249 - val_loss: 7.6160e-04 - val_mae: 0.0194\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0251 - val_loss: 6.7660e-04 - val_mae: 0.0184\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0248 - val_loss: 8.0986e-04 - val_mae: 0.0213\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0245 - val_loss: 8.0170e-04 - val_mae: 0.0202\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 7.9425e-04 - val_mae: 0.0209\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0244 - val_loss: 7.1466e-04 - val_mae: 0.0187\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0244 - val_loss: 7.0502e-04 - val_mae: 0.0190\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0247 - val_loss: 7.2146e-04 - val_mae: 0.0188\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 7.1537e-04 - val_mae: 0.0190\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 7.5391e-04 - val_mae: 0.0196\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 7.9116e-04 - val_mae: 0.0202\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 8.3305e-04 - val_mae: 0.0215\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0245 - val_loss: 6.9406e-04 - val_mae: 0.0191\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 7.1312e-04 - val_mae: 0.0186\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 7.1749e-04 - val_mae: 0.0192\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 7.1631e-04 - val_mae: 0.0192\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0241 - val_loss: 7.3144e-04 - val_mae: 0.0194\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0242 - val_loss: 8.6567e-04 - val_mae: 0.0208\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0239 - val_loss: 7.0975e-04 - val_mae: 0.0193\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 7.5323e-04 - val_mae: 0.0203\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 1s 3ms/step - loss: 0.0013 - mae: 0.0238 - val_loss: 7.6576e-04 - val_mae: 0.0195\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0241 - val_loss: 6.9973e-04 - val_mae: 0.0186\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0241 - val_loss: 7.2529e-04 - val_mae: 0.0192\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0242 - val_loss: 9.3155e-04 - val_mae: 0.0226\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0241 - val_loss: 7.2414e-04 - val_mae: 0.0191\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0242 - val_loss: 8.7004e-04 - val_mae: 0.0212\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0234 - val_loss: 6.6770e-04 - val_mae: 0.0182\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0238 - val_loss: 8.7220e-04 - val_mae: 0.0206\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0240 - val_loss: 7.1456e-04 - val_mae: 0.0190\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0237 - val_loss: 7.0398e-04 - val_mae: 0.0187\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0238 - val_loss: 7.1263e-04 - val_mae: 0.0190\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0238 - val_loss: 6.9134e-04 - val_mae: 0.0185\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0239 - val_loss: 6.7512e-04 - val_mae: 0.0182\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 7.6279e-04 - val_mae: 0.0198\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0233 - val_loss: 6.7459e-04 - val_mae: 0.0180\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0239 - val_loss: 6.6597e-04 - val_mae: 0.0183\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0236 - val_loss: 7.1914e-04 - val_mae: 0.0195\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0240 - val_loss: 7.1030e-04 - val_mae: 0.0189\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 1s 3ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 7.5432e-04 - val_mae: 0.0192\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 1s 3ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 6.9135e-04 - val_mae: 0.0189\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0238 - val_loss: 7.1768e-04 - val_mae: 0.0191\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 7.1381e-04 - val_mae: 0.0185\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 7.6956e-04 - val_mae: 0.0200\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0237 - val_loss: 7.1236e-04 - val_mae: 0.0185\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0238 - val_loss: 7.2376e-04 - val_mae: 0.0189\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 7.0140e-04 - val_mae: 0.0184\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0230 - val_loss: 8.1069e-04 - val_mae: 0.0211\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0237 - val_loss: 7.4498e-04 - val_mae: 0.0192\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 6.8498e-04 - val_mae: 0.0184\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 7.0486e-04 - val_mae: 0.0186\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 7.7034e-04 - val_mae: 0.0200\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0228 - val_loss: 6.8691e-04 - val_mae: 0.0185\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 6.6163e-04 - val_mae: 0.0178\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0227 - val_loss: 7.0084e-04 - val_mae: 0.0187\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0229 - val_loss: 7.2922e-04 - val_mae: 0.0188\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 7.0485e-04 - val_mae: 0.0192\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0228 - val_loss: 7.4373e-04 - val_mae: 0.0191\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0229 - val_loss: 7.8459e-04 - val_mae: 0.0199\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0228 - val_loss: 7.1942e-04 - val_mae: 0.0190\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 7.6088e-04 - val_mae: 0.0194\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 6.8395e-04 - val_mae: 0.0181\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 7.1835e-04 - val_mae: 0.0185\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0227 - val_loss: 7.5407e-04 - val_mae: 0.0195\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0226 - val_loss: 7.2771e-04 - val_mae: 0.0188\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0227 - val_loss: 7.1641e-04 - val_mae: 0.0187\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0226 - val_loss: 7.1382e-04 - val_mae: 0.0189\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0230 - val_loss: 7.2610e-04 - val_mae: 0.0187\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0227 - val_loss: 7.0257e-04 - val_mae: 0.0184\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 9.3002e-04 - val_mae: 0.0216\n",
      "Epoch 117/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0225 - val_loss: 9.2068e-04 - val_mae: 0.0235\n",
      "Epoch 118/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 6.7118e-04 - val_mae: 0.0180\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 7.6007e-04 - val_mae: 0.0193\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0226 - val_loss: 7.5453e-04 - val_mae: 0.0196\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0228 - val_loss: 6.8635e-04 - val_mae: 0.0187\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0225 - val_loss: 6.7151e-04 - val_mae: 0.0180\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0224 - val_loss: 7.2013e-04 - val_mae: 0.0189\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0229 - val_loss: 6.9614e-04 - val_mae: 0.0191\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0229 - val_loss: 6.7366e-04 - val_mae: 0.0185\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.8674e-04 - val_mae: 0.0181\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 7.5835e-04 - val_mae: 0.0193\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0227 - val_loss: 6.9868e-04 - val_mae: 0.0182\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 8.2609e-04 - val_mae: 0.0205\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.8737e-04 - val_mae: 0.0182\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0221 - val_loss: 6.7977e-04 - val_mae: 0.0180\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 8.0452e-04 - val_mae: 0.0209\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 7.2953e-04 - val_mae: 0.0191\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0221 - val_loss: 7.1811e-04 - val_mae: 0.0192\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.9784e-04 - val_mae: 0.0185\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 7.1736e-04 - val_mae: 0.0189\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0221 - val_loss: 6.8741e-04 - val_mae: 0.0182\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0220 - val_loss: 7.1335e-04 - val_mae: 0.0192\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 8.2035e-04 - val_mae: 0.0200\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0219 - val_loss: 7.4738e-04 - val_mae: 0.0189\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0221 - val_loss: 7.0970e-04 - val_mae: 0.0186\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 6.9616e-04 - val_mae: 0.0183\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 6.9672e-04 - val_mae: 0.0185\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 6.8558e-04 - val_mae: 0.0180\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 8.1875e-04 - val_mae: 0.0212\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 6.8983e-04 - val_mae: 0.0185\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0219 - val_loss: 7.0508e-04 - val_mae: 0.0185\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0216 - val_loss: 6.8480e-04 - val_mae: 0.0185\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0218 - val_loss: 7.5957e-04 - val_mae: 0.0190\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 7.4440e-04 - val_mae: 0.0190\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0217 - val_loss: 7.4719e-04 - val_mae: 0.0193\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0220 - val_loss: 6.7402e-04 - val_mae: 0.0180\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0216 - val_loss: 7.0674e-04 - val_mae: 0.0183\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0219 - val_loss: 8.3623e-04 - val_mae: 0.0205\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0218 - val_loss: 7.4415e-04 - val_mae: 0.0186\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0217 - val_loss: 6.7283e-04 - val_mae: 0.0181\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0217 - val_loss: 7.2045e-04 - val_mae: 0.0186\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0219 - val_loss: 7.8071e-04 - val_mae: 0.0200\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0216 - val_loss: 7.2201e-04 - val_mae: 0.0185\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0215 - val_loss: 8.0079e-04 - val_mae: 0.0203\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0218 - val_loss: 7.3339e-04 - val_mae: 0.0188\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0216 - val_loss: 8.0269e-04 - val_mae: 0.0199\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0218 - val_loss: 7.1676e-04 - val_mae: 0.0186\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0214 - val_loss: 6.7842e-04 - val_mae: 0.0179\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0217 - val_loss: 7.8099e-04 - val_mae: 0.0191\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0214 - val_loss: 7.0642e-04 - val_mae: 0.0183\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0216 - val_loss: 7.4357e-04 - val_mae: 0.0190\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0217 - val_loss: 8.3580e-04 - val_mae: 0.0200\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0215 - val_loss: 7.2733e-04 - val_mae: 0.0186\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0213 - val_loss: 7.3268e-04 - val_mae: 0.0188\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0218 - val_loss: 7.3030e-04 - val_mae: 0.0187\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0216 - val_loss: 8.1057e-04 - val_mae: 0.0211\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0216 - val_loss: 7.3047e-04 - val_mae: 0.0187\n",
      "Epoch 174/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0218 - val_loss: 7.2943e-04 - val_mae: 0.0187\n",
      "Epoch 175/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0213 - val_loss: 7.3824e-04 - val_mae: 0.0186\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0214 - val_loss: 7.0663e-04 - val_mae: 0.0182\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0212 - val_loss: 7.6845e-04 - val_mae: 0.0189\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0213 - val_loss: 7.8524e-04 - val_mae: 0.0193\n",
      "Epoch 179/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0211 - val_loss: 7.9020e-04 - val_mae: 0.0196\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0211 - val_loss: 6.9525e-04 - val_mae: 0.0184\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0215 - val_loss: 8.5844e-04 - val_mae: 0.0214\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0214 - val_loss: 7.4552e-04 - val_mae: 0.0185\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0215 - val_loss: 7.2184e-04 - val_mae: 0.0187\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 1s 3ms/step - loss: 0.0010 - mae: 0.0214 - val_loss: 7.2700e-04 - val_mae: 0.0187\n",
      "Epoch 185/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0210 - val_loss: 7.4182e-04 - val_mae: 0.0186\n",
      "Epoch 186/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0213 - val_loss: 7.9382e-04 - val_mae: 0.0208\n",
      "Epoch 187/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0213 - val_loss: 7.0829e-04 - val_mae: 0.0185\n",
      "Epoch 188/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0211 - val_loss: 6.9606e-04 - val_mae: 0.0182\n",
      "Epoch 189/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0214 - val_loss: 7.0265e-04 - val_mae: 0.0182\n",
      "Epoch 190/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0213 - val_loss: 7.3220e-04 - val_mae: 0.0184\n",
      "Epoch 191/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0210 - val_loss: 7.4532e-04 - val_mae: 0.0187\n",
      "Epoch 192/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0212 - val_loss: 8.0817e-04 - val_mae: 0.0204\n",
      "Epoch 193/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0211 - val_loss: 8.0440e-04 - val_mae: 0.0197\n",
      "Epoch 194/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0210 - val_loss: 7.2165e-04 - val_mae: 0.0184\n",
      "Epoch 195/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0215 - val_loss: 7.9090e-04 - val_mae: 0.0195\n",
      "Epoch 196/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 9.9968e-04 - mae: 0.0208 - val_loss: 7.7101e-04 - val_mae: 0.0198\n",
      "Epoch 197/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0209 - val_loss: 7.4720e-04 - val_mae: 0.0193\n",
      "Epoch 198/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 9.9952e-04 - mae: 0.0209 - val_loss: 7.3742e-04 - val_mae: 0.0186\n",
      "Epoch 199/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 9.8418e-04 - mae: 0.0208 - val_loss: 7.0637e-04 - val_mae: 0.0183\n",
      "Epoch 200/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0211 - val_loss: 8.0384e-04 - val_mae: 0.0195\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error is: 11.704137796827624\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('CNN-LSTM')\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_cnnlstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_cnnlstm.load_weights(filepath_simple)\n",
    "preds = simple_cnnlstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention model\n",
    "\n",
    "K.clear_session()\n",
    "atten_cnnlstm = keras.Sequential()\n",
    "atten_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "atten_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3))\n",
    "atten_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_cnnlstm.add(attention(return_sequences=True))\n",
    "atten_cnnlstm.add(keras.layers.Flatten())\n",
    "atten_cnnlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(32))\n",
    "atten_cnnlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_cnnlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0331 - mae: 0.1249 - val_loss: 0.0025 - val_mae: 0.0396\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0053 - mae: 0.0532 - val_loss: 0.0018 - val_mae: 0.0325\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0045 - mae: 0.0478 - val_loss: 0.0016 - val_mae: 0.0305\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0041 - mae: 0.0459 - val_loss: 0.0016 - val_mae: 0.0320\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0038 - mae: 0.0440 - val_loss: 0.0014 - val_mae: 0.0286\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0035 - mae: 0.0424 - val_loss: 0.0013 - val_mae: 0.0278\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0033 - mae: 0.0405 - val_loss: 0.0013 - val_mae: 0.0270\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0030 - mae: 0.0391 - val_loss: 0.0012 - val_mae: 0.0263\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0028 - mae: 0.0374 - val_loss: 0.0012 - val_mae: 0.0259\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0027 - mae: 0.0363 - val_loss: 0.0011 - val_mae: 0.0247\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0025 - mae: 0.0349 - val_loss: 0.0011 - val_mae: 0.0251\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0345 - val_loss: 0.0010 - val_mae: 0.0239\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0024 - mae: 0.0338 - val_loss: 0.0012 - val_mae: 0.0266\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0023 - mae: 0.0331 - val_loss: 9.8688e-04 - val_mae: 0.0229\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0022 - mae: 0.0324 - val_loss: 9.8975e-04 - val_mae: 0.0232\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0022 - mae: 0.0323 - val_loss: 0.0010 - val_mae: 0.0234\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0021 - mae: 0.0316 - val_loss: 9.7558e-04 - val_mae: 0.0232\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0021 - mae: 0.0308 - val_loss: 9.3164e-04 - val_mae: 0.0223\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0020 - mae: 0.0308 - val_loss: 0.0011 - val_mae: 0.0241\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0020 - mae: 0.0300 - val_loss: 9.1586e-04 - val_mae: 0.0221\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0019 - mae: 0.0294 - val_loss: 9.0216e-04 - val_mae: 0.0217\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0019 - mae: 0.0293 - val_loss: 0.0010 - val_mae: 0.0234\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0019 - mae: 0.0298 - val_loss: 9.3300e-04 - val_mae: 0.0229\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0019 - mae: 0.0293 - val_loss: 9.5516e-04 - val_mae: 0.0223\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0018 - mae: 0.0284 - val_loss: 8.4022e-04 - val_mae: 0.0210\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0018 - mae: 0.0288 - val_loss: 8.4958e-04 - val_mae: 0.0211\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0018 - mae: 0.0284 - val_loss: 8.4229e-04 - val_mae: 0.0208\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0279 - val_loss: 0.0010 - val_mae: 0.0229\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0278 - val_loss: 8.4951e-04 - val_mae: 0.0209\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0275 - val_loss: 8.5816e-04 - val_mae: 0.0210\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0274 - val_loss: 7.8174e-04 - val_mae: 0.0202\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0282 - val_loss: 8.7780e-04 - val_mae: 0.0213\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0017 - mae: 0.0275 - val_loss: 8.5377e-04 - val_mae: 0.0215\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0271 - val_loss: 7.7062e-04 - val_mae: 0.0198\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0265 - val_loss: 7.8064e-04 - val_mae: 0.0197\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0270 - val_loss: 8.7481e-04 - val_mae: 0.0212\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0263 - val_loss: 7.4761e-04 - val_mae: 0.0198\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0263 - val_loss: 7.4057e-04 - val_mae: 0.0197\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0265 - val_loss: 8.2908e-04 - val_mae: 0.0205\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0264 - val_loss: 7.5258e-04 - val_mae: 0.0197\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0269 - val_loss: 8.1419e-04 - val_mae: 0.0208\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 7.4926e-04 - val_mae: 0.0195\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0016 - mae: 0.0265 - val_loss: 7.2793e-04 - val_mae: 0.0193\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0259 - val_loss: 7.2482e-04 - val_mae: 0.0192\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0261 - val_loss: 7.8842e-04 - val_mae: 0.0197\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0261 - val_loss: 7.9410e-04 - val_mae: 0.0200\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0260 - val_loss: 7.0684e-04 - val_mae: 0.0190\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0263 - val_loss: 7.0975e-04 - val_mae: 0.0189\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0259 - val_loss: 8.5516e-04 - val_mae: 0.0209\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 7.0201e-04 - val_mae: 0.0188\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 7.3167e-04 - val_mae: 0.0193\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0258 - val_loss: 9.1304e-04 - val_mae: 0.0215\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0255 - val_loss: 8.5950e-04 - val_mae: 0.0209\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0254 - val_loss: 7.5386e-04 - val_mae: 0.0195\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0259 - val_loss: 7.7870e-04 - val_mae: 0.0206\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0261 - val_loss: 8.1956e-04 - val_mae: 0.0204\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0255 - val_loss: 7.4049e-04 - val_mae: 0.0195\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0257 - val_loss: 9.3446e-04 - val_mae: 0.0225\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0255 - val_loss: 8.1228e-04 - val_mae: 0.0216\n",
      "Epoch 60/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0256 - val_loss: 8.0615e-04 - val_mae: 0.0201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0256 - val_loss: 7.0103e-04 - val_mae: 0.0189\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0257 - val_loss: 7.4426e-04 - val_mae: 0.0194\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0257 - val_loss: 7.6743e-04 - val_mae: 0.0201\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0251 - val_loss: 8.3656e-04 - val_mae: 0.0216\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0254 - val_loss: 8.5244e-04 - val_mae: 0.0207\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0251 - val_loss: 7.2500e-04 - val_mae: 0.0190\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0015 - mae: 0.0256 - val_loss: 8.7442e-04 - val_mae: 0.0215\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0256 - val_loss: 7.0826e-04 - val_mae: 0.0188\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0248 - val_loss: 7.2113e-04 - val_mae: 0.0189\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0249 - val_loss: 7.1157e-04 - val_mae: 0.0190\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0252 - val_loss: 9.2877e-04 - val_mae: 0.0218\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0252 - val_loss: 7.0376e-04 - val_mae: 0.0186\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0247 - val_loss: 7.1191e-04 - val_mae: 0.0189\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 7.7036e-04 - val_mae: 0.0203\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0248 - val_loss: 7.4326e-04 - val_mae: 0.0194\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0252 - val_loss: 8.0517e-04 - val_mae: 0.0206\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0249 - val_loss: 8.2798e-04 - val_mae: 0.0208\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 8.4883e-04 - val_mae: 0.0216\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 7.1237e-04 - val_mae: 0.0191\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 7.4838e-04 - val_mae: 0.0189\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0242 - val_loss: 7.3419e-04 - val_mae: 0.0193\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0245 - val_loss: 7.4048e-04 - val_mae: 0.0190\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 7.6651e-04 - val_mae: 0.0193\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0247 - val_loss: 7.5906e-04 - val_mae: 0.0199\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 7.0253e-04 - val_mae: 0.0187\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 7.0154e-04 - val_mae: 0.0188\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 7.3459e-04 - val_mae: 0.0191\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0241 - val_loss: 6.9765e-04 - val_mae: 0.0185\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 7.3160e-04 - val_mae: 0.0197\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 7.2478e-04 - val_mae: 0.0188\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 8.9525e-04 - val_mae: 0.0218\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0249 - val_loss: 7.3506e-04 - val_mae: 0.0191\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 7.0792e-04 - val_mae: 0.0188\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0239 - val_loss: 7.7335e-04 - val_mae: 0.0200\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0240 - val_loss: 7.3769e-04 - val_mae: 0.0192\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0245 - val_loss: 7.9855e-04 - val_mae: 0.0204\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0241 - val_loss: 8.5670e-04 - val_mae: 0.0219\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0241 - val_loss: 7.2420e-04 - val_mae: 0.0194\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 7.1170e-04 - val_mae: 0.0186\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0239 - val_loss: 7.2054e-04 - val_mae: 0.0194\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 7.1371e-04 - val_mae: 0.0189\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0238 - val_loss: 7.3095e-04 - val_mae: 0.0194\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0240 - val_loss: 7.3546e-04 - val_mae: 0.0192\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0239 - val_loss: 7.1095e-04 - val_mae: 0.0189\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0241 - val_loss: 7.2403e-04 - val_mae: 0.0188\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0240 - val_loss: 7.0599e-04 - val_mae: 0.0188\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0236 - val_loss: 7.1515e-04 - val_mae: 0.0190\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0240 - val_loss: 7.4424e-04 - val_mae: 0.0194\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0236 - val_loss: 7.4032e-04 - val_mae: 0.0192\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0238 - val_loss: 7.3382e-04 - val_mae: 0.0193\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0236 - val_loss: 7.5175e-04 - val_mae: 0.0199\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0236 - val_loss: 7.2426e-04 - val_mae: 0.0188\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0237 - val_loss: 7.3346e-04 - val_mae: 0.0196\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0242 - val_loss: 7.2116e-04 - val_mae: 0.0194\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0236 - val_loss: 7.8667e-04 - val_mae: 0.0193\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0234 - val_loss: 9.3695e-04 - val_mae: 0.0223\n",
      "Epoch 117/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0237 - val_loss: 7.5873e-04 - val_mae: 0.0197\n",
      "Epoch 118/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 7.0625e-04 - val_mae: 0.0190\n",
      "Epoch 119/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0237 - val_loss: 6.9127e-04 - val_mae: 0.0186\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0235 - val_loss: 7.8367e-04 - val_mae: 0.0208\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 8.4555e-04 - val_mae: 0.0225\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0234 - val_loss: 7.0921e-04 - val_mae: 0.0186\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 7.0481e-04 - val_mae: 0.0188\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 7.5055e-04 - val_mae: 0.0193\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0235 - val_loss: 7.5368e-04 - val_mae: 0.0195\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 7.1637e-04 - val_mae: 0.0185\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 7.0457e-04 - val_mae: 0.0186\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 7.2811e-04 - val_mae: 0.0195\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 7.7816e-04 - val_mae: 0.0202\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0013 - mae: 0.0236 - val_loss: 7.4967e-04 - val_mae: 0.0195\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 7.0488e-04 - val_mae: 0.0187\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 7.0932e-04 - val_mae: 0.0188\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0229 - val_loss: 7.4142e-04 - val_mae: 0.0193\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 8.4680e-04 - val_mae: 0.0213\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 6.8440e-04 - val_mae: 0.0184\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0229 - val_loss: 6.9737e-04 - val_mae: 0.0184\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 6.9699e-04 - val_mae: 0.0184\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 7.9145e-04 - val_mae: 0.0199\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 7.0964e-04 - val_mae: 0.0185\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0228 - val_loss: 7.0457e-04 - val_mae: 0.0189\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 6.9709e-04 - val_mae: 0.0185\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0226 - val_loss: 6.9026e-04 - val_mae: 0.0183\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0225 - val_loss: 7.9033e-04 - val_mae: 0.0205\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0230 - val_loss: 7.5552e-04 - val_mae: 0.0198\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 7.4113e-04 - val_mae: 0.0190\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 7.6529e-04 - val_mae: 0.0194\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0227 - val_loss: 8.3926e-04 - val_mae: 0.0205\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 8.2381e-04 - val_mae: 0.0199\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0229 - val_loss: 8.3682e-04 - val_mae: 0.0208\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0224 - val_loss: 7.8937e-04 - val_mae: 0.0206\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0228 - val_loss: 8.4765e-04 - val_mae: 0.0208\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0227 - val_loss: 7.2507e-04 - val_mae: 0.0188\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0226 - val_loss: 7.5913e-04 - val_mae: 0.0190\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0228 - val_loss: 8.5861e-04 - val_mae: 0.0221\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0228 - val_loss: 8.0672e-04 - val_mae: 0.0196\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0223 - val_loss: 7.5204e-04 - val_mae: 0.0193\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0228 - val_loss: 7.3764e-04 - val_mae: 0.0190\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 8.0345e-04 - val_mae: 0.0201\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0226 - val_loss: 7.2226e-04 - val_mae: 0.0189\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0225 - val_loss: 8.0500e-04 - val_mae: 0.0202\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0225 - val_loss: 7.1353e-04 - val_mae: 0.0187\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0225 - val_loss: 7.1726e-04 - val_mae: 0.0187\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 7.0761e-04 - val_mae: 0.0188\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 7.4530e-04 - val_mae: 0.0187\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0226 - val_loss: 7.0909e-04 - val_mae: 0.0186\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0229 - val_loss: 7.9212e-04 - val_mae: 0.0201\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 7.2282e-04 - val_mae: 0.0187\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0226 - val_loss: 7.1116e-04 - val_mae: 0.0185\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 7.5825e-04 - val_mae: 0.0193\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 7.2095e-04 - val_mae: 0.0184\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0012 - mae: 0.0224 - val_loss: 8.4068e-04 - val_mae: 0.0201\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 8.3932e-04 - val_mae: 0.0214\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 7.4460e-04 - val_mae: 0.0197\n",
      "Epoch 174/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 6.8295e-04 - val_mae: 0.0182\n",
      "Epoch 175/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 6.9084e-04 - val_mae: 0.0183\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0220 - val_loss: 8.1756e-04 - val_mae: 0.0198\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 7.3399e-04 - val_mae: 0.0195\n",
      "Epoch 178/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 7.6837e-04 - val_mae: 0.0196\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 7.4121e-04 - val_mae: 0.0188\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0219 - val_loss: 7.3238e-04 - val_mae: 0.0186\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0221 - val_loss: 8.2951e-04 - val_mae: 0.0213\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 7.7512e-04 - val_mae: 0.0204\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 7.2688e-04 - val_mae: 0.0185\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 7.0203e-04 - val_mae: 0.0183\n",
      "Epoch 185/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 8.0539e-04 - val_mae: 0.0209\n",
      "Epoch 186/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0221 - val_loss: 8.1576e-04 - val_mae: 0.0207\n",
      "Epoch 187/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0219 - val_loss: 7.4296e-04 - val_mae: 0.0193\n",
      "Epoch 188/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0218 - val_loss: 7.7383e-04 - val_mae: 0.0192\n",
      "Epoch 189/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0219 - val_loss: 7.4192e-04 - val_mae: 0.0190\n",
      "Epoch 190/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 7.5657e-04 - val_mae: 0.0197\n",
      "Epoch 191/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 7.0732e-04 - val_mae: 0.0185\n",
      "Epoch 192/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0220 - val_loss: 7.5235e-04 - val_mae: 0.0191\n",
      "Epoch 193/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 8.0585e-04 - val_mae: 0.0203\n",
      "Epoch 194/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 7.5009e-04 - val_mae: 0.0192\n",
      "Epoch 195/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0217 - val_loss: 7.2348e-04 - val_mae: 0.0189\n",
      "Epoch 196/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0219 - val_loss: 7.4718e-04 - val_mae: 0.0191\n",
      "Epoch 197/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 7.7903e-04 - val_mae: 0.0199\n",
      "Epoch 198/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0221 - val_loss: 7.1718e-04 - val_mae: 0.0188\n",
      "Epoch 199/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0220 - val_loss: 7.5711e-04 - val_mae: 0.0191\n",
      "Epoch 200/200\n",
      "245/245 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0219 - val_loss: 7.4920e-04 - val_mae: 0.0194\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error is: 11.669131477071934\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('CNN-LSTM')\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_cnnlstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_cnnlstm.load_weights(filepath_attention)\n",
    "preds = atten_cnnlstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('CNN-LStM Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_convlstm.hdf5'\n",
    "filepath_attention = 'attention_convlstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_conv =x_train.reshape(x_train.shape[0], 1, 1, x_train.shape[1], x_train.shape[2])\n",
    "x_test_conv = x_test.reshape(x_test.shape[0], 1, 1, x_test.shape[1], x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 3 from 2 for '{{node conv_lst_m2d_2/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv_lst_m2d_2/Sum, conv_lst_m2d_2/zeros)' with input shapes: [?,1,2,64], [1,3,64,64].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1811\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 3 from 2 for '{{node conv_lst_m2d_2/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv_lst_m2d_2/Sum, conv_lst_m2d_2/zeros)' with input shapes: [?,1,2,64], [1,3,64,64].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-37cb265ca5dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                          x_train_conv.shape[3], x_train_conv.shape[4])))\n\u001b[0;32m      6\u001b[0m \u001b[0msimple_convlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConvLSTM2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msimple_convlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConvLSTM2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0msimple_convlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConvLSTM2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msimple_convlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    219\u001b[0m       \u001b[1;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m       \u001b[1;31m# refresh its output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m       \u001b[0moutput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m--> 926\u001b[1;33m                                                 input_list)\n\u001b[0m\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1115\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1117\u001b[1;33m               \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional_recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    883\u001b[0m                                         \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m                                         \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m                                         initial_state=initial_state)\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional_recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;31m# self.input_spec and self.state_spec with complete input shapes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     inputs, initial_state, constants = self._process_inputs(\n\u001b[1;32m--> 303\u001b[1;33m         inputs, initial_state, constants)\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional_recurrent.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    285\u001b[0m                                          array_ops.zeros(tuple(shape),\n\u001b[0;32m    286\u001b[0m                                                          initial_state.dtype),\n\u001b[1;32m--> 287\u001b[1;33m                                          padding=self.cell.padding)\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional_recurrent.py\u001b[0m in \u001b[0;36minput_conv\u001b[1;34m(self, x, w, b, padding)\u001b[0m\n\u001b[0;32m    649\u001b[0m                         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m                         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m                         dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[0;32m    652\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m       conv_out = K.bias_add(conv_out, b,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m   5111\u001b[0m       \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5112\u001b[0m       \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5113\u001b[1;33m       data_format=tf_data_format)\n\u001b[0m\u001b[0;32m   5114\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'channels_first'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtf_data_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'NHWC'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5115\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# NHWC -> NCHW\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[1;34m(input, filter, padding, strides, dilation_rate, name, data_format, filters, dilations)\u001b[0m\n\u001b[0;32m    996\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m       \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilation_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 998\u001b[1;33m       name=name)\n\u001b[0m\u001b[0;32m    999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconvolution_internal\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[0;32m   1146\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m           \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m           name=name)\n\u001b[0m\u001b[0;32m   1149\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mchannel_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_conv2d_expanded_batch\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   2590\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2591\u001b[0m         \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2592\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   2593\u001b[0m   return squeeze_batch_dims(\n\u001b[0;32m   2594\u001b[0m       \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m    977\u001b[0m                   \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m                   \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m                   data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[0;32m    980\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    591\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3483\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3484\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3485\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3486\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3487\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[1;32m-> 1975\u001b[1;33m                                 control_input_ops, op_def)\n\u001b[0m\u001b[0;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1813\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1815\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1817\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 3 from 2 for '{{node conv_lst_m2d_2/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](conv_lst_m2d_2/Sum, conv_lst_m2d_2/zeros)' with input shapes: [?,1,2,64], [1,3,64,64]."
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "simple_convlstm = keras.Sequential()\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True, \n",
    "                                            input_shape=(x_train_conv.shape[1], x_train_conv.shape[2], \n",
    "                                                         x_train_conv.shape[3], x_train_conv.shape[4])))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.Flatten())\n",
    "simple_convlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(32))\n",
    "simple_convlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_convlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('ConvLSTM')\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_convlstm.fit(x_train_conv,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_convlstm.load_weights(filepath_simple)\n",
    "preds = simple_convlstm.predict(x_test_conv)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "atten_convlstm = keras.Sequential()\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True, \n",
    "                                            input_shape=(x_train_conv.shape[1], x_train_conv.shape[2], \n",
    "                                                         x_train_conv.shape[3], x_train_conv.shape[4])))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(attention(return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.Flatten())\n",
    "atten_convlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(32))\n",
    "atten_convlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_convlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('ConvLSTM')\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_convlstm.fit(x_train_conv,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_convlstm.load_weights(filepath_attention)\n",
    "preds = atten_convlstm.predict(x_test_conv)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('ConvLSTM Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_seq2seq.hdf5'\n",
    "filepath_attention = 'attention_seq2seq.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_seq = y_train.reshape(y_train.shape[0], y_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_train = keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "output_train = keras.layers.Input(shape=(y_train_seq.shape[1], y_train_seq.shape[2]))\n",
    "\n",
    "## Encoder Section##\n",
    "encoder_first = keras.layers.LSTM(128, return_sequences=True, return_state=False)(input_train)\n",
    "encoder_second = keras.layers.LSTM(128, return_sequences=True)(encoder_first)\n",
    "encoder_third = keras.layers.LSTM(128, return_sequences=True)(encoder_second)\n",
    "encoder_fourth, encoder_fourth_s1, encoder_fourth_s2 = keras.layers.LSTM(128,return_sequences=False, return_state=True)(encoder_third)\n",
    "\n",
    "##Decorder Section##\n",
    "decoder_first = keras.layers.RepeatVector(output_train.shape[1])(encoder_fourth)\n",
    "decoder_second = keras.layers.LSTM(128, return_state=False, return_sequences=True)(decoder_first,initial_state=[encoder_fourth,encoder_fourth_s2])\n",
    "decoder_third = keras.layers.LSTM(128,return_sequences=True)(decoder_second)\n",
    "decoder_fourth = keras.layers.LSTM(128,return_sequences=True)(decoder_third)\n",
    "decoder_fifth = keras.layers.LSTM(128,return_sequences=True)(decoder_fourth)\n",
    "print(decoder_fifth)\n",
    "\n",
    "##Output Section##\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(output_train.shape[2]))(decoder_fifth)\n",
    "\n",
    "simple_seq = keras.Model(inputs=input_train, outputs=output)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "simple_seq.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "simple_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Seq2Seq')\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_seq.fit(x_train,y_train_seq,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_seq.load_weights(filepath_simple)\n",
    "preds = simple_seq.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " K.clear_session()\n",
    "\n",
    "input_train = keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "output_train = keras.layers.Input(shape=(y_train_seq.shape[1], y_train_seq.shape[2]))\n",
    "\n",
    "## Encoder Section##\n",
    "encoder_first = keras.layers.LSTM(128, return_sequences=True, return_state=False)(input_train)\n",
    "encoder_second = keras.layers.LSTM(128, return_sequences=True)(encoder_first)\n",
    "encoder_third = keras.layers.LSTM(128, return_sequences=True)(encoder_second)\n",
    "encoder_fourth, encoder_fourth_s1, encoder_fourth_s2 = keras.layers.LSTM(128,return_sequences=True,return_state=True)(encoder_third)\n",
    "\n",
    "##Decoder Section##\n",
    "decoder_first = keras.layers.RepeatVector(output_train.shape[1])(encoder_fourth_s1)\n",
    "decoder_second = keras.layers.LSTM(128, return_state=False, return_sequences=True)(decoder_first, initial_state=[encoder_fourth_s1, encoder_fourth_s2])\n",
    "\n",
    "attention = keras.layers.dot([decoder_second, encoder_fourth], axes=[2, 2])\n",
    "attention = keras.layers.Activation('softmax')(attention)\n",
    "context = keras.layers.dot([attention, encoder_fourth], axes=[2, 1])\n",
    "\n",
    "decoder_third = keras.layers.concatenate([context, decoder_second])\n",
    "\n",
    "decoder_fourth = keras.layers.LSTM(128, return_sequences=True)(decoder_third)\n",
    "decoder_fifth = keras.layers.LSTM(128, return_sequences=True)(decoder_fourth)\n",
    "decoder_sixth = keras.layers.LSTM(128, return_sequences=True)(decoder_fifth)\n",
    "\n",
    "##Output Section##\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(output_train.shape[2]))(decoder_sixth)\n",
    "\n",
    "atten_seq = keras.Model(inputs=input_train, outputs=output)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "atten_seq.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "atten_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Seq2Seq')\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_seq.fit(x_train,y_train_seq,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_seq.load_weights(filepath_attention)\n",
    "preds = atten_seq.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('Seq2Seq Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_wavenet.hdf5'\n",
    "filepath_attention = 'attention_wavenet.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 128\n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(7)]\n",
    "\n",
    "inputs = keras.layers.Input(shape=(x_train.shape[1],x_train.shape[2]))\n",
    "x=inputs\n",
    "\n",
    "skips = []\n",
    "for dilation_rate in dilation_rates:\n",
    "\n",
    "    x   = keras.layers.Conv1D(64, 1, padding='same')(x) \n",
    "    x_f = keras.layers.Conv1D(filters=n_filters,kernel_size=filter_width,padding='causal',dilation_rate=dilation_rate)(x)\n",
    "    x_g = keras.layers.Conv1D(filters=n_filters,kernel_size=filter_width, padding='causal',dilation_rate=dilation_rate)(x)\n",
    "\n",
    "    z = keras.layers.Multiply()([keras.layers.Activation('tanh')(x_f),keras.layers.Activation('sigmoid')(x_g)])\n",
    "\n",
    "    z = keras.layers.Conv1D(64, 1, padding='same', activation='relu')(z)\n",
    "\n",
    "    x = keras.layers.Add()([x, z])    \n",
    "\n",
    "    skips.append(z)\n",
    "\n",
    "out = keras.layers.Activation('relu')(keras.layers.Add()(skips)) \n",
    "out = keras.layers.Conv1D(128, 1, padding='same')(out)\n",
    "out = keras.layers.Activation('relu')(out)\n",
    "out = keras.layers.Dropout(0.4)(out)\n",
    "out = keras.layers.Conv1D(1, 1, padding='same')(out)\n",
    "\n",
    "out = keras.layers.Flatten()(out)\n",
    "out = keras.layers.Dense(6)(out)\n",
    "\n",
    "simple_wavenet = keras.Model(inputs, out)\n",
    "\n",
    "simple_wavenet.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Wavenet')\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = simple_wavenet.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "simple_wavenet.load_weights(filepath_simple)\n",
    "preds = simple_wavenet.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 128\n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(7)]\n",
    "\n",
    "inputs = Input(shape=(x_train.shape[1],x_train.shape[2]))\n",
    "x=inputs\n",
    "\n",
    "skips = []\n",
    "for dilation_rate in dilation_rates:\n",
    "\n",
    "    x   = Conv1D(64, 1, padding='same')(x) \n",
    "    x_f = Conv1D(filters=n_filters,kernel_size=filter_width,padding='causal',dilation_rate=dilation_rate)(x)\n",
    "    x_g = Conv1D(filters=n_filters,kernel_size=filter_width, padding='causal',dilation_rate=dilation_rate)(x)\n",
    "\n",
    "    z = Multiply()([keras.layers.Activation('tanh')(x_f),keras.layers.Activation('sigmoid')(x_g)])\n",
    "\n",
    "    z = Conv1D(64, 1, padding='same', activation='relu')(z)\n",
    "\n",
    "    x = Add()([x, z])    \n",
    "\n",
    "    skips.append(z)\n",
    "\n",
    "out = Activation('relu')(keras.layers.Add()(skips)) \n",
    "out = attention(return_sequences=True)(out)\n",
    "out = Conv1D(128, 1, padding='same')(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.4)(out)\n",
    "out = Conv1D(1, 1, padding='same')(out)\n",
    "# out = attention(return_sequences=True)(out)\n",
    "out = Flatten()(out)\n",
    "out = Dense(6)(out)\n",
    "\n",
    "atten_wavenet = keras.Model(inputs, out)\n",
    "\n",
    "atten_wavenet.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n",
    "atten_wavenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Wavenet')\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_wavenet.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_wavenet.load_weights(filepath_attention)\n",
    "preds = atten_wavenet.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('Wavenet Results.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk.save('Wavenet Results.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
