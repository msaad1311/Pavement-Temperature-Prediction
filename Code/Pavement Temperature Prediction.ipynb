{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Loaded\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xlwt \n",
    "from xlwt import Workbook \n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "print('Libraries Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities \n",
    "\n",
    "def read_file(path):\n",
    "    df= pd.read_excel(path)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "def create_dataset(X, y, time_steps, ts_range):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps - ts_range):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.values[(i + time_steps):(i + time_steps + ts_range),0])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def splitter(df,output,lag,duration,ts):\n",
    "    assert (0. <= ts <= 1.)\n",
    "    train_size = int(len(df) * ts)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df[train_size:]\n",
    "    print(train.shape, test.shape)\n",
    "    scaler,scaler_single = MinMaxScaler(feature_range=(0, 1)),MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaler.fit(train)\n",
    "    scaler_single.fit(train[output])\n",
    "\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train), columns=[df.columns])\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test), columns=[df.columns])\n",
    "\n",
    "    df_train = train_scaled.copy(deep=True)\n",
    "    df_test = test_scaled.copy(deep=True)\n",
    "\n",
    "    x_train,y_train = create_dataset(df_train,df_train[[output]],lag,duration)\n",
    "    x_test, y_test = create_dataset(df_test, df_test[[output]], lag, duration)\n",
    "\n",
    "    return x_train,x_test,y_train,y_test,scaler_single\n",
    "\n",
    "class attention(keras.layers.Layer):\n",
    "    '''\n",
    "    if return_sequences=True, it will give 3D vector and if false it will give 2D vector. It is same as LSTMs.\n",
    "\n",
    "    https://stackoverflow.com/questions/62948332/how-to-add-attention-layer-to-a-bi-lstm/62949137#62949137\n",
    "    the  following code is being copied from the above link.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, return_sequences=True, **kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention, self).__init__()\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10896, 7)\n",
      "   Year  Month  Day  Hour  Temp  Solar  Pavement\n",
      "0  2009     11    1     1   8.4    0.0  9.333333\n",
      "1  2009     11    1     2   8.3    0.0  8.933333\n",
      "2  2009     11    1     3   7.9    0.0  8.700000\n",
      "3  2009     11    1     4   7.6    0.0  8.533333\n",
      "4  2009     11    1     5   6.9    0.0  8.533333\n"
     ]
    }
   ],
   "source": [
    "## Loading the file \n",
    "\n",
    "src = r'C:\\Users\\Saad.LAKES\\Desktop\\Pavement-Temperature-Prediction\\Data'\n",
    "filename = r'Pave_data_cleaned.xlsx'\n",
    "\n",
    "dest = r'C:\\Users\\Saad.LAKES\\Desktop\\Pavement-Temperature-Prediction\\Solutions'\n",
    "\n",
    "df = read_file(os.path.join(src,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8716, 2) (2180, 2)\n",
      "The shape of x_train is (8686, 24, 2) and x_test is (2150, 24, 2)\n",
      "The shape of y_train is (8686, 6) and y_test is (2150, 6)\n"
     ]
    }
   ],
   "source": [
    "## Training the training and testing data\n",
    "\n",
    "x_train,x_test,y_train,y_test,scaler = splitter(df[['Temp','Pavement']],['Pavement'],24,6,0.8)\n",
    "print(f'The shape of x_train is {x_train.shape} and x_test is {x_test.shape}')\n",
    "print(f'The shape of y_train is {y_train.shape} and y_test is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_lstm.hdf5'\n",
    "filepath_attention = 'attention_lstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple LSTM\n",
    "K.clear_session()\n",
    "simple_lstm = keras.Sequential()\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.Dropout(0.3))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.Flatten())\n",
    "simple_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dropout(0.3))\n",
    "simple_lstm.add(keras.layers.Dense(32))\n",
    "simple_lstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "The Mean Squared Error is: 5.831630519312005\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('LSTM')\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = simple_lstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "simple_lstm.load_weights(filepath_simple)\n",
    "preds = simple_lstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention model\n",
    "\n",
    "K.clear_session()\n",
    "atten_lstm = keras.Sequential()\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "# atten_lstm.add(attention(return_sequences=True))\n",
    "atten_lstm.add(keras.layers.Dropout(0.3))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_lstm.add(attention(return_sequences=True))\n",
    "atten_lstm.add(keras.layers.Flatten())\n",
    "atten_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dropout(0.3))\n",
    "atten_lstm.add(keras.layers.Dense(32))\n",
    "atten_lstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 3s 10ms/step - loss: 0.0166 - mae: 0.0919 - val_loss: 0.0046 - val_mae: 0.0550\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0067 - mae: 0.0614 - val_loss: 0.0032 - val_mae: 0.0446\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0018 - val_mae: 0.0335\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0043 - mae: 0.0485 - val_loss: 0.0017 - val_mae: 0.0328\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0035 - mae: 0.0434 - val_loss: 0.0014 - val_mae: 0.0290\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0012 - val_mae: 0.0278\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0025 - mae: 0.0359 - val_loss: 8.1998e-04 - val_mae: 0.0230\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0023 - mae: 0.0345 - val_loss: 9.7574e-04 - val_mae: 0.0243\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0022 - mae: 0.0336 - val_loss: 7.8868e-04 - val_mae: 0.0227\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0021 - mae: 0.0326 - val_loss: 6.6729e-04 - val_mae: 0.0201\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0021 - mae: 0.0329 - val_loss: 8.4943e-04 - val_mae: 0.0225\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0020 - mae: 0.0316 - val_loss: 6.4670e-04 - val_mae: 0.0196\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0019 - mae: 0.0312 - val_loss: 5.9379e-04 - val_mae: 0.0191\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0305 - val_loss: 6.2856e-04 - val_mae: 0.0194\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0302 - val_loss: 5.0913e-04 - val_mae: 0.0172\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 7.6135e-04 - val_mae: 0.0214\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 7.9579e-04 - val_mae: 0.0223\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0299 - val_loss: 5.0978e-04 - val_mae: 0.0174\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0291 - val_loss: 5.1046e-04 - val_mae: 0.0172\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 8.1448e-04 - val_mae: 0.0236\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 5.9474e-04 - val_mae: 0.0184\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0293 - val_loss: 6.7651e-04 - val_mae: 0.0208\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0289 - val_loss: 7.7463e-04 - val_mae: 0.0218\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0292 - val_loss: 6.6507e-04 - val_mae: 0.0196\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0289 - val_loss: 8.9247e-04 - val_mae: 0.0236\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 5.9263e-04 - val_mae: 0.0188\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 5.0157e-04 - val_mae: 0.0169\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 6.0686e-04 - val_mae: 0.0194\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 4.6925e-04 - val_mae: 0.0167\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0281 - val_loss: 8.8835e-04 - val_mae: 0.0236\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 0.0015 - val_mae: 0.0325\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 5.2127e-04 - val_mae: 0.0177\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 5.1254e-04 - val_mae: 0.0172\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 4.7164e-04 - val_mae: 0.0161\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 6.1034e-04 - val_mae: 0.0189\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 6.2952e-04 - val_mae: 0.0197\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 0.0010 - val_mae: 0.0256\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0276 - val_loss: 7.1237e-04 - val_mae: 0.0214\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 4.6310e-04 - val_mae: 0.0161\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0272 - val_loss: 6.4891e-04 - val_mae: 0.0190\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 5.3622e-04 - val_mae: 0.0180\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 8.9043e-04 - val_mae: 0.0235\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0262 - val_loss: 6.7912e-04 - val_mae: 0.0198\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 5.0501e-04 - val_mae: 0.0171\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0262 - val_loss: 5.3638e-04 - val_mae: 0.0175\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 6.9804e-04 - val_mae: 0.0211\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 7.6167e-04 - val_mae: 0.0221\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.3928e-04 - val_mae: 0.0216\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 7.1989e-04 - val_mae: 0.0215\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 4.9343e-04 - val_mae: 0.0165\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 6.0149e-04 - val_mae: 0.0183\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0257 - val_loss: 4.9858e-04 - val_mae: 0.0168\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 6.9319e-04 - val_mae: 0.0199\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 4.8471e-04 - val_mae: 0.0163\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0256 - val_loss: 7.5512e-04 - val_mae: 0.0227\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0252 - val_loss: 6.9901e-04 - val_mae: 0.0192\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 0.0014 - val_mae: 0.0317\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 6.0929e-04 - val_mae: 0.0187\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0252 - val_loss: 5.4922e-04 - val_mae: 0.0171\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0253 - val_loss: 4.8564e-04 - val_mae: 0.0166\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 4.5125e-04 - val_mae: 0.0159\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 5.4331e-04 - val_mae: 0.0177\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 4.4389e-04 - val_mae: 0.0156\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0243 - val_loss: 4.8704e-04 - val_mae: 0.0163\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0250 - val_loss: 4.6202e-04 - val_mae: 0.0157\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 6.0256e-04 - val_mae: 0.0186\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 6.7643e-04 - val_mae: 0.0210\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 4.3849e-04 - val_mae: 0.0155\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 4.7764e-04 - val_mae: 0.0162\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 5.9106e-04 - val_mae: 0.0185\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 4.8004e-04 - val_mae: 0.0162\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 4.9090e-04 - val_mae: 0.0167\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 5.1473e-04 - val_mae: 0.0167\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0241 - val_loss: 5.0822e-04 - val_mae: 0.0166\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 5.6761e-04 - val_mae: 0.0183\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0234 - val_loss: 4.4018e-04 - val_mae: 0.0156\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 5.1249e-04 - val_mae: 0.0168\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0241 - val_loss: 5.7695e-04 - val_mae: 0.0187\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0244 - val_loss: 7.8082e-04 - val_mae: 0.0217\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 7.4469e-04 - val_mae: 0.0211\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 4.6033e-04 - val_mae: 0.0160\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0232 - val_loss: 4.6329e-04 - val_mae: 0.0163\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 5.7212e-04 - val_mae: 0.0180\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 6.6960e-04 - val_mae: 0.0203\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 7.7749e-04 - val_mae: 0.0219\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 5.6750e-04 - val_mae: 0.0181\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 5.4974e-04 - val_mae: 0.0172\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0236 - val_loss: 5.5364e-04 - val_mae: 0.0184\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.8295e-04 - mae: 0.0226 - val_loss: 4.5449e-04 - val_mae: 0.0160\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 5.0291e-04 - val_mae: 0.0162\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.8726e-04 - mae: 0.0227 - val_loss: 5.9155e-04 - val_mae: 0.0179\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0228 - val_loss: 5.4267e-04 - val_mae: 0.0175\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.8774e-04 - mae: 0.0227 - val_loss: 5.6822e-04 - val_mae: 0.0183\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0232 - val_loss: 4.7659e-04 - val_mae: 0.0162\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 4.8763e-04 - val_mae: 0.0161\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7805e-04 - mae: 0.0226 - val_loss: 7.5191e-04 - val_mae: 0.0206\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0233 - val_loss: 5.5288e-04 - val_mae: 0.0177\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0227 - val_loss: 5.0948e-04 - val_mae: 0.0172\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4608e-04 - mae: 0.0221 - val_loss: 5.2689e-04 - val_mae: 0.0178\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.5670e-04 - mae: 0.0224 - val_loss: 4.9545e-04 - val_mae: 0.0168\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1978e-04 - mae: 0.0219 - val_loss: 5.5618e-04 - val_mae: 0.0181\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3504e-04 - mae: 0.0221 - val_loss: 5.0430e-04 - val_mae: 0.0169\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7556e-04 - mae: 0.0225 - val_loss: 5.6658e-04 - val_mae: 0.0182\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.6598e-04 - mae: 0.0225 - val_loss: 5.9647e-04 - val_mae: 0.0192\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4924e-04 - mae: 0.0223 - val_loss: 4.7508e-04 - val_mae: 0.0161\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3782e-04 - mae: 0.0220 - val_loss: 5.4138e-04 - val_mae: 0.0173\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4446e-04 - mae: 0.0221 - val_loss: 5.3772e-04 - val_mae: 0.0178\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9935e-04 - mae: 0.0217 - val_loss: 4.8833e-04 - val_mae: 0.0167\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.2538e-04 - mae: 0.0221 - val_loss: 4.8161e-04 - val_mae: 0.0166\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7096e-04 - mae: 0.0224 - val_loss: 9.7204e-04 - val_mae: 0.0236\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1934e-04 - mae: 0.0220 - val_loss: 5.0129e-04 - val_mae: 0.0161\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1058e-04 - mae: 0.0219 - val_loss: 5.0125e-04 - val_mae: 0.0164\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.0438e-04 - mae: 0.0216 - val_loss: 4.8580e-04 - val_mae: 0.0168\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.0101e-04 - mae: 0.0216 - val_loss: 6.0205e-04 - val_mae: 0.0188\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4897e-04 - mae: 0.0221 - val_loss: 4.4494e-04 - val_mae: 0.0155\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8195e-04 - mae: 0.0215 - val_loss: 4.6716e-04 - val_mae: 0.0161\n",
      "Epoch 117/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9175e-04 - mae: 0.0217 - val_loss: 5.6525e-04 - val_mae: 0.0183\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9078e-04 - mae: 0.0215 - val_loss: 5.2264e-04 - val_mae: 0.0172\n",
      "Epoch 119/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.0543e-04 - mae: 0.0217 - val_loss: 6.5982e-04 - val_mae: 0.0201\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9142e-04 - mae: 0.0216 - val_loss: 4.9140e-04 - val_mae: 0.0167\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7356e-04 - mae: 0.0212 - val_loss: 5.5472e-04 - val_mae: 0.0179\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9580e-04 - mae: 0.0214 - val_loss: 4.8830e-04 - val_mae: 0.0164\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9202e-04 - mae: 0.0214 - val_loss: 5.7242e-04 - val_mae: 0.0178\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8238e-04 - mae: 0.0213 - val_loss: 4.3649e-04 - val_mae: 0.0156\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4146e-04 - mae: 0.0220 - val_loss: 7.1329e-04 - val_mae: 0.0210\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1766e-04 - mae: 0.0216 - val_loss: 5.4910e-04 - val_mae: 0.0180\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8786e-04 - mae: 0.0213 - val_loss: 4.9622e-04 - val_mae: 0.0167\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7645e-04 - mae: 0.0213 - val_loss: 5.0900e-04 - val_mae: 0.0172\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6961e-04 - mae: 0.0212 - val_loss: 4.6907e-04 - val_mae: 0.0160\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7499e-04 - mae: 0.0212 - val_loss: 4.5605e-04 - val_mae: 0.0159\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7462e-04 - mae: 0.0212 - val_loss: 5.2591e-04 - val_mae: 0.0169\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6613e-04 - mae: 0.0211 - val_loss: 4.5996e-04 - val_mae: 0.0158\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9369e-04 - mae: 0.0213 - val_loss: 4.9049e-04 - val_mae: 0.0168\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6253e-04 - mae: 0.0212 - val_loss: 5.1040e-04 - val_mae: 0.0173\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5911e-04 - mae: 0.0210 - val_loss: 0.0011 - val_mae: 0.0270\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7370e-04 - mae: 0.0212 - val_loss: 6.9705e-04 - val_mae: 0.0207\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1493e-04 - mae: 0.0216 - val_loss: 5.6303e-04 - val_mae: 0.0179\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6946e-04 - mae: 0.0212 - val_loss: 5.7483e-04 - val_mae: 0.0183\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8351e-04 - mae: 0.0211 - val_loss: 5.5137e-04 - val_mae: 0.0177\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6095e-04 - mae: 0.0212 - val_loss: 4.7717e-04 - val_mae: 0.0161\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6476e-04 - mae: 0.0209 - val_loss: 5.4535e-04 - val_mae: 0.0182\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2093e-04 - mae: 0.0206 - val_loss: 5.4468e-04 - val_mae: 0.0174\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5931e-04 - mae: 0.0210 - val_loss: 5.3032e-04 - val_mae: 0.0172\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.4424e-04 - mae: 0.0208 - val_loss: 4.5169e-04 - val_mae: 0.0155\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5548e-04 - mae: 0.0209 - val_loss: 7.3757e-04 - val_mae: 0.0210\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.4553e-04 - mae: 0.0207 - val_loss: 4.8686e-04 - val_mae: 0.0167\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.4373e-04 - mae: 0.0208 - val_loss: 4.8167e-04 - val_mae: 0.0163\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5182e-04 - mae: 0.0209 - val_loss: 5.3387e-04 - val_mae: 0.0170\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1782e-04 - mae: 0.0207 - val_loss: 4.8156e-04 - val_mae: 0.0166\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1447e-04 - mae: 0.0205 - val_loss: 4.8484e-04 - val_mae: 0.0166\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.0512e-04 - mae: 0.0204 - val_loss: 5.2836e-04 - val_mae: 0.0171\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.0794e-04 - mae: 0.0205 - val_loss: 4.8704e-04 - val_mae: 0.0166\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2947e-04 - mae: 0.0205 - val_loss: 5.7756e-04 - val_mae: 0.0183\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5727e-04 - mae: 0.0209 - val_loss: 6.1027e-04 - val_mae: 0.0183\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1789e-04 - mae: 0.0205 - val_loss: 4.8282e-04 - val_mae: 0.0159\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.4968e-04 - mae: 0.0207 - val_loss: 6.2155e-04 - val_mae: 0.0193\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.3651e-04 - mae: 0.0206 - val_loss: 4.9101e-04 - val_mae: 0.0164\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2479e-04 - mae: 0.0205 - val_loss: 5.3363e-04 - val_mae: 0.0177\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2365e-04 - mae: 0.0205 - val_loss: 4.9522e-04 - val_mae: 0.0165\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9751e-04 - mae: 0.0201 - val_loss: 5.0268e-04 - val_mae: 0.0173\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7028e-04 - mae: 0.0199 - val_loss: 5.0949e-04 - val_mae: 0.0169\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9885e-04 - mae: 0.0203 - val_loss: 5.5046e-04 - val_mae: 0.0178\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9663e-04 - mae: 0.0201 - val_loss: 4.7762e-04 - val_mae: 0.0162\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8949e-04 - mae: 0.0201 - val_loss: 5.1915e-04 - val_mae: 0.0173\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1346e-04 - mae: 0.0203 - val_loss: 5.6353e-04 - val_mae: 0.0180\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8592e-04 - mae: 0.0202 - val_loss: 4.8551e-04 - val_mae: 0.0159\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1087e-04 - mae: 0.0204 - val_loss: 5.2328e-04 - val_mae: 0.0172\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8152e-04 - mae: 0.0199 - val_loss: 5.1955e-04 - val_mae: 0.0171\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7043e-04 - mae: 0.0200 - val_loss: 4.8279e-04 - val_mae: 0.0164\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9113e-04 - mae: 0.0201 - val_loss: 6.0441e-04 - val_mae: 0.0187\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8267e-04 - mae: 0.0200 - val_loss: 5.3228e-04 - val_mae: 0.0173\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8489e-04 - mae: 0.0200 - val_loss: 4.6516e-04 - val_mae: 0.0161\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2217e-04 - mae: 0.0204 - val_loss: 5.1090e-04 - val_mae: 0.0174\n",
      "Epoch 174/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7475e-04 - mae: 0.0200 - val_loss: 5.5545e-04 - val_mae: 0.0179\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8380e-04 - mae: 0.0199 - val_loss: 5.3275e-04 - val_mae: 0.0172\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9122e-04 - mae: 0.0200 - val_loss: 5.0088e-04 - val_mae: 0.0173\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8802e-04 - mae: 0.0199 - val_loss: 4.5936e-04 - val_mae: 0.0156\n",
      "Epoch 178/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8169e-04 - mae: 0.0200 - val_loss: 4.8299e-04 - val_mae: 0.0162\n",
      "Epoch 179/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7544e-04 - mae: 0.0199 - val_loss: 4.8265e-04 - val_mae: 0.0167\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.6379e-04 - mae: 0.0199 - val_loss: 5.1002e-04 - val_mae: 0.0173\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8607e-04 - mae: 0.0200 - val_loss: 5.3755e-04 - val_mae: 0.0173\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.5366e-04 - mae: 0.0196 - val_loss: 6.1328e-04 - val_mae: 0.0188\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8392e-04 - mae: 0.0199 - val_loss: 5.8162e-04 - val_mae: 0.0177\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.0141e-04 - mae: 0.0200 - val_loss: 4.9190e-04 - val_mae: 0.0164\n",
      "Epoch 185/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7495e-04 - mae: 0.0198 - val_loss: 4.4135e-04 - val_mae: 0.0156\n",
      "Epoch 186/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.5836e-04 - mae: 0.0195 - val_loss: 5.5903e-04 - val_mae: 0.0177\n",
      "Epoch 187/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.6309e-04 - mae: 0.0198 - val_loss: 6.0272e-04 - val_mae: 0.0191\n",
      "Epoch 188/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.4513e-04 - mae: 0.0195 - val_loss: 4.9822e-04 - val_mae: 0.0168\n",
      "Epoch 189/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7511e-04 - mae: 0.0197 - val_loss: 5.0211e-04 - val_mae: 0.0168\n",
      "Epoch 190/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.4107e-04 - mae: 0.0194 - val_loss: 5.5299e-04 - val_mae: 0.0186\n",
      "Epoch 191/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.6471e-04 - mae: 0.0196 - val_loss: 4.9453e-04 - val_mae: 0.0163\n",
      "Epoch 192/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.6762e-04 - mae: 0.0197 - val_loss: 4.7398e-04 - val_mae: 0.0161\n",
      "Epoch 193/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8908e-04 - mae: 0.0201 - val_loss: 8.9701e-04 - val_mae: 0.0227\n",
      "Epoch 194/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.0021e-04 - mae: 0.0202 - val_loss: 5.7299e-04 - val_mae: 0.0183\n",
      "Epoch 195/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.3829e-04 - mae: 0.0193 - val_loss: 4.9209e-04 - val_mae: 0.0167\n",
      "Epoch 196/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.3725e-04 - mae: 0.0193 - val_loss: 5.6572e-04 - val_mae: 0.0176\n",
      "Epoch 197/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.5500e-04 - mae: 0.0196 - val_loss: 7.1731e-04 - val_mae: 0.0214\n",
      "Epoch 198/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7683e-04 - mae: 0.0198 - val_loss: 5.1770e-04 - val_mae: 0.0176\n",
      "Epoch 199/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.6047e-04 - mae: 0.0196 - val_loss: 5.2973e-04 - val_mae: 0.0168\n",
      "Epoch 200/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.2178e-04 - mae: 0.0192 - val_loss: 5.0018e-04 - val_mae: 0.0166\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error is: 6.021731217632003\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('LSTM')\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_lstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_lstm.load_weights(filepath_attention)\n",
    "preds = atten_lstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "\n",
    "wk.save(f'LSTM Result.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
