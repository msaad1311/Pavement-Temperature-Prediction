{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Loaded\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xlwt \n",
    "from xlwt import Workbook \n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "print('Libraries Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities \n",
    "\n",
    "def read_file(path):\n",
    "    df= pd.read_excel(path)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "def create_dataset(X, y, time_steps, ts_range):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps - ts_range):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.values[(i + time_steps):(i + time_steps + ts_range),0])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def splitter(df,output,lag,duration,ts):\n",
    "    assert (0. <= ts <= 1.)\n",
    "    train_size = int(len(df) * ts)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df[train_size:]\n",
    "    print(train.shape, test.shape)\n",
    "    scaler,scaler_single = MinMaxScaler(feature_range=(0, 1)),MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaler.fit(train)\n",
    "    scaler_single.fit(train[output])\n",
    "\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train), columns=[df.columns])\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test), columns=[df.columns])\n",
    "\n",
    "    df_train = train_scaled.copy(deep=True)\n",
    "    df_test = test_scaled.copy(deep=True)\n",
    "\n",
    "    x_train,y_train = create_dataset(df_train,df_train[[output]],lag,duration)\n",
    "    x_test, y_test = create_dataset(df_test, df_test[[output]], lag, duration)\n",
    "\n",
    "    return x_train,x_test,y_train,y_test,scaler_single\n",
    "\n",
    "class attention(keras.layers.Layer):\n",
    "    '''\n",
    "    if return_sequences=True, it will give 3D vector and if false it will give 2D vector. It is same as LSTMs.\n",
    "\n",
    "    https://stackoverflow.com/questions/62948332/how-to-add-attention-layer-to-a-bi-lstm/62949137#62949137\n",
    "    the  following code is being copied from the above link.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, return_sequences=True, **kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention, self).__init__()\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10896, 7)\n",
      "   Year  Month  Day  Hour  Temp  Solar  Pavement\n",
      "0  2009     11    1     1   8.4    0.0  9.333333\n",
      "1  2009     11    1     2   8.3    0.0  8.933333\n",
      "2  2009     11    1     3   7.9    0.0  8.700000\n",
      "3  2009     11    1     4   7.6    0.0  8.533333\n",
      "4  2009     11    1     5   6.9    0.0  8.533333\n"
     ]
    }
   ],
   "source": [
    "## Loading the file \n",
    "\n",
    "src = r'C:\\Users\\Saad.LAKES\\Desktop\\Pavement-Temperature-Prediction\\Data'\n",
    "filename = r'Pave_data_cleaned.xlsx'\n",
    "\n",
    "dest = r'C:\\Users\\Saad.LAKES\\Desktop\\Pavement-Temperature-Prediction\\Solutions\\Six Hours Lag'\n",
    "\n",
    "df = read_file(os.path.join(src,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8716, 2) (2180, 2)\n",
      "The shape of x_train is (8704, 6, 2) and x_test is (2168, 6, 2)\n",
      "The shape of y_train is (8704, 6) and y_test is (2168, 6)\n"
     ]
    }
   ],
   "source": [
    "## Training the training and testing data\n",
    "\n",
    "x_train,x_test,y_train,y_test,scaler = splitter(df[['Temp','Pavement']],['Pavement'],6,6,0.8)\n",
    "print(f'The shape of x_train is {x_train.shape} and x_test is {x_test.shape}')\n",
    "print(f'The shape of y_train is {y_train.shape} and y_test is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_lstm.hdf5'\n",
    "filepath_attention = 'attention_lstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple LSTM\n",
    "K.clear_session()\n",
    "simple_lstm = keras.Sequential()\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.Dropout(0.3))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.Flatten())\n",
    "simple_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dropout(0.3))\n",
    "simple_lstm.add(keras.layers.Dense(32))\n",
    "simple_lstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new directory......\n",
      "New Directory Created\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0146 - mae: 0.0852 - val_loss: 0.0023 - val_mae: 0.0370\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0069 - mae: 0.0611 - val_loss: 0.0022 - val_mae: 0.0365\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0056 - mae: 0.0552 - val_loss: 0.0022 - val_mae: 0.0367\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0046 - mae: 0.0497 - val_loss: 0.0014 - val_mae: 0.0284\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0041 - mae: 0.0459 - val_loss: 0.0014 - val_mae: 0.0272\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0036 - mae: 0.0431 - val_loss: 0.0015 - val_mae: 0.0295\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0035 - mae: 0.0426 - val_loss: 0.0012 - val_mae: 0.0260\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0032 - mae: 0.0407 - val_loss: 0.0020 - val_mae: 0.0357\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0030 - mae: 0.0402 - val_loss: 0.0010 - val_mae: 0.0242\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0029 - mae: 0.0391 - val_loss: 0.0012 - val_mae: 0.0281\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0027 - mae: 0.0373 - val_loss: 0.0013 - val_mae: 0.0279\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0026 - mae: 0.0367 - val_loss: 9.7834e-04 - val_mae: 0.0230\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0027 - mae: 0.0370 - val_loss: 0.0010 - val_mae: 0.0250\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0026 - mae: 0.0365 - val_loss: 9.7266e-04 - val_mae: 0.0236\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0025 - mae: 0.0355 - val_loss: 0.0011 - val_mae: 0.0263\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0351 - val_loss: 0.0014 - val_mae: 0.0290\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0352 - val_loss: 0.0011 - val_mae: 0.0259\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0348 - val_loss: 9.2194e-04 - val_mae: 0.0231\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0340 - val_loss: 8.6621e-04 - val_mae: 0.0220\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0345 - val_loss: 0.0011 - val_mae: 0.0256\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0340 - val_loss: 9.2318e-04 - val_mae: 0.0226\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0335 - val_loss: 8.9590e-04 - val_mae: 0.0223\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0335 - val_loss: 8.9534e-04 - val_mae: 0.0221\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0335 - val_loss: 8.1338e-04 - val_mae: 0.0207\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0333 - val_loss: 8.4413e-04 - val_mae: 0.0209\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0329 - val_loss: 8.8885e-04 - val_mae: 0.0226\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0329 - val_loss: 9.0732e-04 - val_mae: 0.0230\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0327 - val_loss: 0.0011 - val_mae: 0.0257\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0325 - val_loss: 8.5654e-04 - val_mae: 0.0220\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0321 - val_loss: 9.1475e-04 - val_mae: 0.0233\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0324 - val_loss: 7.9062e-04 - val_mae: 0.0209\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0319 - val_loss: 7.4094e-04 - val_mae: 0.0195\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0326 - val_loss: 0.0013 - val_mae: 0.0293\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0326 - val_loss: 7.8354e-04 - val_mae: 0.0205\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0318 - val_loss: 8.3236e-04 - val_mae: 0.0206\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 8.0783e-04 - val_mae: 0.0211\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0313 - val_loss: 8.2644e-04 - val_mae: 0.0207\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0321 - val_loss: 0.0010 - val_mae: 0.0238\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0315 - val_loss: 8.4793e-04 - val_mae: 0.0216\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0319 - val_loss: 8.2382e-04 - val_mae: 0.0197\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0320 - val_loss: 7.4777e-04 - val_mae: 0.0199\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0317 - val_loss: 7.4088e-04 - val_mae: 0.0198\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0314 - val_loss: 8.7723e-04 - val_mae: 0.0217\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0304 - val_loss: 8.6330e-04 - val_mae: 0.0223\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0314 - val_loss: 7.7653e-04 - val_mae: 0.0201\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 7.3451e-04 - val_mae: 0.0196\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0308 - val_loss: 7.7454e-04 - val_mae: 0.0192\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0312 - val_loss: 8.3103e-04 - val_mae: 0.0209\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 8.3696e-04 - val_mae: 0.0208\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0309 - val_loss: 7.2785e-04 - val_mae: 0.0195\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 9.0795e-04 - val_mae: 0.0209\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 8.6529e-04 - val_mae: 0.0220\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0019 - mae: 0.0305 - val_loss: 0.0011 - val_mae: 0.0253\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0298 - val_loss: 8.4696e-04 - val_mae: 0.0216\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 7.0835e-04 - val_mae: 0.0194\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 7.3532e-04 - val_mae: 0.0192\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0300 - val_loss: 8.3972e-04 - val_mae: 0.0221\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0298 - val_loss: 7.6693e-04 - val_mae: 0.0206\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0300 - val_loss: 7.2297e-04 - val_mae: 0.0200\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0300 - val_loss: 7.7074e-04 - val_mae: 0.0206\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 9.6617e-04 - val_mae: 0.0243\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0310 - val_loss: 8.5748e-04 - val_mae: 0.0224\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 7.1669e-04 - val_mae: 0.0200\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0296 - val_loss: 7.6433e-04 - val_mae: 0.0207\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0299 - val_loss: 7.6293e-04 - val_mae: 0.0203\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 6.8059e-04 - val_mae: 0.0185\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 7.8179e-04 - val_mae: 0.0203\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0297 - val_loss: 6.9912e-04 - val_mae: 0.0194\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0299 - val_loss: 7.3879e-04 - val_mae: 0.0192\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 7.4835e-04 - val_mae: 0.0196\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 9.2646e-04 - val_mae: 0.0221\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0302 - val_loss: 0.0012 - val_mae: 0.0270\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0296 - val_loss: 7.2881e-04 - val_mae: 0.0201\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 8.3453e-04 - val_mae: 0.0222\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 8.6633e-04 - val_mae: 0.0222\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0289 - val_loss: 7.5719e-04 - val_mae: 0.0194\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0287 - val_loss: 8.8940e-04 - val_mae: 0.0218\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0297 - val_loss: 6.6091e-04 - val_mae: 0.0188\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 8.2296e-04 - val_mae: 0.0215\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 8.4799e-04 - val_mae: 0.0221\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0297 - val_loss: 6.9173e-04 - val_mae: 0.0190\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 8.0499e-04 - val_mae: 0.0212\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 9.0890e-04 - val_mae: 0.0233\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0286 - val_loss: 8.9849e-04 - val_mae: 0.0230\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 0.0010 - val_mae: 0.0238\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 7.9315e-04 - val_mae: 0.0201\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 6.7464e-04 - val_mae: 0.0188\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 8.4237e-04 - val_mae: 0.0220\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 7.9438e-04 - val_mae: 0.0202\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 8.7215e-04 - val_mae: 0.0220\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 7.8217e-04 - val_mae: 0.0211\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0288 - val_loss: 7.5208e-04 - val_mae: 0.0202\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0286 - val_loss: 7.7745e-04 - val_mae: 0.0204\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 7.6917e-04 - val_mae: 0.0205\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 8.5075e-04 - val_mae: 0.0223\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 7.8915e-04 - val_mae: 0.0203\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 8.1809e-04 - val_mae: 0.0216\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0286 - val_loss: 0.0010 - val_mae: 0.0240\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 7.1980e-04 - val_mae: 0.0191\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 7.7559e-04 - val_mae: 0.0195\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 9.1498e-04 - val_mae: 0.0216\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 7.6044e-04 - val_mae: 0.0196\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 8.5384e-04 - val_mae: 0.0205\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 8.5643e-04 - val_mae: 0.0210\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 0.0010 - val_mae: 0.0239\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.3890e-04 - val_mae: 0.0192\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 7.3880e-04 - val_mae: 0.0193\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 7.9752e-04 - val_mae: 0.0204\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 7.1612e-04 - val_mae: 0.0193\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0279 - val_loss: 9.0503e-04 - val_mae: 0.0218\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 7.8252e-04 - val_mae: 0.0206\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 8.5672e-04 - val_mae: 0.0217\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 8.5172e-04 - val_mae: 0.0206\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 7.6126e-04 - val_mae: 0.0202\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0281 - val_loss: 7.0811e-04 - val_mae: 0.0190\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 7.4392e-04 - val_mae: 0.0192\n",
      "Epoch 117/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.2853e-04 - val_mae: 0.0196\n",
      "Epoch 118/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 9.5897e-04 - val_mae: 0.0234\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 8.0539e-04 - val_mae: 0.0210\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0278 - val_loss: 8.6833e-04 - val_mae: 0.0213\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 9.8666e-04 - val_mae: 0.0240\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.7944e-04 - val_mae: 0.0201\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 7.9892e-04 - val_mae: 0.0199\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 7.6120e-04 - val_mae: 0.0196\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 0.0013 - val_mae: 0.0271\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 8.0362e-04 - val_mae: 0.0205\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 7.6504e-04 - val_mae: 0.0193\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0276 - val_loss: 8.2691e-04 - val_mae: 0.0207\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 7.9200e-04 - val_mae: 0.0202\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 8.6818e-04 - val_mae: 0.0207\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 0.0010 - val_mae: 0.0254\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 9.3438e-04 - val_mae: 0.0225\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 7.5784e-04 - val_mae: 0.0202\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 8.0414e-04 - val_mae: 0.0202\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 0.0010 - val_mae: 0.0230\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.6841e-04 - val_mae: 0.0202\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 8.8575e-04 - val_mae: 0.0211\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.9197e-04 - val_mae: 0.0202\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 8.5646e-04 - val_mae: 0.0210\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 7.9322e-04 - val_mae: 0.0204\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 7.7534e-04 - val_mae: 0.0202\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.9249e-04 - val_mae: 0.0207\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 8.3167e-04 - val_mae: 0.0206\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 7.3123e-04 - val_mae: 0.0193\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 8.0289e-04 - val_mae: 0.0198\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 8.0347e-04 - val_mae: 0.0199\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.4373e-04 - val_mae: 0.0191\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.2872e-04 - val_mae: 0.0209\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 0.0010 - val_mae: 0.0230\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 7.5154e-04 - val_mae: 0.0197\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 8.7847e-04 - val_mae: 0.0219\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 7.6216e-04 - val_mae: 0.0202\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 8.8959e-04 - val_mae: 0.0227\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 0.0011 - val_mae: 0.0258\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.4885e-04 - val_mae: 0.0214\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 0.0014 - val_mae: 0.0290\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 8.5135e-04 - val_mae: 0.0214\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 8.1426e-04 - val_mae: 0.0206\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.9252e-04 - val_mae: 0.0204\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 7.6319e-04 - val_mae: 0.0200\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 8.6093e-04 - val_mae: 0.0212\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 7.4374e-04 - val_mae: 0.0193\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 9.2227e-04 - val_mae: 0.0219\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 9.5011e-04 - val_mae: 0.0223\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.9101e-04 - val_mae: 0.0204\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 7.8269e-04 - val_mae: 0.0198\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 7.7988e-04 - val_mae: 0.0204\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.1635e-04 - val_mae: 0.0203\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 8.0689e-04 - val_mae: 0.0207\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0273 - val_loss: 8.6265e-04 - val_mae: 0.0206\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 0.0012 - val_mae: 0.0263\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 8.1575e-04 - val_mae: 0.0211\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0263 - val_loss: 7.8355e-04 - val_mae: 0.0192\n",
      "Epoch 174/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.3257e-04 - val_mae: 0.0191\n",
      "Epoch 175/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.6884e-04 - val_mae: 0.0196\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 7.5175e-04 - val_mae: 0.0197\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 7.7034e-04 - val_mae: 0.0192\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 7.4060e-04 - val_mae: 0.0190\n",
      "Epoch 179/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 7.5415e-04 - val_mae: 0.0196\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 7.6827e-04 - val_mae: 0.0198\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 8.2486e-04 - val_mae: 0.0200\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 9.3326e-04 - val_mae: 0.0221\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0261 - val_loss: 0.0010 - val_mae: 0.0242\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 7.8020e-04 - val_mae: 0.0203\n",
      "Epoch 185/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 7.9389e-04 - val_mae: 0.0210\n",
      "Epoch 186/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 8.1494e-04 - val_mae: 0.0208\n",
      "Epoch 187/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 8.3797e-04 - val_mae: 0.0209\n",
      "Epoch 188/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 8.0297e-04 - val_mae: 0.0202\n",
      "Epoch 189/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 9.0414e-04 - val_mae: 0.0214\n",
      "Epoch 190/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 7.8015e-04 - val_mae: 0.0199\n",
      "Epoch 191/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 7.7391e-04 - val_mae: 0.0194\n",
      "Epoch 192/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 9.6838e-04 - val_mae: 0.0226\n",
      "Epoch 193/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0257 - val_loss: 8.2800e-04 - val_mae: 0.0212\n",
      "Epoch 194/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 8.4145e-04 - val_mae: 0.0207\n",
      "Epoch 195/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 7.4491e-04 - val_mae: 0.0190\n",
      "Epoch 196/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 7.7399e-04 - val_mae: 0.0197\n",
      "Epoch 197/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 7.9242e-04 - val_mae: 0.0201\n",
      "Epoch 198/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 7.4055e-04 - val_mae: 0.0194\n",
      "Epoch 199/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 8.1176e-04 - val_mae: 0.0194\n",
      "Epoch 200/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 9.1598e-04 - val_mae: 0.0212\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error is: 12.577021916561455\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('LSTM')\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_lstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_lstm.load_weights(filepath_simple)\n",
    "preds = simple_lstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention model\n",
    "\n",
    "K.clear_session()\n",
    "atten_lstm = keras.Sequential()\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "# atten_lstm.add(attention(return_sequences=True))\n",
    "atten_lstm.add(keras.layers.Dropout(0.3))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_lstm.add(attention(return_sequences=True))\n",
    "atten_lstm.add(keras.layers.Flatten())\n",
    "atten_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dropout(0.3))\n",
    "atten_lstm.add(keras.layers.Dense(32))\n",
    "atten_lstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.0169 - mae: 0.0914 - val_loss: 0.0028 - val_mae: 0.0430\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0071 - mae: 0.0621 - val_loss: 0.0024 - val_mae: 0.0375\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0063 - mae: 0.0581 - val_loss: 0.0021 - val_mae: 0.0366\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0053 - mae: 0.0531 - val_loss: 0.0016 - val_mae: 0.0307\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0044 - mae: 0.0484 - val_loss: 0.0015 - val_mae: 0.0302\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0038 - mae: 0.0447 - val_loss: 0.0013 - val_mae: 0.0271\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0037 - mae: 0.0439 - val_loss: 0.0015 - val_mae: 0.0307\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0034 - mae: 0.0421 - val_loss: 0.0017 - val_mae: 0.0323\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - ETA: 0s - loss: 0.0031 - mae: 0.040 - 1s 5ms/step - loss: 0.0031 - mae: 0.0406 - val_loss: 0.0014 - val_mae: 0.0283\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0030 - mae: 0.0394 - val_loss: 0.0011 - val_mae: 0.0256\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0029 - mae: 0.0386 - val_loss: 0.0011 - val_mae: 0.0244\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0015 - val_mae: 0.0294\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0027 - mae: 0.0370 - val_loss: 0.0018 - val_mae: 0.0338\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0027 - mae: 0.0372 - val_loss: 0.0011 - val_mae: 0.0244\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0027 - mae: 0.0376 - val_loss: 0.0011 - val_mae: 0.0253\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0027 - mae: 0.0376 - val_loss: 8.9347e-04 - val_mae: 0.0215\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0027 - mae: 0.0370 - val_loss: 9.2033e-04 - val_mae: 0.0228\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0026 - mae: 0.0360 - val_loss: 0.0013 - val_mae: 0.0284\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0026 - mae: 0.0362 - val_loss: 8.9940e-04 - val_mae: 0.0220\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0026 - mae: 0.0362 - val_loss: 9.6491e-04 - val_mae: 0.0232\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0025 - mae: 0.0357 - val_loss: 8.8832e-04 - val_mae: 0.0217\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0348 - val_loss: 9.4178e-04 - val_mae: 0.0229\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0349 - val_loss: 9.2025e-04 - val_mae: 0.0226\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0023 - mae: 0.0337 - val_loss: 8.2059e-04 - val_mae: 0.0211\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0344 - val_loss: 9.9881e-04 - val_mae: 0.0244\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0024 - mae: 0.0346 - val_loss: 0.0012 - val_mae: 0.0272\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0023 - mae: 0.0340 - val_loss: 8.4872e-04 - val_mae: 0.0214\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0022 - mae: 0.0329 - val_loss: 7.9549e-04 - val_mae: 0.0205\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0335 - val_loss: 0.0015 - val_mae: 0.0295\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0325 - val_loss: 8.7829e-04 - val_mae: 0.0209\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0323 - val_loss: 8.9556e-04 - val_mae: 0.0227\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0022 - mae: 0.0330 - val_loss: 7.8043e-04 - val_mae: 0.0207\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0321 - val_loss: 0.0012 - val_mae: 0.0265\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0021 - mae: 0.0325 - val_loss: 7.5122e-04 - val_mae: 0.0197\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0327 - val_loss: 9.4840e-04 - val_mae: 0.0225\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0322 - val_loss: 8.1564e-04 - val_mae: 0.0209\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0321 - val_loss: 8.6964e-04 - val_mae: 0.0225\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0318 - val_loss: 9.5667e-04 - val_mae: 0.0242\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0314 - val_loss: 7.5720e-04 - val_mae: 0.0203\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0321 - val_loss: 7.6363e-04 - val_mae: 0.0203\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0315 - val_loss: 8.6772e-04 - val_mae: 0.0225\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 7.5282e-04 - val_mae: 0.0194\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0312 - val_loss: 8.6436e-04 - val_mae: 0.0214\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0316 - val_loss: 7.5114e-04 - val_mae: 0.0194\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0315 - val_loss: 9.7814e-04 - val_mae: 0.0239\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0311 - val_loss: 7.2606e-04 - val_mae: 0.0185\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0320 - val_loss: 8.0594e-04 - val_mae: 0.0210\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0322 - val_loss: 8.0358e-04 - val_mae: 0.0196\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0309 - val_loss: 7.4059e-04 - val_mae: 0.0194\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 7.5571e-04 - val_mae: 0.0207\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 8.0491e-04 - val_mae: 0.0203\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0316 - val_loss: 8.6829e-04 - val_mae: 0.0224\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0019 - mae: 0.0311 - val_loss: 6.9629e-04 - val_mae: 0.0185\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 8.7870e-04 - val_mae: 0.0215\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0306 - val_loss: 8.6077e-04 - val_mae: 0.0216\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 7.3772e-04 - val_mae: 0.0192\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0312 - val_loss: 6.9988e-04 - val_mae: 0.0185\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0019 - mae: 0.0308 - val_loss: 8.6720e-04 - val_mae: 0.0218\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 7.2425e-04 - val_mae: 0.0190\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0303 - val_loss: 8.0739e-04 - val_mae: 0.0209\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0309 - val_loss: 8.0182e-04 - val_mae: 0.0209\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0310 - val_loss: 7.8321e-04 - val_mae: 0.0204\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0308 - val_loss: 8.3131e-04 - val_mae: 0.0213\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 8.1192e-04 - val_mae: 0.0209\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 0.0010 - val_mae: 0.0235\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 6.9679e-04 - val_mae: 0.0190\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 7.1078e-04 - val_mae: 0.0188\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0018 - mae: 0.0299 - val_loss: 7.7962e-04 - val_mae: 0.0208\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0019 - mae: 0.0306 - val_loss: 7.4561e-04 - val_mae: 0.0195\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0295 - val_loss: 7.7086e-04 - val_mae: 0.0203\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0018 - mae: 0.0299 - val_loss: 6.6565e-04 - val_mae: 0.0186\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0293 - val_loss: 8.2684e-04 - val_mae: 0.0220\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0018 - mae: 0.0298 - val_loss: 0.0013 - val_mae: 0.0285\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 9.3685e-04 - val_mae: 0.0237\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 7.1773e-04 - val_mae: 0.0199\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0296 - val_loss: 8.9911e-04 - val_mae: 0.0214\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0294 - val_loss: 7.6854e-04 - val_mae: 0.0209\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0300 - val_loss: 8.5287e-04 - val_mae: 0.0214\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0302 - val_loss: 8.4699e-04 - val_mae: 0.0222\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 7.1390e-04 - val_mae: 0.0198\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0298 - val_loss: 7.5513e-04 - val_mae: 0.0202\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0296 - val_loss: 7.9904e-04 - val_mae: 0.0205\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 8.5854e-04 - val_mae: 0.0205\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 7.4370e-04 - val_mae: 0.0196\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0293 - val_loss: 7.4370e-04 - val_mae: 0.0193\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 7.2264e-04 - val_mae: 0.0191\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 9.8388e-04 - val_mae: 0.0235\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 7.8953e-04 - val_mae: 0.0198\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0289 - val_loss: 8.0544e-04 - val_mae: 0.0204\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 7.9083e-04 - val_mae: 0.0202\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 7.6312e-04 - val_mae: 0.0202\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0289 - val_loss: 7.5814e-04 - val_mae: 0.0201\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 8.5744e-04 - val_mae: 0.0214\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 7.3611e-04 - val_mae: 0.0198\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 8.7179e-04 - val_mae: 0.0229\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 7.9605e-04 - val_mae: 0.0206\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0290 - val_loss: 8.6844e-04 - val_mae: 0.0222\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0289 - val_loss: 7.6976e-04 - val_mae: 0.0196\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0289 - val_loss: 7.5846e-04 - val_mae: 0.0195\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 7.6989e-04 - val_mae: 0.0201\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 7.1824e-04 - val_mae: 0.0186\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 7.5634e-04 - val_mae: 0.0196\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 7.2341e-04 - val_mae: 0.0197\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0287 - val_loss: 8.0563e-04 - val_mae: 0.0204\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 8.4059e-04 - val_mae: 0.0209\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0285 - val_loss: 7.2786e-04 - val_mae: 0.0196\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 7.1600e-04 - val_mae: 0.0195\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 8.2106e-04 - val_mae: 0.0205\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.6741e-04 - val_mae: 0.0202\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0288 - val_loss: 7.8090e-04 - val_mae: 0.0206\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 7.0475e-04 - val_mae: 0.0187\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 0.0010 - val_mae: 0.0250\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 7.0807e-04 - val_mae: 0.0190\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 7.3636e-04 - val_mae: 0.0195\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.0542e-04 - val_mae: 0.0187\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 7.7008e-04 - val_mae: 0.0195\n",
      "Epoch 117/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.9860e-04 - val_mae: 0.0202\n",
      "Epoch 118/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 7.9580e-04 - val_mae: 0.0205\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 7.7175e-04 - val_mae: 0.0203\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 7.6365e-04 - val_mae: 0.0198\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 8.8329e-04 - val_mae: 0.0218\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 7.3877e-04 - val_mae: 0.0194\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 8.1677e-04 - val_mae: 0.0204\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 8.8620e-04 - val_mae: 0.0230\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 7.8148e-04 - val_mae: 0.0212\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 8.1870e-04 - val_mae: 0.0215\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 7.5653e-04 - val_mae: 0.0193\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 7.5188e-04 - val_mae: 0.0193\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 7.6331e-04 - val_mae: 0.0203\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0279 - val_loss: 8.8493e-04 - val_mae: 0.0215\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 0.0010 - val_mae: 0.0241\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 8.9763e-04 - val_mae: 0.0210\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 8.2356e-04 - val_mae: 0.0199\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0016 - mae: 0.0286 - val_loss: 6.8807e-04 - val_mae: 0.0187\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 7.9138e-04 - val_mae: 0.0200\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 7.8524e-04 - val_mae: 0.0193\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 8.1196e-04 - val_mae: 0.0212\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0276 - val_loss: 7.8225e-04 - val_mae: 0.0207\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 7.3939e-04 - val_mae: 0.0198\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 7.8387e-04 - val_mae: 0.0201\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 8.7912e-04 - val_mae: 0.0213\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 7.5780e-04 - val_mae: 0.0198\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.9792e-04 - val_mae: 0.0204\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 8.2278e-04 - val_mae: 0.0202\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 7.6322e-04 - val_mae: 0.0194\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 8.8231e-04 - val_mae: 0.0205\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.6386e-04 - val_mae: 0.0194\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 8.0544e-04 - val_mae: 0.0200\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 7.0468e-04 - val_mae: 0.0183\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 9.3577e-04 - val_mae: 0.0227\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 9.2337e-04 - val_mae: 0.0214\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 9.5924e-04 - val_mae: 0.0234\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 0.0014 - val_mae: 0.0293\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 7.5074e-04 - val_mae: 0.0193\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 6.9919e-04 - val_mae: 0.0183\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 7.0424e-04 - val_mae: 0.0189\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 8.2517e-04 - val_mae: 0.0204\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 8.0714e-04 - val_mae: 0.0205\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 7.2420e-04 - val_mae: 0.0186\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 8.0537e-04 - val_mae: 0.0206\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0270 - val_loss: 8.5042e-04 - val_mae: 0.0217\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.4208e-04 - val_mae: 0.0201\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 7.1223e-04 - val_mae: 0.0192\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 7.6394e-04 - val_mae: 0.0194\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 7.6218e-04 - val_mae: 0.0189\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 7.6643e-04 - val_mae: 0.0198\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 7.2839e-04 - val_mae: 0.0190\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 7.7493e-04 - val_mae: 0.0205\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 7.6375e-04 - val_mae: 0.0189\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 7.3289e-04 - val_mae: 0.0196\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 7.7810e-04 - val_mae: 0.0195\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.3367e-04 - val_mae: 0.0208\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 9.5072e-04 - val_mae: 0.0221\n",
      "Epoch 174/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 7.6632e-04 - val_mae: 0.0199\n",
      "Epoch 175/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.1473e-04 - val_mae: 0.0205\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 9.7365e-04 - val_mae: 0.0237\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 7.8817e-04 - val_mae: 0.0200\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 8.1443e-04 - val_mae: 0.0210\n",
      "Epoch 179/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 8.0422e-04 - val_mae: 0.0199\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 8.3764e-04 - val_mae: 0.0212\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 7.2208e-04 - val_mae: 0.0189\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 8.6961e-04 - val_mae: 0.0212\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0014 - mae: 0.0263 - val_loss: 7.8018e-04 - val_mae: 0.0203\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 1s 6ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 7.4548e-04 - val_mae: 0.0201\n",
      "Epoch 185/200\n",
      "145/245 [================>.............] - ETA: 0s - loss: 0.0013 - mae: 0.0262"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('LSTM')\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_lstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_lstm.load_weights(filepath_attention)\n",
    "preds = atten_lstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "\n",
    "wk.save(f'LSTM Result.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_cnnlstm.hdf5'\n",
    "filepath_attention = 'attention_cnnlstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "simple_cnnlstm = keras.Sequential()\n",
    "simple_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "simple_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3))\n",
    "simple_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_cnnlstm.add(keras.layers.Flatten())\n",
    "simple_cnnlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(32))\n",
    "simple_cnnlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_cnnlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('CNN-LSTM')\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = simple_cnnlstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "simple_cnnlstm.load_weights(filepath_simple)\n",
    "preds = simple_cnnlstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention model\n",
    "\n",
    "K.clear_session()\n",
    "atten_cnnlstm = keras.Sequential()\n",
    "atten_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "atten_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3))\n",
    "atten_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_cnnlstm.add(attention(return_sequences=True))\n",
    "atten_cnnlstm.add(keras.layers.Flatten())\n",
    "atten_cnnlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(32))\n",
    "atten_cnnlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_cnnlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('CNN-LSTM')\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = atten_cnnlstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "atten_cnnlstm.load_weights(filepath_attention)\n",
    "preds = atten_cnnlstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('CNN-LStM Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_convlstm.hdf5'\n",
    "filepath_attention = 'attention_convlstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_conv =x_train.reshape(x_train.shape[0], 1, 1, x_train.shape[1], x_train.shape[2])\n",
    "x_test_conv = x_test.reshape(x_test.shape[0], 1, 1, x_test.shape[1], x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "simple_convlstm = keras.Sequential()\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True, \n",
    "                                            input_shape=(x_train_conv.shape[1], x_train_conv.shape[2], \n",
    "                                                         x_train_conv.shape[3], x_train_conv.shape[4])))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.Flatten())\n",
    "simple_convlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(32))\n",
    "simple_convlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_convlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('ConvLSTM')\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_convlstm.fit(x_train_conv,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_convlstm.load_weights(filepath_simple)\n",
    "preds = simple_convlstm.predict(x_test_conv)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "atten_convlstm = keras.Sequential()\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True, \n",
    "                                            input_shape=(x_train_conv.shape[1], x_train_conv.shape[2], \n",
    "                                                         x_train_conv.shape[3], x_train_conv.shape[4])))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(attention(return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.Flatten())\n",
    "atten_convlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(32))\n",
    "atten_convlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_convlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('ConvLSTM')\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_convlstm.fit(x_train_conv,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_convlstm.load_weights(filepath_attention)\n",
    "preds = atten_convlstm.predict(x_test_conv)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('ConvLSTM Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_seq2seq.hdf5'\n",
    "filepath_attention = 'attention_seq2seq.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_seq = y_train.reshape(y_train.shape[0], y_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_train = keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "output_train = keras.layers.Input(shape=(y_train_seq.shape[1], y_train_seq.shape[2]))\n",
    "\n",
    "## Encoder Section##\n",
    "encoder_first = keras.layers.LSTM(128, return_sequences=True, return_state=False)(input_train)\n",
    "encoder_second = keras.layers.LSTM(128, return_sequences=True)(encoder_first)\n",
    "encoder_third = keras.layers.LSTM(128, return_sequences=True)(encoder_second)\n",
    "encoder_fourth, encoder_fourth_s1, encoder_fourth_s2 = keras.layers.LSTM(128,return_sequences=False, return_state=True)(encoder_third)\n",
    "\n",
    "##Decorder Section##\n",
    "decoder_first = keras.layers.RepeatVector(output_train.shape[1])(encoder_fourth)\n",
    "decoder_second = keras.layers.LSTM(128, return_state=False, return_sequences=True)(decoder_first,initial_state=[encoder_fourth,encoder_fourth_s2])\n",
    "decoder_third = keras.layers.LSTM(128,return_sequences=True)(decoder_second)\n",
    "decoder_fourth = keras.layers.LSTM(128,return_sequences=True)(decoder_third)\n",
    "decoder_fifth = keras.layers.LSTM(128,return_sequences=True)(decoder_fourth)\n",
    "print(decoder_fifth)\n",
    "\n",
    "##Output Section##\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(output_train.shape[2]))(decoder_fifth)\n",
    "\n",
    "simple_seq = keras.Model(inputs=input_train, outputs=output)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "simple_seq.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "simple_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Seq2Seq')\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_seq.fit(x_train,y_train_seq,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_seq.load_weights(filepath_simple)\n",
    "preds = simple_seq.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " K.clear_session()\n",
    "\n",
    "input_train = keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "output_train = keras.layers.Input(shape=(y_train_seq.shape[1], y_train_seq.shape[2]))\n",
    "\n",
    "## Encoder Section##\n",
    "encoder_first = keras.layers.LSTM(128, return_sequences=True, return_state=False)(input_train)\n",
    "encoder_second = keras.layers.LSTM(128, return_sequences=True)(encoder_first)\n",
    "encoder_third = keras.layers.LSTM(128, return_sequences=True)(encoder_second)\n",
    "encoder_fourth, encoder_fourth_s1, encoder_fourth_s2 = keras.layers.LSTM(128,return_sequences=True,return_state=True)(encoder_third)\n",
    "\n",
    "##Decoder Section##\n",
    "decoder_first = keras.layers.RepeatVector(output_train.shape[1])(encoder_fourth_s1)\n",
    "decoder_second = keras.layers.LSTM(128, return_state=False, return_sequences=True)(decoder_first, initial_state=[encoder_fourth_s1, encoder_fourth_s2])\n",
    "\n",
    "attention = keras.layers.dot([decoder_second, encoder_fourth], axes=[2, 2])\n",
    "attention = keras.layers.Activation('softmax')(attention)\n",
    "context = keras.layers.dot([attention, encoder_fourth], axes=[2, 1])\n",
    "\n",
    "decoder_third = keras.layers.concatenate([context, decoder_second])\n",
    "\n",
    "decoder_fourth = keras.layers.LSTM(128, return_sequences=True)(decoder_third)\n",
    "decoder_fifth = keras.layers.LSTM(128, return_sequences=True)(decoder_fourth)\n",
    "decoder_sixth = keras.layers.LSTM(128, return_sequences=True)(decoder_fifth)\n",
    "\n",
    "##Output Section##\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(output_train.shape[2]))(decoder_sixth)\n",
    "\n",
    "atten_seq = keras.Model(inputs=input_train, outputs=output)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "atten_seq.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "atten_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Seq2Seq')\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_seq.fit(x_train,y_train_seq,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_seq.load_weights(filepath_attention)\n",
    "preds = atten_seq.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('Seq2Seq Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_wavenet.hdf5'\n",
    "filepath_attention = 'attention_wavenet.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 128\n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(7)]\n",
    "\n",
    "inputs = keras.layers.Input(shape=(x_train.shape[1],x_train.shape[2]))\n",
    "x=inputs\n",
    "\n",
    "skips = []\n",
    "for dilation_rate in dilation_rates:\n",
    "\n",
    "    x   = keras.layers.Conv1D(64, 1, padding='same')(x) \n",
    "    x_f = keras.layers.Conv1D(filters=n_filters,kernel_size=filter_width,padding='causal',dilation_rate=dilation_rate)(x)\n",
    "    x_g = keras.layers.Conv1D(filters=n_filters,kernel_size=filter_width, padding='causal',dilation_rate=dilation_rate)(x)\n",
    "\n",
    "    z = keras.layers.Multiply()([keras.layers.Activation('tanh')(x_f),keras.layers.Activation('sigmoid')(x_g)])\n",
    "\n",
    "    z = keras.layers.Conv1D(64, 1, padding='same', activation='relu')(z)\n",
    "\n",
    "    x = keras.layers.Add()([x, z])    \n",
    "\n",
    "    skips.append(z)\n",
    "\n",
    "out = keras.layers.Activation('relu')(keras.layers.Add()(skips)) \n",
    "out = keras.layers.Conv1D(128, 1, padding='same')(out)\n",
    "out = keras.layers.Activation('relu')(out)\n",
    "out = keras.layers.Dropout(0.4)(out)\n",
    "out = keras.layers.Conv1D(1, 1, padding='same')(out)\n",
    "\n",
    "out = keras.layers.Flatten()(out)\n",
    "out = keras.layers.Dense(6)(out)\n",
    "\n",
    "simple_wavenet = keras.Model(inputs, out)\n",
    "\n",
    "simple_wavenet.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Wavenet')\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = simple_wavenet.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "simple_wavenet.load_weights(filepath_simple)\n",
    "preds = simple_wavenet.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 128\n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(7)]\n",
    "\n",
    "inputs = Input(shape=(x_train.shape[1],x_train.shape[2]))\n",
    "x=inputs\n",
    "\n",
    "skips = []\n",
    "for dilation_rate in dilation_rates:\n",
    "\n",
    "    x   = Conv1D(64, 1, padding='same')(x) \n",
    "    x_f = Conv1D(filters=n_filters,kernel_size=filter_width,padding='causal',dilation_rate=dilation_rate)(x)\n",
    "    x_g = Conv1D(filters=n_filters,kernel_size=filter_width, padding='causal',dilation_rate=dilation_rate)(x)\n",
    "\n",
    "    z = Multiply()([keras.layers.Activation('tanh')(x_f),keras.layers.Activation('sigmoid')(x_g)])\n",
    "\n",
    "    z = Conv1D(64, 1, padding='same', activation='relu')(z)\n",
    "\n",
    "    x = Add()([x, z])    \n",
    "\n",
    "    skips.append(z)\n",
    "\n",
    "out = Activation('relu')(keras.layers.Add()(skips)) \n",
    "out = attention(return_sequences=True)(out)\n",
    "out = Conv1D(128, 1, padding='same')(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.4)(out)\n",
    "out = Conv1D(1, 1, padding='same')(out)\n",
    "# out = attention(return_sequences=True)(out)\n",
    "out = Flatten()(out)\n",
    "out = Dense(6)(out)\n",
    "\n",
    "atten_wavenet = keras.Model(inputs, out)\n",
    "\n",
    "atten_wavenet.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n",
    "atten_wavenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Wavenet')\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_wavenet.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_wavenet.load_weights(filepath_attention)\n",
    "preds = atten_wavenet.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('Wavenet Results.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk.save('Wavenet Results.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
