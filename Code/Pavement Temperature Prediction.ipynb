{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Loaded\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xlwt \n",
    "from xlwt import Workbook \n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "print('Libraries Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities \n",
    "\n",
    "def read_file(path):\n",
    "    df= pd.read_excel(path)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "def create_dataset(X, y, time_steps, ts_range):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps - ts_range):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.values[(i + time_steps):(i + time_steps + ts_range),0])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def splitter(df,output,lag,duration,ts):\n",
    "    assert (0. <= ts <= 1.)\n",
    "    train_size = int(len(df) * ts)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df[train_size:]\n",
    "    print(train.shape, test.shape)\n",
    "    scaler,scaler_single = MinMaxScaler(feature_range=(0, 1)),MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaler.fit(train)\n",
    "    scaler_single.fit(train[output])\n",
    "\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train), columns=[df.columns])\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test), columns=[df.columns])\n",
    "\n",
    "    df_train = train_scaled.copy(deep=True)\n",
    "    df_test = test_scaled.copy(deep=True)\n",
    "\n",
    "    x_train,y_train = create_dataset(df_train,df_train[[output]],lag,duration)\n",
    "    x_test, y_test = create_dataset(df_test, df_test[[output]], lag, duration)\n",
    "\n",
    "    return x_train,x_test,y_train,y_test,scaler_single\n",
    "\n",
    "class attention(keras.layers.Layer):\n",
    "    '''\n",
    "    if return_sequences=True, it will give 3D vector and if false it will give 2D vector. It is same as LSTMs.\n",
    "\n",
    "    https://stackoverflow.com/questions/62948332/how-to-add-attention-layer-to-a-bi-lstm/62949137#62949137\n",
    "    the  following code is being copied from the above link.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, return_sequences=True, **kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention, self).__init__()\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10896, 7)\n",
      "   Year  Month  Day  Hour  Temp  Solar  Pavement\n",
      "0  2009     11    1     1   8.4    0.0  9.333333\n",
      "1  2009     11    1     2   8.3    0.0  8.933333\n",
      "2  2009     11    1     3   7.9    0.0  8.700000\n",
      "3  2009     11    1     4   7.6    0.0  8.533333\n",
      "4  2009     11    1     5   6.9    0.0  8.533333\n"
     ]
    }
   ],
   "source": [
    "## Loading the file \n",
    "\n",
    "src = r'C:\\Users\\Saad.LAKES\\Desktop\\Pavement-Temperature-Prediction\\Data'\n",
    "filename = r'Pave_data_cleaned.xlsx'\n",
    "\n",
    "dest = r'C:\\Users\\Saad.LAKES\\Desktop\\Pavement-Temperature-Prediction\\Solutions'\n",
    "\n",
    "df = read_file(os.path.join(src,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8716, 2) (2180, 2)\n",
      "The shape of x_train is (8686, 24, 2) and x_test is (2150, 24, 2)\n",
      "The shape of y_train is (8686, 6) and y_test is (2150, 6)\n"
     ]
    }
   ],
   "source": [
    "## Training the training and testing data\n",
    "\n",
    "x_train,x_test,y_train,y_test,scaler = splitter(df[['Temp','Pavement']],['Pavement'],24,6,0.8)\n",
    "print(f'The shape of x_train is {x_train.shape} and x_test is {x_test.shape}')\n",
    "print(f'The shape of y_train is {y_train.shape} and y_test is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_lstm.hdf5'\n",
    "filepath_attention = 'attention_lstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple LSTM\n",
    "K.clear_session()\n",
    "simple_lstm = keras.Sequential()\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.Dropout(0.3))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.Flatten())\n",
    "simple_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dropout(0.3))\n",
    "simple_lstm.add(keras.layers.Dense(32))\n",
    "simple_lstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 2s 10ms/step - loss: 0.0123 - mae: 0.0804 - val_loss: 0.0024 - val_mae: 0.0386\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0055 - mae: 0.0554 - val_loss: 0.0030 - val_mae: 0.0422\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0044 - mae: 0.0487 - val_loss: 0.0018 - val_mae: 0.0334\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0035 - mae: 0.0427 - val_loss: 0.0012 - val_mae: 0.0275\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0029 - mae: 0.0391 - val_loss: 0.0010 - val_mae: 0.0257\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0026 - mae: 0.0372 - val_loss: 8.5992e-04 - val_mae: 0.0231\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0023 - mae: 0.0347 - val_loss: 9.4382e-04 - val_mae: 0.0236\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0022 - mae: 0.0339 - val_loss: 8.0202e-04 - val_mae: 0.0223\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0021 - mae: 0.0328 - val_loss: 6.9850e-04 - val_mae: 0.0206\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0021 - mae: 0.0329 - val_loss: 6.4615e-04 - val_mae: 0.0195\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0020 - mae: 0.0322 - val_loss: 7.0880e-04 - val_mae: 0.0205\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0020 - mae: 0.0324 - val_loss: 6.4716e-04 - val_mae: 0.0196\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0019 - mae: 0.0316 - val_loss: 6.0073e-04 - val_mae: 0.0192\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0019 - mae: 0.0314 - val_loss: 7.1979e-04 - val_mae: 0.0213\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 6.0946e-04 - val_mae: 0.0190\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 8.3034e-04 - val_mae: 0.0228\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 5.0359e-04 - val_mae: 0.0170\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 8.1243e-04 - val_mae: 0.0225\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0298 - val_loss: 5.9302e-04 - val_mae: 0.0184\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0288 - val_loss: 7.9148e-04 - val_mae: 0.0213\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 4.8167e-04 - val_mae: 0.0166\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 4.8721e-04 - val_mae: 0.0169\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 6.2407e-04 - val_mae: 0.0187\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 5.4239e-04 - val_mae: 0.0178\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 2s 6ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 5.6603e-04 - val_mae: 0.0184\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 5.3721e-04 - val_mae: 0.0170\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 5.2539e-04 - val_mae: 0.0174\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 7.4174e-04 - val_mae: 0.0213\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0282 - val_loss: 5.1700e-04 - val_mae: 0.0171\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 5.7042e-04 - val_mae: 0.0186\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 4.7680e-04 - val_mae: 0.0163\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 7.4395e-04 - val_mae: 0.0206\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 8.4198e-04 - val_mae: 0.0223\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0271 - val_loss: 5.9815e-04 - val_mae: 0.0188\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 5.5612e-04 - val_mae: 0.0185\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 6.4682e-04 - val_mae: 0.0205\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 5.6644e-04 - val_mae: 0.0184\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 6.0213e-04 - val_mae: 0.0189\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 5.4298e-04 - val_mae: 0.0177\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0272 - val_loss: 5.5568e-04 - val_mae: 0.0177\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 8.1512e-04 - val_mae: 0.0216\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 5.9721e-04 - val_mae: 0.0187\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.7201e-04 - val_mae: 0.0229\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 6.8882e-04 - val_mae: 0.0213\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0262 - val_loss: 5.8947e-04 - val_mae: 0.0186\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 4.9902e-04 - val_mae: 0.0166\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 7.9168e-04 - val_mae: 0.0212\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 5.6869e-04 - val_mae: 0.0181\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 6.1830e-04 - val_mae: 0.0193\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 0.0011 - val_mae: 0.0255\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 6.1996e-04 - val_mae: 0.0197\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0253 - val_loss: 5.7816e-04 - val_mae: 0.0178\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0260 - val_loss: 7.6795e-04 - val_mae: 0.0220\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0253 - val_loss: 4.2033e-04 - val_mae: 0.0150\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 4.8909e-04 - val_mae: 0.0166\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 4.6326e-04 - val_mae: 0.0158\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 4.5719e-04 - val_mae: 0.0162\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 4.6677e-04 - val_mae: 0.0163\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0244 - val_loss: 5.3057e-04 - val_mae: 0.0175\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 4.7764e-04 - val_mae: 0.0161\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 4.6118e-04 - val_mae: 0.0160\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0245 - val_loss: 4.9449e-04 - val_mae: 0.0165\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 5.5637e-04 - val_mae: 0.0186\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 5.4299e-04 - val_mae: 0.0178\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0241 - val_loss: 5.2563e-04 - val_mae: 0.0174\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0240 - val_loss: 5.4610e-04 - val_mae: 0.0183\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0241 - val_loss: 4.5580e-04 - val_mae: 0.0160\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 5.1066e-04 - val_mae: 0.0167\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 5.1687e-04 - val_mae: 0.0169\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0234 - val_loss: 4.5972e-04 - val_mae: 0.0157\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0232 - val_loss: 5.9071e-04 - val_mae: 0.0192\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 4.9464e-04 - val_mae: 0.0168\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0235 - val_loss: 6.4597e-04 - val_mae: 0.0189\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0234 - val_loss: 5.1968e-04 - val_mae: 0.0170\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 4.8083e-04 - val_mae: 0.0165\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0233 - val_loss: 4.7806e-04 - val_mae: 0.0164\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0233 - val_loss: 6.3470e-04 - val_mae: 0.0189\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0233 - val_loss: 6.3189e-04 - val_mae: 0.0187\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7888e-04 - mae: 0.0226 - val_loss: 5.2877e-04 - val_mae: 0.0179\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 5.0306e-04 - val_mae: 0.0165\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 4.5700e-04 - val_mae: 0.0162\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0232 - val_loss: 5.0494e-04 - val_mae: 0.0170\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.8312e-04 - mae: 0.0226 - val_loss: 5.0814e-04 - val_mae: 0.0172\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.9397e-04 - mae: 0.0228 - val_loss: 8.8217e-04 - val_mae: 0.0230\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 4.6492e-04 - val_mae: 0.0159\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7340e-04 - mae: 0.0225 - val_loss: 5.1525e-04 - val_mae: 0.0172\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7705e-04 - mae: 0.0226 - val_loss: 5.0066e-04 - val_mae: 0.0172\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7332e-04 - mae: 0.0226 - val_loss: 4.8759e-04 - val_mae: 0.0169\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.9977e-04 - mae: 0.0228 - val_loss: 6.6733e-04 - val_mae: 0.0204\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.8974e-04 - mae: 0.0226 - val_loss: 5.8880e-04 - val_mae: 0.0178\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.5440e-04 - mae: 0.0224 - val_loss: 5.8095e-04 - val_mae: 0.0185\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 5.0612e-04 - val_mae: 0.0167\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 4.6014e-04 - val_mae: 0.0158\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7752e-04 - mae: 0.0225 - val_loss: 5.6617e-04 - val_mae: 0.0182\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3885e-04 - mae: 0.0222 - val_loss: 5.1712e-04 - val_mae: 0.0171\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4527e-04 - mae: 0.0222 - val_loss: 4.7997e-04 - val_mae: 0.0161\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.8567e-04 - mae: 0.0227 - val_loss: 5.9201e-04 - val_mae: 0.0183\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4312e-04 - mae: 0.0224 - val_loss: 5.1717e-04 - val_mae: 0.0167\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.6947e-04 - mae: 0.0226 - val_loss: 5.4422e-04 - val_mae: 0.0179\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.6599e-04 - mae: 0.0224 - val_loss: 7.2282e-04 - val_mae: 0.0216\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.6805e-04 - mae: 0.0225 - val_loss: 4.7398e-04 - val_mae: 0.0157\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3067e-04 - mae: 0.0221 - val_loss: 5.1045e-04 - val_mae: 0.0169\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4459e-04 - mae: 0.0221 - val_loss: 4.6514e-04 - val_mae: 0.0159\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4890e-04 - mae: 0.0222 - val_loss: 5.1787e-04 - val_mae: 0.0171\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.2156e-04 - mae: 0.0219 - val_loss: 4.5490e-04 - val_mae: 0.0157\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.6342e-04 - mae: 0.0223 - val_loss: 4.5171e-04 - val_mae: 0.0151\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.6982e-04 - mae: 0.0224 - val_loss: 5.8115e-04 - val_mae: 0.0185\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3648e-04 - mae: 0.0221 - val_loss: 5.3498e-04 - val_mae: 0.0172\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 5.3083e-04 - val_mae: 0.0174\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1976e-04 - mae: 0.0219 - val_loss: 5.0072e-04 - val_mae: 0.0171\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.0966e-04 - mae: 0.0218 - val_loss: 4.8410e-04 - val_mae: 0.0168\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.2836e-04 - mae: 0.0220 - val_loss: 5.7655e-04 - val_mae: 0.0182\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7201e-04 - mae: 0.0224 - val_loss: 5.0895e-04 - val_mae: 0.0174\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1359e-04 - mae: 0.0218 - val_loss: 5.0561e-04 - val_mae: 0.0170\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.0073e-04 - mae: 0.0216 - val_loss: 4.6069e-04 - val_mae: 0.0160\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3849e-04 - mae: 0.0220 - val_loss: 5.7683e-04 - val_mae: 0.0184\n",
      "Epoch 117/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1180e-04 - mae: 0.0219 - val_loss: 5.9338e-04 - val_mae: 0.0193\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9575e-04 - mae: 0.0216 - val_loss: 4.2548e-04 - val_mae: 0.0151\n",
      "Epoch 119/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3316e-04 - mae: 0.0219 - val_loss: 7.4406e-04 - val_mae: 0.0205\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.2704e-04 - mae: 0.0219 - val_loss: 5.5844e-04 - val_mae: 0.0185\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.2129e-04 - mae: 0.0221 - val_loss: 7.4165e-04 - val_mae: 0.0208\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5709e-04 - mae: 0.0213 - val_loss: 5.4138e-04 - val_mae: 0.0178\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6919e-04 - mae: 0.0211 - val_loss: 4.7449e-04 - val_mae: 0.0165\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9850e-04 - mae: 0.0215 - val_loss: 5.5397e-04 - val_mae: 0.0182\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.0314e-04 - mae: 0.0217 - val_loss: 6.4313e-04 - val_mae: 0.0198\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9172e-04 - mae: 0.0214 - val_loss: 5.0269e-04 - val_mae: 0.0167\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8526e-04 - mae: 0.0214 - val_loss: 5.5526e-04 - val_mae: 0.0177\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3476e-04 - mae: 0.0221 - val_loss: 5.4648e-04 - val_mae: 0.0173\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9752e-04 - mae: 0.0215 - val_loss: 6.4062e-04 - val_mae: 0.0196\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3130e-04 - mae: 0.0221 - val_loss: 4.8789e-04 - val_mae: 0.0162\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.0542e-04 - mae: 0.0217 - val_loss: 4.9595e-04 - val_mae: 0.0166\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9032e-04 - mae: 0.0214 - val_loss: 4.3527e-04 - val_mae: 0.0153\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9420e-04 - mae: 0.0214 - val_loss: 4.7208e-04 - val_mae: 0.0159\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5185e-04 - mae: 0.0211 - val_loss: 4.6388e-04 - val_mae: 0.0159\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6162e-04 - mae: 0.0211 - val_loss: 4.5730e-04 - val_mae: 0.0155\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5141e-04 - mae: 0.0209 - val_loss: 5.1945e-04 - val_mae: 0.0168\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7205e-04 - mae: 0.0213 - val_loss: 4.3867e-04 - val_mae: 0.0154\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7321e-04 - mae: 0.0213 - val_loss: 4.5506e-04 - val_mae: 0.0156\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7122e-04 - mae: 0.0212 - val_loss: 5.7691e-04 - val_mae: 0.0179\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8921e-04 - mae: 0.0214 - val_loss: 5.2030e-04 - val_mae: 0.0164\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.3773e-04 - mae: 0.0209 - val_loss: 5.1477e-04 - val_mae: 0.0165\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7398e-04 - mae: 0.0213 - val_loss: 4.8543e-04 - val_mae: 0.0160\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6959e-04 - mae: 0.0213 - val_loss: 4.3276e-04 - val_mae: 0.0156\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8799e-04 - mae: 0.0214 - val_loss: 4.7675e-04 - val_mae: 0.0162\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1462e-04 - mae: 0.0208 - val_loss: 4.8233e-04 - val_mae: 0.0165\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.3411e-04 - mae: 0.0209 - val_loss: 4.6627e-04 - val_mae: 0.0159\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7824e-04 - mae: 0.0212 - val_loss: 5.0851e-04 - val_mae: 0.0171\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.3447e-04 - mae: 0.0208 - val_loss: 4.6272e-04 - val_mae: 0.0158\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1887e-04 - mae: 0.0206 - val_loss: 5.6429e-04 - val_mae: 0.0180\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9167e-04 - mae: 0.0215 - val_loss: 5.5831e-04 - val_mae: 0.0175\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7179e-04 - mae: 0.0212 - val_loss: 5.3227e-04 - val_mae: 0.0170\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5206e-04 - mae: 0.0210 - val_loss: 4.8540e-04 - val_mae: 0.0163\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1723e-04 - mae: 0.0207 - val_loss: 8.2628e-04 - val_mae: 0.0221\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.4806e-04 - mae: 0.0209 - val_loss: 5.2524e-04 - val_mae: 0.0172\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.4939e-04 - mae: 0.0209 - val_loss: 6.0685e-04 - val_mae: 0.0185\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6287e-04 - mae: 0.0210 - val_loss: 4.4847e-04 - val_mae: 0.0158\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2425e-04 - mae: 0.0207 - val_loss: 4.9187e-04 - val_mae: 0.0166\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.3728e-04 - mae: 0.0209 - val_loss: 5.7604e-04 - val_mae: 0.0188\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.3158e-04 - mae: 0.0207 - val_loss: 4.6198e-04 - val_mae: 0.0159\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9597e-04 - mae: 0.0205 - val_loss: 4.1488e-04 - val_mae: 0.0150\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2733e-04 - mae: 0.0208 - val_loss: 5.1045e-04 - val_mae: 0.0175\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2589e-04 - mae: 0.0206 - val_loss: 5.6840e-04 - val_mae: 0.0180\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7022e-04 - mae: 0.0212 - val_loss: 5.1315e-04 - val_mae: 0.0171\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2951e-04 - mae: 0.0211 - val_loss: 5.5702e-04 - val_mae: 0.0175\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9344e-04 - mae: 0.0203 - val_loss: 4.7404e-04 - val_mae: 0.0163\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2038e-04 - mae: 0.0205 - val_loss: 6.3239e-04 - val_mae: 0.0193\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.3142e-04 - mae: 0.0209 - val_loss: 5.0923e-04 - val_mae: 0.0171\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9760e-04 - mae: 0.0204 - val_loss: 4.5523e-04 - val_mae: 0.0157\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.0343e-04 - mae: 0.0204 - val_loss: 5.4410e-04 - val_mae: 0.0175\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6592e-04 - mae: 0.0212 - val_loss: 4.4099e-04 - val_mae: 0.0157\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.0867e-04 - mae: 0.0204 - val_loss: 5.5503e-04 - val_mae: 0.0181\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9892e-04 - mae: 0.0204 - val_loss: 5.7580e-04 - val_mae: 0.0179\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9529e-04 - mae: 0.0203 - val_loss: 5.0019e-04 - val_mae: 0.0167\n",
      "Epoch 174/200\n",
      "159/245 [==================>...........] - ETA: 0s - loss: 8.7587e-04 - mae: 0.0212"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('LSTM')\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_lstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_lstm.load_weights(filepath_simple)\n",
    "preds = simple_lstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention model\n",
    "\n",
    "K.clear_session()\n",
    "atten_lstm = keras.Sequential()\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "# atten_lstm.add(attention(return_sequences=True))\n",
    "atten_lstm.add(keras.layers.Dropout(0.3))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_lstm.add(attention(return_sequences=True))\n",
    "atten_lstm.add(keras.layers.Flatten())\n",
    "atten_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dropout(0.3))\n",
    "atten_lstm.add(keras.layers.Dense(32))\n",
    "atten_lstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('LSTM')\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_lstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_lstm.load_weights(filepath_attention)\n",
    "preds = atten_lstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "\n",
    "wk.save(f'LSTM Result.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_cnnlstm.hdf5'\n",
    "filepath_attention = 'attention_cnnlstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "simple_cnnlstm = keras.Sequential()\n",
    "simple_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "simple_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3))\n",
    "simple_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_cnnlstm.add(keras.layers.Flatten())\n",
    "simple_cnnlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(32))\n",
    "simple_cnnlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_cnnlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('CNN-LSTM')\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = simple_cnnlstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "simple_cnnlstm.load_weights(filepath_simple)\n",
    "preds = simple_cnnlstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention model\n",
    "\n",
    "K.clear_session()\n",
    "atten_cnnlstm = keras.Sequential()\n",
    "atten_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "atten_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3))\n",
    "atten_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_cnnlstm.add(attention(return_sequences=True))\n",
    "atten_cnnlstm.add(keras.layers.Flatten())\n",
    "atten_cnnlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(32))\n",
    "atten_cnnlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_cnnlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('CNN-LSTM')\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = atten_cnnlstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "atten_cnnlstm.load_weights(filepath_attention)\n",
    "preds = atten_cnnlstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('CNN-LStM Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_convlstm.hdf5'\n",
    "filepath_attention = 'attention_convlstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_conv =x_train.reshape(x_train.shape[0], 1, 1, x_train.shape[1], x_train.shape[2])\n",
    "x_test_conv = x_test.reshape(x_test.shape[0], 1, 1, x_test.shape[1], x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "simple_convlstm = keras.Sequential()\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True, \n",
    "                                            input_shape=(x_train_conv.shape[1], x_train_conv.shape[2], \n",
    "                                                         x_train_conv.shape[3], x_train_conv.shape[4])))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.Flatten())\n",
    "simple_convlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(32))\n",
    "simple_convlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_convlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('ConvLSTM')\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_convlstm.fit(x_train_conv,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_convlstm.load_weights(filepath_simple)\n",
    "preds = simple_convlstm.predict(x_test_conv)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "atten_convlstm = keras.Sequential()\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True, \n",
    "                                            input_shape=(x_train_conv.shape[1], x_train_conv.shape[2], \n",
    "                                                         x_train_conv.shape[3], x_train_conv.shape[4])))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(attention(return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.Flatten())\n",
    "atten_convlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(32))\n",
    "atten_convlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_convlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('ConvLSTM')\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_convlstm.fit(x_train_conv,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_convlstm.load_weights(filepath_attention)\n",
    "preds = atten_convlstm.predict(x_test_conv)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('ConvLSTM Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_seq2seq.hdf5'\n",
    "filepath_attention = 'attention_seq2seq.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_seq = y_train.reshape(y_train.shape[0], y_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "input_train = keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "output_train = keras.layers.Input(shape=(y_train_seq.shape[1], y_train_seq.shape[2]))\n",
    "\n",
    "## Encoder Section##\n",
    "encoder_first = keras.layers.LSTM(128, return_sequences=True, return_state=False)(input_train)\n",
    "encoder_second = keras.layers.LSTM(128, return_sequences=True)(encoder_first)\n",
    "encoder_third = keras.layers.LSTM(128, return_sequences=True)(encoder_second)\n",
    "encoder_fourth, encoder_fourth_s1, encoder_fourth_s2 = keras.layers.LSTM(128,return_sequences=False, return_state=True)(encoder_third)\n",
    "\n",
    "##Decorder Section##\n",
    "decoder_first = keras.layers.RepeatVector(output_train.shape[1])(encoder_fourth)\n",
    "decoder_second = keras.layers.LSTM(128, return_state=False, return_sequences=True)(decoder_first,initial_state=[encoder_fourth,encoder_fourth_s2])\n",
    "decoder_third = keras.layers.LSTM(128,return_sequences=True)(decoder_second)\n",
    "decoder_fourth = keras.layers.LSTM(128,return_sequences=True)(decoder_third)\n",
    "decoder_fifth = keras.layers.LSTM(128,return_sequences=True)(decoder_fourth)\n",
    "print(decoder_fifth)\n",
    "\n",
    "##Output Section##\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(output_train.shape[2]))(decoder_fifth)\n",
    "\n",
    "simple_seq = keras.Model(inputs=input_train, outputs=output)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "simple_seq.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "simple_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Seq2Seq')\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_seq.fit(x_train,y_train_seq,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_seq.load_weights(filepath_simple)\n",
    "preds = simple_seq.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " K.clear_session()\n",
    "\n",
    "input_train = keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "output_train = keras.layers.Input(shape=(y_train_seq.shape[1], y_train_seq.shape[2]))\n",
    "\n",
    "## Encoder Section##\n",
    "encoder_first = keras.layers.LSTM(128, return_sequences=True, return_state=False)(input_train)\n",
    "encoder_second = keras.layers.LSTM(128, return_sequences=True)(encoder_first)\n",
    "encoder_third = keras.layers.LSTM(128, return_sequences=True)(encoder_second)\n",
    "encoder_fourth, encoder_fourth_s1, encoder_fourth_s2 = keras.layers.LSTM(128,return_sequences=True,return_state=True)(encoder_third)\n",
    "\n",
    "##Decoder Section##\n",
    "decoder_first = keras.layers.RepeatVector(output_train.shape[1])(encoder_fourth_s1)\n",
    "decoder_second = keras.layers.LSTM(128, return_state=False, return_sequences=True)(decoder_first, initial_state=[encoder_fourth_s1, encoder_fourth_s2])\n",
    "\n",
    "attention = keras.layers.dot([decoder_second, encoder_fourth], axes=[2, 2])\n",
    "attention = keras.layers.Activation('softmax')(attention)\n",
    "context = keras.layers.dot([attention, encoder_fourth], axes=[2, 1])\n",
    "\n",
    "decoder_third = keras.layers.concatenate([context, decoder_second])\n",
    "\n",
    "decoder_fourth = keras.layers.LSTM(128, return_sequences=True)(decoder_third)\n",
    "decoder_fifth = keras.layers.LSTM(128, return_sequences=True)(decoder_fourth)\n",
    "decoder_sixth = keras.layers.LSTM(128, return_sequences=True)(decoder_fifth)\n",
    "\n",
    "##Output Section##\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(output_train.shape[2]))(decoder_sixth)\n",
    "\n",
    "atten_seq = keras.Model(inputs=input_train, outputs=output)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "atten_seq.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "atten_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Seq2Seq')\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_seq.fit(x_train,y_train_seq,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_seq.load_weights(filepath_attention)\n",
    "preds = atten_seq.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('Seq2Seq Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_wavenet.hdf5'\n",
    "filepath_attention = 'attention_wavenet.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 128\n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(7)]\n",
    "\n",
    "inputs = keras.layers.Input(shape=(x_train.shape[1],x_train.shape[2]))\n",
    "x=inputs\n",
    "\n",
    "skips = []\n",
    "for dilation_rate in dilation_rates:\n",
    "\n",
    "    x   = keras.layers.Conv1D(64, 1, padding='same')(x) \n",
    "    x_f = keras.layers.Conv1D(filters=n_filters,kernel_size=filter_width,padding='causal',dilation_rate=dilation_rate)(x)\n",
    "    x_g = keras.layers.Conv1D(filters=n_filters,kernel_size=filter_width, padding='causal',dilation_rate=dilation_rate)(x)\n",
    "\n",
    "    z = keras.layers.Multiply()([keras.layers.Activation('tanh')(x_f),keras.layers.Activation('sigmoid')(x_g)])\n",
    "\n",
    "    z = keras.layers.Conv1D(64, 1, padding='same', activation='relu')(z)\n",
    "\n",
    "    x = keras.layers.Add()([x, z])    \n",
    "\n",
    "    skips.append(z)\n",
    "\n",
    "out = keras.layers.Activation('relu')(keras.layers.Add()(skips)) \n",
    "out = keras.layers.Conv1D(128, 1, padding='same')(out)\n",
    "out = keras.layers.Activation('relu')(out)\n",
    "out = keras.layers.Dropout(0.4)(out)\n",
    "out = keras.layers.Conv1D(1, 1, padding='same')(out)\n",
    "\n",
    "out = keras.layers.Flatten()(out)\n",
    "out = keras.layers.Dense(6)(out)\n",
    "\n",
    "simple_wavenet = keras.Model(inputs, out)\n",
    "\n",
    "simple_wavenet.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "(2150, 6)\n",
      "The Mean Squared Error is: 5.5206542048612635\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Wavenet')\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = simple_wavenet.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "simple_wavenet.load_weights(filepath_simple)\n",
    "preds = simple_wavenet.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 24, 2)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 24, 64)       192         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 24, 128)      16512       conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 24, 128)      16512       conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 24, 128)      0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 24, 128)      0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 24, 128)      0           activation_16[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 24, 64)       8256        multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 24, 64)       0           conv1d_30[0][0]                  \n",
      "                                                                 conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 24, 64)       4160        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 24, 128)      16512       conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 24, 128)      16512       conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 24, 128)      0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 24, 128)      0           conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 24, 128)      0           activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, 24, 64)       8256        multiply_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 24, 64)       0           conv1d_34[0][0]                  \n",
      "                                                                 conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 24, 64)       4160        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 24, 128)      16512       conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 24, 128)      16512       conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 24, 128)      0           conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 24, 128)      0           conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 24, 128)      0           activation_20[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 24, 64)       8256        multiply_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 24, 64)       0           conv1d_38[0][0]                  \n",
      "                                                                 conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 24, 64)       4160        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, 24, 128)      16512       conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 24, 128)      16512       conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 24, 128)      0           conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 24, 128)      0           conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 24, 128)      0           activation_22[0][0]              \n",
      "                                                                 activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 24, 64)       8256        multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 24, 64)       0           conv1d_42[0][0]                  \n",
      "                                                                 conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 24, 64)       4160        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 24, 128)      16512       conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 24, 128)      16512       conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 24, 128)      0           conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 24, 128)      0           conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 24, 128)      0           activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, 24, 64)       8256        multiply_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 24, 64)       0           conv1d_46[0][0]                  \n",
      "                                                                 conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, 24, 64)       4160        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, 24, 128)      16512       conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, 24, 128)      16512       conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 24, 128)      0           conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 24, 128)      0           conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 24, 128)      0           activation_26[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 24, 64)       8256        multiply_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 24, 64)       0           conv1d_50[0][0]                  \n",
      "                                                                 conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 24, 64)       4160        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 24, 128)      16512       conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, 24, 128)      16512       conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 24, 128)      0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 24, 128)      0           conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 24, 128)      0           activation_28[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, 24, 64)       8256        multiply_13[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 24, 64)       0           conv1d_33[0][0]                  \n",
      "                                                                 conv1d_37[0][0]                  \n",
      "                                                                 conv1d_41[0][0]                  \n",
      "                                                                 conv1d_45[0][0]                  \n",
      "                                                                 conv1d_49[0][0]                  \n",
      "                                                                 conv1d_53[0][0]                  \n",
      "                                                                 conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 24, 64)       0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (attention)         (None, 24, 64)       88          activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, 24, 128)      8320        attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 24, 128)      0           conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 24, 128)      0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, 24, 1)        129         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 24)           0           conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            150         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 322,799\n",
      "Trainable params: 322,799\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_filters = 128\n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(7)]\n",
    "\n",
    "inputs = Input(shape=(x_train.shape[1],x_train.shape[2]))\n",
    "x=inputs\n",
    "\n",
    "skips = []\n",
    "for dilation_rate in dilation_rates:\n",
    "\n",
    "    x   = Conv1D(64, 1, padding='same')(x) \n",
    "    x_f = Conv1D(filters=n_filters,kernel_size=filter_width,padding='causal',dilation_rate=dilation_rate)(x)\n",
    "    x_g = Conv1D(filters=n_filters,kernel_size=filter_width, padding='causal',dilation_rate=dilation_rate)(x)\n",
    "\n",
    "    z = Multiply()([keras.layers.Activation('tanh')(x_f),keras.layers.Activation('sigmoid')(x_g)])\n",
    "\n",
    "    z = Conv1D(64, 1, padding='same', activation='relu')(z)\n",
    "\n",
    "    x = Add()([x, z])    \n",
    "\n",
    "    skips.append(z)\n",
    "\n",
    "out = Activation('relu')(keras.layers.Add()(skips)) \n",
    "out = attention(return_sequences=True)(out)\n",
    "out = Conv1D(128, 1, padding='same')(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(0.4)(out)\n",
    "out = Conv1D(1, 1, padding='same')(out)\n",
    "# out = attention(return_sequences=True)(out)\n",
    "out = Flatten()(out)\n",
    "out = Dense(6)(out)\n",
    "\n",
    "atten_wavenet = keras.Model(inputs, out)\n",
    "\n",
    "atten_wavenet.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n",
    "atten_wavenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 3s 10ms/step - loss: 0.0099 - mae: 0.0668 - val_loss: 0.0016 - val_mae: 0.0311\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.0035 - mae: 0.0438 - val_loss: 0.0013 - val_mae: 0.0283\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.0028 - mae: 0.0388 - val_loss: 9.7996e-04 - val_mae: 0.0239\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.0021 - mae: 0.0338 - val_loss: 9.1804e-04 - val_mae: 0.0236\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.0019 - mae: 0.0317 - val_loss: 9.1355e-04 - val_mae: 0.0232\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0018 - mae: 0.0311 - val_loss: 8.9225e-04 - val_mae: 0.0234\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0017 - mae: 0.0296 - val_loss: 0.0012 - val_mae: 0.0282\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0015 - mae: 0.0285 - val_loss: 6.8777e-04 - val_mae: 0.0202\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0014 - mae: 0.0274 - val_loss: 0.0011 - val_mae: 0.0261\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0014 - mae: 0.0271 - val_loss: 7.0481e-04 - val_mae: 0.0206\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 6.1484e-04 - val_mae: 0.0188\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 5.3740e-04 - val_mae: 0.0176\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0012 - mae: 0.0244 - val_loss: 9.1798e-04 - val_mae: 0.0236\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0012 - mae: 0.0247 - val_loss: 0.0012 - val_mae: 0.0290\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 5.7310e-04 - val_mae: 0.0183\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0012 - mae: 0.0253 - val_loss: 5.6819e-04 - val_mae: 0.0180\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0011 - mae: 0.0243 - val_loss: 6.8779e-04 - val_mae: 0.0206\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 5.3343e-04 - val_mae: 0.0172\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 8.1554e-04 - val_mae: 0.0224\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 5.2261e-04 - val_mae: 0.0171\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 0.0010 - mae: 0.0230 - val_loss: 4.6052e-04 - val_mae: 0.0164\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 9.9776e-04 - mae: 0.0225 - val_loss: 5.1373e-04 - val_mae: 0.0168\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 8.1836e-04 - val_mae: 0.0234\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0010 - mae: 0.0230 - val_loss: 5.6607e-04 - val_mae: 0.0181\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 9.5596e-04 - mae: 0.0221 - val_loss: 5.5985e-04 - val_mae: 0.0183\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 9.1380e-04 - mae: 0.0216 - val_loss: 4.5307e-04 - val_mae: 0.0159\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 9.3909e-04 - mae: 0.0220 - val_loss: 7.0907e-04 - val_mae: 0.0203\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 9.7288e-04 - mae: 0.0225 - val_loss: 8.1088e-04 - val_mae: 0.0222\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 8.6965e-04 - mae: 0.0211 - val_loss: 5.8737e-04 - val_mae: 0.0186\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 8.0281e-04 - mae: 0.0203 - val_loss: 4.4903e-04 - val_mae: 0.0158\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 8.5986e-04 - mae: 0.0212 - val_loss: 5.0728e-04 - val_mae: 0.0170\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 8.3551e-04 - mae: 0.0209 - val_loss: 5.0984e-04 - val_mae: 0.0169\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 8.2530e-04 - mae: 0.0207 - val_loss: 5.6004e-04 - val_mae: 0.0179\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 8.2195e-04 - mae: 0.0209 - val_loss: 6.7366e-04 - val_mae: 0.0198\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 8.5907e-04 - mae: 0.0212 - val_loss: 6.2932e-04 - val_mae: 0.0193\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 7.2400e-04 - mae: 0.0195 - val_loss: 5.3514e-04 - val_mae: 0.0177\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 7.2620e-04 - mae: 0.0195 - val_loss: 8.5062e-04 - val_mae: 0.0231\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 8.2941e-04 - mae: 0.0209 - val_loss: 8.4333e-04 - val_mae: 0.0228\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 6.9059e-04 - mae: 0.0191 - val_loss: 6.1383e-04 - val_mae: 0.0189\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 6.6946e-04 - mae: 0.0190 - val_loss: 6.3759e-04 - val_mae: 0.0196\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 6.6952e-04 - mae: 0.0190 - val_loss: 4.8051e-04 - val_mae: 0.0164\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 6.7494e-04 - mae: 0.0189 - val_loss: 6.1696e-04 - val_mae: 0.0189\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 6.1257e-04 - mae: 0.0183 - val_loss: 5.0331e-04 - val_mae: 0.0166\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 6.1794e-04 - mae: 0.0184 - val_loss: 5.5063e-04 - val_mae: 0.0175\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 5.7952e-04 - mae: 0.0178 - val_loss: 5.8307e-04 - val_mae: 0.0185\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 5.9207e-04 - mae: 0.0182 - val_loss: 5.8681e-04 - val_mae: 0.0176\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 5.8155e-04 - mae: 0.0179 - val_loss: 7.3334e-04 - val_mae: 0.0211\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 5.9667e-04 - mae: 0.0180 - val_loss: 7.0986e-04 - val_mae: 0.0205\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 5.0445e-04 - mae: 0.0167 - val_loss: 5.4614e-04 - val_mae: 0.0175\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 5.1987e-04 - mae: 0.0170 - val_loss: 5.6246e-04 - val_mae: 0.0176\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 4.6802e-04 - mae: 0.0161 - val_loss: 6.8300e-04 - val_mae: 0.0197\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 4.4024e-04 - mae: 0.0157 - val_loss: 5.9352e-04 - val_mae: 0.0181\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 4.8356e-04 - mae: 0.0164 - val_loss: 5.9053e-04 - val_mae: 0.0184\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 4.5087e-04 - mae: 0.0159 - val_loss: 5.8346e-04 - val_mae: 0.0180\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 4.5220e-04 - mae: 0.0160 - val_loss: 5.4586e-04 - val_mae: 0.0177\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 4.4689e-04 - mae: 0.0160 - val_loss: 7.6234e-04 - val_mae: 0.0214\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 4.0844e-04 - mae: 0.0152 - val_loss: 5.8669e-04 - val_mae: 0.0184\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 4.0696e-04 - mae: 0.0152 - val_loss: 5.7396e-04 - val_mae: 0.0181\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 4.3142e-04 - mae: 0.0156 - val_loss: 5.7589e-04 - val_mae: 0.0179\n",
      "Epoch 60/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 3.7139e-04 - mae: 0.0145 - val_loss: 5.6511e-04 - val_mae: 0.0177\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 3.8145e-04 - mae: 0.0147 - val_loss: 5.5268e-04 - val_mae: 0.0172\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 3.2734e-04 - mae: 0.0137 - val_loss: 6.3067e-04 - val_mae: 0.0185\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 3.3116e-04 - mae: 0.0137 - val_loss: 7.4706e-04 - val_mae: 0.0205\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 3.2136e-04 - mae: 0.0135 - val_loss: 6.1386e-04 - val_mae: 0.0185\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 3.1149e-04 - mae: 0.0134 - val_loss: 6.8842e-04 - val_mae: 0.0202\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 3.3556e-04 - mae: 0.0138 - val_loss: 7.4049e-04 - val_mae: 0.0210\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 3.0893e-04 - mae: 0.0133 - val_loss: 6.0126e-04 - val_mae: 0.0185\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 3.0911e-04 - mae: 0.0133 - val_loss: 5.7400e-04 - val_mae: 0.0182\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 3.1066e-04 - mae: 0.0133 - val_loss: 5.8922e-04 - val_mae: 0.0187\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 3.1690e-04 - mae: 0.0135 - val_loss: 6.2093e-04 - val_mae: 0.0190\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.9217e-04 - mae: 0.0129 - val_loss: 6.2409e-04 - val_mae: 0.0189\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.7715e-04 - mae: 0.0126 - val_loss: 6.4851e-04 - val_mae: 0.0196\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.5917e-04 - mae: 0.0121 - val_loss: 6.3497e-04 - val_mae: 0.0191\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.5272e-04 - mae: 0.0121 - val_loss: 5.9939e-04 - val_mae: 0.0190\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.5642e-04 - mae: 0.0121 - val_loss: 5.4563e-04 - val_mae: 0.0179\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.6058e-04 - mae: 0.0122 - val_loss: 6.4748e-04 - val_mae: 0.0198\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.9236e-04 - mae: 0.0129 - val_loss: 6.4511e-04 - val_mae: 0.0195\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.6667e-04 - mae: 0.0123 - val_loss: 5.9554e-04 - val_mae: 0.0181\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.3789e-04 - mae: 0.0116 - val_loss: 5.7793e-04 - val_mae: 0.0176\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.4314e-04 - mae: 0.0118 - val_loss: 6.0403e-04 - val_mae: 0.0183\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.4007e-04 - mae: 0.0117 - val_loss: 5.4551e-04 - val_mae: 0.0173\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.4640e-04 - mae: 0.0119 - val_loss: 6.0588e-04 - val_mae: 0.0190\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.3612e-04 - mae: 0.0116 - val_loss: 5.3410e-04 - val_mae: 0.0172\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.3503e-04 - mae: 0.0115 - val_loss: 6.1132e-04 - val_mae: 0.0182\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.4713e-04 - mae: 0.0119 - val_loss: 5.7905e-04 - val_mae: 0.0182\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.2998e-04 - mae: 0.0114 - val_loss: 5.8714e-04 - val_mae: 0.0186\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.2530e-04 - mae: 0.0114 - val_loss: 6.1619e-04 - val_mae: 0.0181\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.1934e-04 - mae: 0.0112 - val_loss: 5.8835e-04 - val_mae: 0.0185\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.0930e-04 - mae: 0.0109 - val_loss: 5.8771e-04 - val_mae: 0.0179\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.4384e-04 - mae: 0.0118 - val_loss: 5.8619e-04 - val_mae: 0.0183\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.1780e-04 - mae: 0.0111 - val_loss: 5.8515e-04 - val_mae: 0.0183\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.2891e-04 - mae: 0.0114 - val_loss: 6.2155e-04 - val_mae: 0.0187\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.1223e-04 - mae: 0.0110 - val_loss: 6.1953e-04 - val_mae: 0.0189\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.2842e-04 - mae: 0.0114 - val_loss: 7.4688e-04 - val_mae: 0.0214\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.0672e-04 - mae: 0.0109 - val_loss: 7.3520e-04 - val_mae: 0.0208\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.8630e-04 - mae: 0.0103 - val_loss: 6.1104e-04 - val_mae: 0.0185\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.2207e-04 - mae: 0.0112 - val_loss: 5.9470e-04 - val_mae: 0.0185\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.8076e-04 - mae: 0.0101 - val_loss: 6.1276e-04 - val_mae: 0.0192\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.8813e-04 - mae: 0.0103 - val_loss: 5.7571e-04 - val_mae: 0.0179\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.0490e-04 - mae: 0.0108 - val_loss: 6.4699e-04 - val_mae: 0.0195\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.9613e-04 - mae: 0.0105 - val_loss: 5.9777e-04 - val_mae: 0.0186\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.1505e-04 - mae: 0.0110 - val_loss: 6.2103e-04 - val_mae: 0.0191\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.9400e-04 - mae: 0.0105 - val_loss: 6.3193e-04 - val_mae: 0.0189\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.9116e-04 - mae: 0.0104 - val_loss: 6.5759e-04 - val_mae: 0.0196\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.7293e-04 - mae: 0.0100 - val_loss: 5.6069e-04 - val_mae: 0.0179\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.8537e-04 - mae: 0.0103 - val_loss: 6.2251e-04 - val_mae: 0.0192\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.0752e-04 - mae: 0.0110 - val_loss: 6.4126e-04 - val_mae: 0.0195\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.1612e-04 - mae: 0.0111 - val_loss: 8.4453e-04 - val_mae: 0.0230\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.9557e-04 - mae: 0.0105 - val_loss: 6.4759e-04 - val_mae: 0.0190\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.8461e-04 - mae: 0.0102 - val_loss: 5.8617e-04 - val_mae: 0.0186\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6266e-04 - mae: 0.0096 - val_loss: 5.4772e-04 - val_mae: 0.0175\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.7676e-04 - mae: 0.0100 - val_loss: 5.8215e-04 - val_mae: 0.0180\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.8018e-04 - mae: 0.0101 - val_loss: 8.9677e-04 - val_mae: 0.0238\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 2.0797e-04 - mae: 0.0108 - val_loss: 6.7617e-04 - val_mae: 0.0198\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.7105e-04 - mae: 0.0098 - val_loss: 6.6689e-04 - val_mae: 0.0195\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6792e-04 - mae: 0.0098 - val_loss: 5.9163e-04 - val_mae: 0.0183\n",
      "Epoch 117/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6488e-04 - mae: 0.0097 - val_loss: 6.7407e-04 - val_mae: 0.0200\n",
      "Epoch 118/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.7305e-04 - mae: 0.0099 - val_loss: 6.4008e-04 - val_mae: 0.0192\n",
      "Epoch 119/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.9132e-04 - mae: 0.0105 - val_loss: 5.9929e-04 - val_mae: 0.0184\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.7495e-04 - mae: 0.0100 - val_loss: 5.9747e-04 - val_mae: 0.0188\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5618e-04 - mae: 0.0094 - val_loss: 6.1097e-04 - val_mae: 0.0185\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6975e-04 - mae: 0.0098 - val_loss: 6.3204e-04 - val_mae: 0.0184\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5543e-04 - mae: 0.0094 - val_loss: 6.2755e-04 - val_mae: 0.0187\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5658e-04 - mae: 0.0094 - val_loss: 6.4936e-04 - val_mae: 0.0190\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6249e-04 - mae: 0.0096 - val_loss: 6.2568e-04 - val_mae: 0.0189\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6620e-04 - mae: 0.0097 - val_loss: 5.9511e-04 - val_mae: 0.0184\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6499e-04 - mae: 0.0097 - val_loss: 5.8639e-04 - val_mae: 0.0181\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.7426e-04 - mae: 0.0099 - val_loss: 6.3651e-04 - val_mae: 0.0191\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.7907e-04 - mae: 0.0100 - val_loss: 6.5034e-04 - val_mae: 0.0192\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4762e-04 - mae: 0.0091 - val_loss: 6.2945e-04 - val_mae: 0.0191\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.7701e-04 - mae: 0.0100 - val_loss: 6.7273e-04 - val_mae: 0.0196\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5886e-04 - mae: 0.0095 - val_loss: 6.2957e-04 - val_mae: 0.0188\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5743e-04 - mae: 0.0094 - val_loss: 5.6668e-04 - val_mae: 0.0180\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6308e-04 - mae: 0.0095 - val_loss: 6.7340e-04 - val_mae: 0.0200\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6939e-04 - mae: 0.0098 - val_loss: 6.5848e-04 - val_mae: 0.0196\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6448e-04 - mae: 0.0096 - val_loss: 5.9891e-04 - val_mae: 0.0187\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6527e-04 - mae: 0.0097 - val_loss: 8.5415e-04 - val_mae: 0.0231\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6234e-04 - mae: 0.0096 - val_loss: 6.4235e-04 - val_mae: 0.0191\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6472e-04 - mae: 0.0096 - val_loss: 6.3920e-04 - val_mae: 0.0187\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5790e-04 - mae: 0.0094 - val_loss: 7.4318e-04 - val_mae: 0.0209\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5944e-04 - mae: 0.0094 - val_loss: 7.2441e-04 - val_mae: 0.0206\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4705e-04 - mae: 0.0091 - val_loss: 7.7559e-04 - val_mae: 0.0221\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5238e-04 - mae: 0.0093 - val_loss: 6.4778e-04 - val_mae: 0.0195\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4774e-04 - mae: 0.0091 - val_loss: 7.8022e-04 - val_mae: 0.0220\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5393e-04 - mae: 0.0093 - val_loss: 6.3568e-04 - val_mae: 0.0191\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5604e-04 - mae: 0.0094 - val_loss: 6.4653e-04 - val_mae: 0.0194\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4331e-04 - mae: 0.0090 - val_loss: 6.3611e-04 - val_mae: 0.0195\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4496e-04 - mae: 0.0090 - val_loss: 6.2124e-04 - val_mae: 0.0193\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4076e-04 - mae: 0.0089 - val_loss: 7.0882e-04 - val_mae: 0.0201\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 1.3909e-04 - mae: 0.0089 - val_loss: 5.7791e-04 - val_mae: 0.0185\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5273e-04 - mae: 0.0093 - val_loss: 6.2583e-04 - val_mae: 0.0188\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3469e-04 - mae: 0.0087 - val_loss: 6.0303e-04 - val_mae: 0.0186\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4566e-04 - mae: 0.0090 - val_loss: 7.1679e-04 - val_mae: 0.0206\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.6062e-04 - mae: 0.0096 - val_loss: 6.0061e-04 - val_mae: 0.0188\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4683e-04 - mae: 0.0091 - val_loss: 7.1558e-04 - val_mae: 0.0206\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3199e-04 - mae: 0.0086 - val_loss: 7.2055e-04 - val_mae: 0.0207\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3240e-04 - mae: 0.0087 - val_loss: 6.9234e-04 - val_mae: 0.0201\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3756e-04 - mae: 0.0088 - val_loss: 6.7348e-04 - val_mae: 0.0195\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5510e-04 - mae: 0.0093 - val_loss: 8.0630e-04 - val_mae: 0.0216\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4030e-04 - mae: 0.0089 - val_loss: 6.8086e-04 - val_mae: 0.0203\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3154e-04 - mae: 0.0086 - val_loss: 6.9308e-04 - val_mae: 0.0198\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5820e-04 - mae: 0.0095 - val_loss: 6.8659e-04 - val_mae: 0.0198\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3550e-04 - mae: 0.0087 - val_loss: 6.6530e-04 - val_mae: 0.0193\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3081e-04 - mae: 0.0086 - val_loss: 6.8144e-04 - val_mae: 0.0195\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - ETA: 0s - loss: 1.3235e-04 - mae: 0.008 - 2s 8ms/step - loss: 1.3210e-04 - mae: 0.0086 - val_loss: 6.5833e-04 - val_mae: 0.0194\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.1915e-04 - mae: 0.0082 - val_loss: 6.8910e-04 - val_mae: 0.0199\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3967e-04 - mae: 0.0089 - val_loss: 6.3926e-04 - val_mae: 0.0189\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5958e-04 - mae: 0.0095 - val_loss: 7.0269e-04 - val_mae: 0.0197\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2442e-04 - mae: 0.0084 - val_loss: 6.2968e-04 - val_mae: 0.0188\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3021e-04 - mae: 0.0086 - val_loss: 6.0722e-04 - val_mae: 0.0188\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4480e-04 - mae: 0.0090 - val_loss: 6.9293e-04 - val_mae: 0.0199\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5349e-04 - mae: 0.0093 - val_loss: 7.0105e-04 - val_mae: 0.0202\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4446e-04 - mae: 0.0090 - val_loss: 7.1011e-04 - val_mae: 0.0197\n",
      "Epoch 174/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2960e-04 - mae: 0.0085 - val_loss: 6.4048e-04 - val_mae: 0.0192\n",
      "Epoch 175/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.1359e-04 - mae: 0.0080 - val_loss: 6.7580e-04 - val_mae: 0.0195\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2376e-04 - mae: 0.0084 - val_loss: 7.8303e-04 - val_mae: 0.0217\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2806e-04 - mae: 0.0085 - val_loss: 7.0585e-04 - val_mae: 0.0200\n",
      "Epoch 178/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2927e-04 - mae: 0.0085 - val_loss: 6.5420e-04 - val_mae: 0.0191\n",
      "Epoch 179/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4773e-04 - mae: 0.0091 - val_loss: 7.0540e-04 - val_mae: 0.0204\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2511e-04 - mae: 0.0084 - val_loss: 7.2155e-04 - val_mae: 0.0208\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.1819e-04 - mae: 0.0082 - val_loss: 8.1205e-04 - val_mae: 0.0225\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3357e-04 - mae: 0.0087 - val_loss: 7.2273e-04 - val_mae: 0.0205\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4343e-04 - mae: 0.0090 - val_loss: 7.5024e-04 - val_mae: 0.0215\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3056e-04 - mae: 0.0086 - val_loss: 6.9439e-04 - val_mae: 0.0199\n",
      "Epoch 185/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2475e-04 - mae: 0.0084 - val_loss: 6.7545e-04 - val_mae: 0.0195\n",
      "Epoch 186/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4099e-04 - mae: 0.0089 - val_loss: 7.6739e-04 - val_mae: 0.0210\n",
      "Epoch 187/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2397e-04 - mae: 0.0083 - val_loss: 7.3272e-04 - val_mae: 0.0206\n",
      "Epoch 188/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2070e-04 - mae: 0.0083 - val_loss: 7.3824e-04 - val_mae: 0.0208\n",
      "Epoch 189/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.1441e-04 - mae: 0.0080 - val_loss: 6.8557e-04 - val_mae: 0.0201\n",
      "Epoch 190/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2510e-04 - mae: 0.0084 - val_loss: 7.2666e-04 - val_mae: 0.0205\n",
      "Epoch 191/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.5227e-04 - mae: 0.0093 - val_loss: 6.8661e-04 - val_mae: 0.0204\n",
      "Epoch 192/200\n",
      "245/245 [==============================] - 2s 9ms/step - loss: 1.3012e-04 - mae: 0.0086 - val_loss: 6.8121e-04 - val_mae: 0.0201\n",
      "Epoch 193/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2647e-04 - mae: 0.0085 - val_loss: 6.3293e-04 - val_mae: 0.0192\n",
      "Epoch 194/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2688e-04 - mae: 0.0084 - val_loss: 6.4909e-04 - val_mae: 0.0193\n",
      "Epoch 195/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3072e-04 - mae: 0.0086 - val_loss: 6.1301e-04 - val_mae: 0.0189\n",
      "Epoch 196/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.4377e-04 - mae: 0.0090 - val_loss: 6.5427e-04 - val_mae: 0.0195\n",
      "Epoch 197/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.3308e-04 - mae: 0.0087 - val_loss: 8.2788e-04 - val_mae: 0.0225\n",
      "Epoch 198/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2246e-04 - mae: 0.0083 - val_loss: 6.6918e-04 - val_mae: 0.0199\n",
      "Epoch 199/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.2223e-04 - mae: 0.0083 - val_loss: 7.0083e-04 - val_mae: 0.0201\n",
      "Epoch 200/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 1.0867e-04 - mae: 0.0078 - val_loss: 6.6404e-04 - val_mae: 0.0193\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2150, 6)\n",
      "The Mean Squared Error is: 6.485297764252889\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Wavenet')\n",
    "    os.chdir(os.path.join(dest,'Wavenet'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_wavenet.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_wavenet.load_weights(filepath_attention)\n",
    "preds = atten_wavenet.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('Wavenet Results.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wk.save('Wavenet Results.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
