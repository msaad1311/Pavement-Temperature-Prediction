{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Loaded\n"
     ]
    }
   ],
   "source": [
    "# Libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xlwt \n",
    "from xlwt import Workbook \n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "print('Libraries Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities \n",
    "\n",
    "def read_file(path):\n",
    "    df= pd.read_excel(path)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    print(df.shape)\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "def create_dataset(X, y, time_steps, ts_range):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps - ts_range):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.values[(i + time_steps):(i + time_steps + ts_range),0])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def splitter(df,output,lag,duration,ts):\n",
    "    assert (0. <= ts <= 1.)\n",
    "    train_size = int(len(df) * ts)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df.iloc[0:train_size], df[train_size:]\n",
    "    print(train.shape, test.shape)\n",
    "    scaler,scaler_single = MinMaxScaler(feature_range=(0, 1)),MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    scaler.fit(train)\n",
    "    scaler_single.fit(train[output])\n",
    "\n",
    "    train_scaled = pd.DataFrame(scaler.transform(train), columns=[df.columns])\n",
    "    test_scaled = pd.DataFrame(scaler.transform(test), columns=[df.columns])\n",
    "\n",
    "    df_train = train_scaled.copy(deep=True)\n",
    "    df_test = test_scaled.copy(deep=True)\n",
    "\n",
    "    x_train,y_train = create_dataset(df_train,df_train[[output]],lag,duration)\n",
    "    x_test, y_test = create_dataset(df_test, df_test[[output]], lag, duration)\n",
    "\n",
    "    return x_train,x_test,y_train,y_test,scaler_single\n",
    "\n",
    "class attention(keras.layers.Layer):\n",
    "    '''\n",
    "    if return_sequences=True, it will give 3D vector and if false it will give 2D vector. It is same as LSTMs.\n",
    "\n",
    "    https://stackoverflow.com/questions/62948332/how-to-add-attention-layer-to-a-bi-lstm/62949137#62949137\n",
    "    the  following code is being copied from the above link.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, return_sequences=True, **kwargs):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention, self).__init__()\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        return cfg\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10896, 7)\n",
      "   Year  Month  Day  Hour  Temp  Solar  Pavement\n",
      "0  2009     11    1     1   8.4    0.0  9.333333\n",
      "1  2009     11    1     2   8.3    0.0  8.933333\n",
      "2  2009     11    1     3   7.9    0.0  8.700000\n",
      "3  2009     11    1     4   7.6    0.0  8.533333\n",
      "4  2009     11    1     5   6.9    0.0  8.533333\n"
     ]
    }
   ],
   "source": [
    "## Loading the file \n",
    "\n",
    "src = r'C:\\Users\\Saad.LAKES\\Desktop\\Pavement-Temperature-Prediction\\Data'\n",
    "filename = r'Pave_data_cleaned.xlsx'\n",
    "\n",
    "dest = r'C:\\Users\\Saad.LAKES\\Desktop\\Pavement-Temperature-Prediction\\Solutions'\n",
    "\n",
    "df = read_file(os.path.join(src,filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8716, 2) (2180, 2)\n",
      "The shape of x_train is (8686, 24, 2) and x_test is (2150, 24, 2)\n",
      "The shape of y_train is (8686, 6) and y_test is (2150, 6)\n"
     ]
    }
   ],
   "source": [
    "## Training the training and testing data\n",
    "\n",
    "x_train,x_test,y_train,y_test,scaler = splitter(df[['Temp','Pavement']],['Pavement'],24,6,0.8)\n",
    "print(f'The shape of x_train is {x_train.shape} and x_test is {x_test.shape}')\n",
    "print(f'The shape of y_train is {y_train.shape} and y_test is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_lstm.hdf5'\n",
    "filepath_attention = 'attention_lstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple LSTM\n",
    "K.clear_session()\n",
    "simple_lstm = keras.Sequential()\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.Dropout(0.3))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_lstm.add(keras.layers.Flatten())\n",
    "simple_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_lstm.add(keras.layers.Dropout(0.3))\n",
    "simple_lstm.add(keras.layers.Dense(32))\n",
    "simple_lstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "The Mean Squared Error is: 5.831630519312005\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('LSTM')\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = simple_lstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "simple_lstm.load_weights(filepath_simple)\n",
    "preds = simple_lstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention model\n",
    "\n",
    "K.clear_session()\n",
    "atten_lstm = keras.Sequential()\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "# atten_lstm.add(attention(return_sequences=True))\n",
    "atten_lstm.add(keras.layers.Dropout(0.3))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_lstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_lstm.add(attention(return_sequences=True))\n",
    "atten_lstm.add(keras.layers.Flatten())\n",
    "atten_lstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_lstm.add(keras.layers.Dropout(0.3))\n",
    "atten_lstm.add(keras.layers.Dense(32))\n",
    "atten_lstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_lstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 3s 10ms/step - loss: 0.0166 - mae: 0.0919 - val_loss: 0.0046 - val_mae: 0.0550\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0067 - mae: 0.0614 - val_loss: 0.0032 - val_mae: 0.0446\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0018 - val_mae: 0.0335\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0043 - mae: 0.0485 - val_loss: 0.0017 - val_mae: 0.0328\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0035 - mae: 0.0434 - val_loss: 0.0014 - val_mae: 0.0290\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0012 - val_mae: 0.0278\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0025 - mae: 0.0359 - val_loss: 8.1998e-04 - val_mae: 0.0230\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0023 - mae: 0.0345 - val_loss: 9.7574e-04 - val_mae: 0.0243\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0022 - mae: 0.0336 - val_loss: 7.8868e-04 - val_mae: 0.0227\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 2s 8ms/step - loss: 0.0021 - mae: 0.0326 - val_loss: 6.6729e-04 - val_mae: 0.0201\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0021 - mae: 0.0329 - val_loss: 8.4943e-04 - val_mae: 0.0225\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0020 - mae: 0.0316 - val_loss: 6.4670e-04 - val_mae: 0.0196\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0019 - mae: 0.0312 - val_loss: 5.9379e-04 - val_mae: 0.0191\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0305 - val_loss: 6.2856e-04 - val_mae: 0.0194\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0302 - val_loss: 5.0913e-04 - val_mae: 0.0172\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 7.6135e-04 - val_mae: 0.0214\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0018 - mae: 0.0303 - val_loss: 7.9579e-04 - val_mae: 0.0223\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0299 - val_loss: 5.0978e-04 - val_mae: 0.0174\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0291 - val_loss: 5.1046e-04 - val_mae: 0.0172\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 8.1448e-04 - val_mae: 0.0236\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0292 - val_loss: 5.9474e-04 - val_mae: 0.0184\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0017 - mae: 0.0293 - val_loss: 6.7651e-04 - val_mae: 0.0208\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0289 - val_loss: 7.7463e-04 - val_mae: 0.0218\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0292 - val_loss: 6.6507e-04 - val_mae: 0.0196\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0289 - val_loss: 8.9247e-04 - val_mae: 0.0236\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 5.9263e-04 - val_mae: 0.0188\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0283 - val_loss: 5.0157e-04 - val_mae: 0.0169\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 6.0686e-04 - val_mae: 0.0194\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0278 - val_loss: 4.6925e-04 - val_mae: 0.0167\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0281 - val_loss: 8.8835e-04 - val_mae: 0.0236\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0016 - mae: 0.0282 - val_loss: 0.0015 - val_mae: 0.0325\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 5.2127e-04 - val_mae: 0.0177\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 5.1254e-04 - val_mae: 0.0172\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 4.7164e-04 - val_mae: 0.0161\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 6.1034e-04 - val_mae: 0.0189\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0274 - val_loss: 6.2952e-04 - val_mae: 0.0197\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 0.0010 - val_mae: 0.0256\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0276 - val_loss: 7.1237e-04 - val_mae: 0.0214\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0015 - mae: 0.0273 - val_loss: 4.6310e-04 - val_mae: 0.0161\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0272 - val_loss: 6.4891e-04 - val_mae: 0.0190\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0267 - val_loss: 5.3622e-04 - val_mae: 0.0180\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 8.9043e-04 - val_mae: 0.0235\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0262 - val_loss: 6.7912e-04 - val_mae: 0.0198\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 5.0501e-04 - val_mae: 0.0171\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0262 - val_loss: 5.3638e-04 - val_mae: 0.0175\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 6.9804e-04 - val_mae: 0.0211\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 7.6167e-04 - val_mae: 0.0221\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 7.3928e-04 - val_mae: 0.0216\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 7.1989e-04 - val_mae: 0.0215\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 4.9343e-04 - val_mae: 0.0165\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0259 - val_loss: 6.0149e-04 - val_mae: 0.0183\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0257 - val_loss: 4.9858e-04 - val_mae: 0.0168\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 6.9319e-04 - val_mae: 0.0199\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 4.8471e-04 - val_mae: 0.0163\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0256 - val_loss: 7.5512e-04 - val_mae: 0.0227\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0252 - val_loss: 6.9901e-04 - val_mae: 0.0192\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 0.0014 - val_mae: 0.0317\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 6.0929e-04 - val_mae: 0.0187\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0013 - mae: 0.0252 - val_loss: 5.4922e-04 - val_mae: 0.0171\n",
      "Epoch 60/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0253 - val_loss: 4.8564e-04 - val_mae: 0.0166\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 4.5125e-04 - val_mae: 0.0159\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 5.4331e-04 - val_mae: 0.0177\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 4.4389e-04 - val_mae: 0.0156\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0243 - val_loss: 4.8704e-04 - val_mae: 0.0163\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0250 - val_loss: 4.6202e-04 - val_mae: 0.0157\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 6.0256e-04 - val_mae: 0.0186\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0251 - val_loss: 6.7643e-04 - val_mae: 0.0210\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0248 - val_loss: 4.3849e-04 - val_mae: 0.0155\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0246 - val_loss: 4.7764e-04 - val_mae: 0.0162\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0249 - val_loss: 5.9106e-04 - val_mae: 0.0185\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 4.8004e-04 - val_mae: 0.0162\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0239 - val_loss: 4.9090e-04 - val_mae: 0.0167\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0242 - val_loss: 5.1473e-04 - val_mae: 0.0167\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0241 - val_loss: 5.0822e-04 - val_mae: 0.0166\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0244 - val_loss: 5.6761e-04 - val_mae: 0.0183\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0234 - val_loss: 4.4018e-04 - val_mae: 0.0156\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 5.1249e-04 - val_mae: 0.0168\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0241 - val_loss: 5.7695e-04 - val_mae: 0.0187\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0012 - mae: 0.0244 - val_loss: 7.8082e-04 - val_mae: 0.0217\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 7.4469e-04 - val_mae: 0.0211\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0236 - val_loss: 4.6033e-04 - val_mae: 0.0160\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0232 - val_loss: 4.6329e-04 - val_mae: 0.0163\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 5.7212e-04 - val_mae: 0.0180\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 6.6960e-04 - val_mae: 0.0203\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0011 - mae: 0.0237 - val_loss: 7.7749e-04 - val_mae: 0.0219\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 5.6750e-04 - val_mae: 0.0181\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 5.4974e-04 - val_mae: 0.0172\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0236 - val_loss: 5.5364e-04 - val_mae: 0.0184\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.8295e-04 - mae: 0.0226 - val_loss: 4.5449e-04 - val_mae: 0.0160\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0231 - val_loss: 5.0291e-04 - val_mae: 0.0162\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.8726e-04 - mae: 0.0227 - val_loss: 5.9155e-04 - val_mae: 0.0179\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0228 - val_loss: 5.4267e-04 - val_mae: 0.0175\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.8774e-04 - mae: 0.0227 - val_loss: 5.6822e-04 - val_mae: 0.0183\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0232 - val_loss: 4.7659e-04 - val_mae: 0.0162\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0229 - val_loss: 4.8763e-04 - val_mae: 0.0161\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7805e-04 - mae: 0.0226 - val_loss: 7.5191e-04 - val_mae: 0.0206\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0233 - val_loss: 5.5288e-04 - val_mae: 0.0177\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 0.0010 - mae: 0.0227 - val_loss: 5.0948e-04 - val_mae: 0.0172\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4608e-04 - mae: 0.0221 - val_loss: 5.2689e-04 - val_mae: 0.0178\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.5670e-04 - mae: 0.0224 - val_loss: 4.9545e-04 - val_mae: 0.0168\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1978e-04 - mae: 0.0219 - val_loss: 5.5618e-04 - val_mae: 0.0181\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3504e-04 - mae: 0.0221 - val_loss: 5.0430e-04 - val_mae: 0.0169\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7556e-04 - mae: 0.0225 - val_loss: 5.6658e-04 - val_mae: 0.0182\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.6598e-04 - mae: 0.0225 - val_loss: 5.9647e-04 - val_mae: 0.0192\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4924e-04 - mae: 0.0223 - val_loss: 4.7508e-04 - val_mae: 0.0161\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.3782e-04 - mae: 0.0220 - val_loss: 5.4138e-04 - val_mae: 0.0173\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4446e-04 - mae: 0.0221 - val_loss: 5.3772e-04 - val_mae: 0.0178\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9935e-04 - mae: 0.0217 - val_loss: 4.8833e-04 - val_mae: 0.0167\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.2538e-04 - mae: 0.0221 - val_loss: 4.8161e-04 - val_mae: 0.0166\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.7096e-04 - mae: 0.0224 - val_loss: 9.7204e-04 - val_mae: 0.0236\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1934e-04 - mae: 0.0220 - val_loss: 5.0129e-04 - val_mae: 0.0161\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1058e-04 - mae: 0.0219 - val_loss: 5.0125e-04 - val_mae: 0.0164\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.0438e-04 - mae: 0.0216 - val_loss: 4.8580e-04 - val_mae: 0.0168\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.0101e-04 - mae: 0.0216 - val_loss: 6.0205e-04 - val_mae: 0.0188\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4897e-04 - mae: 0.0221 - val_loss: 4.4494e-04 - val_mae: 0.0155\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8195e-04 - mae: 0.0215 - val_loss: 4.6716e-04 - val_mae: 0.0161\n",
      "Epoch 117/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9175e-04 - mae: 0.0217 - val_loss: 5.6525e-04 - val_mae: 0.0183\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9078e-04 - mae: 0.0215 - val_loss: 5.2264e-04 - val_mae: 0.0172\n",
      "Epoch 119/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.0543e-04 - mae: 0.0217 - val_loss: 6.5982e-04 - val_mae: 0.0201\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9142e-04 - mae: 0.0216 - val_loss: 4.9140e-04 - val_mae: 0.0167\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7356e-04 - mae: 0.0212 - val_loss: 5.5472e-04 - val_mae: 0.0179\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9580e-04 - mae: 0.0214 - val_loss: 4.8830e-04 - val_mae: 0.0164\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9202e-04 - mae: 0.0214 - val_loss: 5.7242e-04 - val_mae: 0.0178\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8238e-04 - mae: 0.0213 - val_loss: 4.3649e-04 - val_mae: 0.0156\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.4146e-04 - mae: 0.0220 - val_loss: 7.1329e-04 - val_mae: 0.0210\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1766e-04 - mae: 0.0216 - val_loss: 5.4910e-04 - val_mae: 0.0180\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8786e-04 - mae: 0.0213 - val_loss: 4.9622e-04 - val_mae: 0.0167\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7645e-04 - mae: 0.0213 - val_loss: 5.0900e-04 - val_mae: 0.0172\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6961e-04 - mae: 0.0212 - val_loss: 4.6907e-04 - val_mae: 0.0160\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7499e-04 - mae: 0.0212 - val_loss: 4.5605e-04 - val_mae: 0.0159\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7462e-04 - mae: 0.0212 - val_loss: 5.2591e-04 - val_mae: 0.0169\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6613e-04 - mae: 0.0211 - val_loss: 4.5996e-04 - val_mae: 0.0158\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.9369e-04 - mae: 0.0213 - val_loss: 4.9049e-04 - val_mae: 0.0168\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6253e-04 - mae: 0.0212 - val_loss: 5.1040e-04 - val_mae: 0.0173\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5911e-04 - mae: 0.0210 - val_loss: 0.0011 - val_mae: 0.0270\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.7370e-04 - mae: 0.0212 - val_loss: 6.9705e-04 - val_mae: 0.0207\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 9.1493e-04 - mae: 0.0216 - val_loss: 5.6303e-04 - val_mae: 0.0179\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6946e-04 - mae: 0.0212 - val_loss: 5.7483e-04 - val_mae: 0.0183\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.8351e-04 - mae: 0.0211 - val_loss: 5.5137e-04 - val_mae: 0.0177\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6095e-04 - mae: 0.0212 - val_loss: 4.7717e-04 - val_mae: 0.0161\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.6476e-04 - mae: 0.0209 - val_loss: 5.4535e-04 - val_mae: 0.0182\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2093e-04 - mae: 0.0206 - val_loss: 5.4468e-04 - val_mae: 0.0174\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5931e-04 - mae: 0.0210 - val_loss: 5.3032e-04 - val_mae: 0.0172\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.4424e-04 - mae: 0.0208 - val_loss: 4.5169e-04 - val_mae: 0.0155\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5548e-04 - mae: 0.0209 - val_loss: 7.3757e-04 - val_mae: 0.0210\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.4553e-04 - mae: 0.0207 - val_loss: 4.8686e-04 - val_mae: 0.0167\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.4373e-04 - mae: 0.0208 - val_loss: 4.8167e-04 - val_mae: 0.0163\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5182e-04 - mae: 0.0209 - val_loss: 5.3387e-04 - val_mae: 0.0170\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1782e-04 - mae: 0.0207 - val_loss: 4.8156e-04 - val_mae: 0.0166\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1447e-04 - mae: 0.0205 - val_loss: 4.8484e-04 - val_mae: 0.0166\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.0512e-04 - mae: 0.0204 - val_loss: 5.2836e-04 - val_mae: 0.0171\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.0794e-04 - mae: 0.0205 - val_loss: 4.8704e-04 - val_mae: 0.0166\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2947e-04 - mae: 0.0205 - val_loss: 5.7756e-04 - val_mae: 0.0183\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.5727e-04 - mae: 0.0209 - val_loss: 6.1027e-04 - val_mae: 0.0183\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1789e-04 - mae: 0.0205 - val_loss: 4.8282e-04 - val_mae: 0.0159\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.4968e-04 - mae: 0.0207 - val_loss: 6.2155e-04 - val_mae: 0.0193\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.3651e-04 - mae: 0.0206 - val_loss: 4.9101e-04 - val_mae: 0.0164\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2479e-04 - mae: 0.0205 - val_loss: 5.3363e-04 - val_mae: 0.0177\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2365e-04 - mae: 0.0205 - val_loss: 4.9522e-04 - val_mae: 0.0165\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9751e-04 - mae: 0.0201 - val_loss: 5.0268e-04 - val_mae: 0.0173\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7028e-04 - mae: 0.0199 - val_loss: 5.0949e-04 - val_mae: 0.0169\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9885e-04 - mae: 0.0203 - val_loss: 5.5046e-04 - val_mae: 0.0178\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9663e-04 - mae: 0.0201 - val_loss: 4.7762e-04 - val_mae: 0.0162\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8949e-04 - mae: 0.0201 - val_loss: 5.1915e-04 - val_mae: 0.0173\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1346e-04 - mae: 0.0203 - val_loss: 5.6353e-04 - val_mae: 0.0180\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8592e-04 - mae: 0.0202 - val_loss: 4.8551e-04 - val_mae: 0.0159\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.1087e-04 - mae: 0.0204 - val_loss: 5.2328e-04 - val_mae: 0.0172\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8152e-04 - mae: 0.0199 - val_loss: 5.1955e-04 - val_mae: 0.0171\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7043e-04 - mae: 0.0200 - val_loss: 4.8279e-04 - val_mae: 0.0164\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9113e-04 - mae: 0.0201 - val_loss: 6.0441e-04 - val_mae: 0.0187\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8267e-04 - mae: 0.0200 - val_loss: 5.3228e-04 - val_mae: 0.0173\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8489e-04 - mae: 0.0200 - val_loss: 4.6516e-04 - val_mae: 0.0161\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.2217e-04 - mae: 0.0204 - val_loss: 5.1090e-04 - val_mae: 0.0174\n",
      "Epoch 174/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7475e-04 - mae: 0.0200 - val_loss: 5.5545e-04 - val_mae: 0.0179\n",
      "Epoch 175/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8380e-04 - mae: 0.0199 - val_loss: 5.3275e-04 - val_mae: 0.0172\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.9122e-04 - mae: 0.0200 - val_loss: 5.0088e-04 - val_mae: 0.0173\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8802e-04 - mae: 0.0199 - val_loss: 4.5936e-04 - val_mae: 0.0156\n",
      "Epoch 178/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8169e-04 - mae: 0.0200 - val_loss: 4.8299e-04 - val_mae: 0.0162\n",
      "Epoch 179/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7544e-04 - mae: 0.0199 - val_loss: 4.8265e-04 - val_mae: 0.0167\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.6379e-04 - mae: 0.0199 - val_loss: 5.1002e-04 - val_mae: 0.0173\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8607e-04 - mae: 0.0200 - val_loss: 5.3755e-04 - val_mae: 0.0173\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.5366e-04 - mae: 0.0196 - val_loss: 6.1328e-04 - val_mae: 0.0188\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8392e-04 - mae: 0.0199 - val_loss: 5.8162e-04 - val_mae: 0.0177\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.0141e-04 - mae: 0.0200 - val_loss: 4.9190e-04 - val_mae: 0.0164\n",
      "Epoch 185/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7495e-04 - mae: 0.0198 - val_loss: 4.4135e-04 - val_mae: 0.0156\n",
      "Epoch 186/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.5836e-04 - mae: 0.0195 - val_loss: 5.5903e-04 - val_mae: 0.0177\n",
      "Epoch 187/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.6309e-04 - mae: 0.0198 - val_loss: 6.0272e-04 - val_mae: 0.0191\n",
      "Epoch 188/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.4513e-04 - mae: 0.0195 - val_loss: 4.9822e-04 - val_mae: 0.0168\n",
      "Epoch 189/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7511e-04 - mae: 0.0197 - val_loss: 5.0211e-04 - val_mae: 0.0168\n",
      "Epoch 190/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.4107e-04 - mae: 0.0194 - val_loss: 5.5299e-04 - val_mae: 0.0186\n",
      "Epoch 191/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.6471e-04 - mae: 0.0196 - val_loss: 4.9453e-04 - val_mae: 0.0163\n",
      "Epoch 192/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.6762e-04 - mae: 0.0197 - val_loss: 4.7398e-04 - val_mae: 0.0161\n",
      "Epoch 193/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.8908e-04 - mae: 0.0201 - val_loss: 8.9701e-04 - val_mae: 0.0227\n",
      "Epoch 194/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 8.0021e-04 - mae: 0.0202 - val_loss: 5.7299e-04 - val_mae: 0.0183\n",
      "Epoch 195/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.3829e-04 - mae: 0.0193 - val_loss: 4.9209e-04 - val_mae: 0.0167\n",
      "Epoch 196/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.3725e-04 - mae: 0.0193 - val_loss: 5.6572e-04 - val_mae: 0.0176\n",
      "Epoch 197/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.5500e-04 - mae: 0.0196 - val_loss: 7.1731e-04 - val_mae: 0.0214\n",
      "Epoch 198/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.7683e-04 - mae: 0.0198 - val_loss: 5.1770e-04 - val_mae: 0.0176\n",
      "Epoch 199/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.6047e-04 - mae: 0.0196 - val_loss: 5.2973e-04 - val_mae: 0.0168\n",
      "Epoch 200/200\n",
      "245/245 [==============================] - 2s 7ms/step - loss: 7.2178e-04 - mae: 0.0192 - val_loss: 5.0018e-04 - val_mae: 0.0166\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error is: 6.021731217632003\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('LSTM')\n",
    "    os.chdir(os.path.join(dest,'LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_lstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_lstm.load_weights(filepath_attention)\n",
    "preds = atten_lstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "\n",
    "wk.save(f'LSTM Result.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_cnnlstm.hdf5'\n",
    "filepath_attention = 'attention_cnnlstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "simple_cnnlstm = keras.Sequential()\n",
    "simple_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "simple_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3))\n",
    "simple_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "simple_cnnlstm.add(keras.layers.Flatten())\n",
    "simple_cnnlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_cnnlstm.add(keras.layers.Dense(32))\n",
    "simple_cnnlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_cnnlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "The Mean Squared Error is: 7.532575327078259\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('CNN-LSTM')\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = simple_cnnlstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "simple_cnnlstm.load_weights(filepath_simple)\n",
    "preds = simple_cnnlstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention model\n",
    "\n",
    "K.clear_session()\n",
    "atten_cnnlstm = keras.Sequential()\n",
    "atten_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3, input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "atten_cnnlstm.add(keras.layers.Conv1D(64, kernel_size=3))\n",
    "atten_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_cnnlstm.add(keras.layers.LSTM(64, return_sequences=True))\n",
    "atten_cnnlstm.add(attention(return_sequences=True))\n",
    "atten_cnnlstm.add(keras.layers.Flatten())\n",
    "atten_cnnlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_cnnlstm.add(keras.layers.Dense(32))\n",
    "atten_cnnlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_cnnlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "The Mean Squared Error is: 6.202507857381984\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('CNN-LSTM')\n",
    "    os.chdir(os.path.join(dest,'CNN-LSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "# history = atten_cnnlstm.fit(x_train,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "# plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "atten_cnnlstm.load_weights(filepath_attention)\n",
    "preds = atten_cnnlstm.predict(x_test)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('CNN-LStM Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_convlstm.hdf5'\n",
    "filepath_attention = 'attention_convlstm.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_conv =x_train.reshape(x_train.shape[0], 1, 1, x_train.shape[1], x_train.shape[2])\n",
    "x_test_conv = x_test.reshape(x_test.shape[0], 1, 1, x_test.shape[1], x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "simple_convlstm = keras.Sequential()\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True, \n",
    "                                            input_shape=(x_train_conv.shape[1], x_train_conv.shape[2], \n",
    "                                                         x_train_conv.shape[3], x_train_conv.shape[4])))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "simple_convlstm.add(keras.layers.Flatten())\n",
    "simple_convlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "simple_convlstm.add(keras.layers.Dense(32))\n",
    "simple_convlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "simple_convlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new directory......\n",
      "New Directory Created\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 7s 28ms/step - loss: 0.0359 - mae: 0.1351 - val_loss: 0.0068 - val_mae: 0.0649\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 0.0081 - mae: 0.0669 - val_loss: 0.0038 - val_mae: 0.0475\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0060 - mae: 0.0571 - val_loss: 0.0029 - val_mae: 0.0415\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0050 - mae: 0.0521 - val_loss: 0.0027 - val_mae: 0.0400\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0047 - mae: 0.0501 - val_loss: 0.0025 - val_mae: 0.0383\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 0.0043 - mae: 0.0480 - val_loss: 0.0024 - val_mae: 0.0379\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0041 - mae: 0.0464 - val_loss: 0.0024 - val_mae: 0.0375\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 0.0037 - mae: 0.0438 - val_loss: 0.0021 - val_mae: 0.0343\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0035 - mae: 0.0417 - val_loss: 0.0019 - val_mae: 0.0329\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0033 - mae: 0.0404 - val_loss: 0.0017 - val_mae: 0.0312\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0031 - mae: 0.0393 - val_loss: 0.0017 - val_mae: 0.0306\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0029 - mae: 0.0377 - val_loss: 0.0017 - val_mae: 0.0321\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0027 - mae: 0.0365 - val_loss: 0.0015 - val_mae: 0.0285\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0026 - mae: 0.0351 - val_loss: 0.0014 - val_mae: 0.0278\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0025 - mae: 0.0344 - val_loss: 0.0013 - val_mae: 0.0277\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0023 - mae: 0.0333 - val_loss: 0.0015 - val_mae: 0.0300\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0022 - mae: 0.0328 - val_loss: 0.0012 - val_mae: 0.0256\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0021 - mae: 0.0312 - val_loss: 0.0012 - val_mae: 0.0266\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0020 - mae: 0.0306 - val_loss: 0.0011 - val_mae: 0.0248\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0020 - mae: 0.0304 - val_loss: 9.7103e-04 - val_mae: 0.0230\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0018 - mae: 0.0291 - val_loss: 9.3282e-04 - val_mae: 0.0224\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0018 - mae: 0.0292 - val_loss: 8.9993e-04 - val_mae: 0.0219\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0017 - mae: 0.0286 - val_loss: 8.9788e-04 - val_mae: 0.0219\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 0.0017 - mae: 0.0282 - val_loss: 9.4265e-04 - val_mae: 0.0229\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0016 - mae: 0.0277 - val_loss: 9.7108e-04 - val_mae: 0.0228\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0016 - mae: 0.0274 - val_loss: 8.5328e-04 - val_mae: 0.0214\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0015 - mae: 0.0268 - val_loss: 8.2489e-04 - val_mae: 0.0209\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0015 - mae: 0.0267 - val_loss: 7.8864e-04 - val_mae: 0.0204\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0015 - mae: 0.0270 - val_loss: 8.4589e-04 - val_mae: 0.0216\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0014 - mae: 0.0259 - val_loss: 8.3331e-04 - val_mae: 0.0216\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0015 - mae: 0.0263 - val_loss: 8.4253e-04 - val_mae: 0.0217\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0014 - mae: 0.0259 - val_loss: 7.7774e-04 - val_mae: 0.0203\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 7.9886e-04 - val_mae: 0.0208\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 7.8058e-04 - val_mae: 0.0206\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0013 - mae: 0.0250 - val_loss: 7.3790e-04 - val_mae: 0.0195\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0014 - mae: 0.0254 - val_loss: 7.4034e-04 - val_mae: 0.0199\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 7.2474e-04 - val_mae: 0.0195\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0013 - mae: 0.0247 - val_loss: 7.8823e-04 - val_mae: 0.0202\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0013 - mae: 0.0248 - val_loss: 7.4711e-04 - val_mae: 0.0202\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 6.8027e-04 - val_mae: 0.0186\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 7.4919e-04 - val_mae: 0.0201\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0013 - mae: 0.0242 - val_loss: 7.8344e-04 - val_mae: 0.0210\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 7.9717e-04 - val_mae: 0.0214\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 7.0696e-04 - val_mae: 0.0192\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 6.8987e-04 - val_mae: 0.0190\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 6.5796e-04 - val_mae: 0.0185\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 6.8658e-04 - val_mae: 0.0189\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 8.0791e-04 - val_mae: 0.0211\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 7.4875e-04 - val_mae: 0.0203\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 6.8356e-04 - val_mae: 0.0191\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0011 - mae: 0.0228 - val_loss: 6.4327e-04 - val_mae: 0.0182\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0011 - mae: 0.0228 - val_loss: 7.0123e-04 - val_mae: 0.0197\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 7.1814e-04 - val_mae: 0.0199\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0011 - mae: 0.0226 - val_loss: 6.4863e-04 - val_mae: 0.0185\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 6.6119e-04 - val_mae: 0.0190\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 6.2495e-04 - val_mae: 0.0181\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0011 - mae: 0.0223 - val_loss: 7.1048e-04 - val_mae: 0.0200\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 7.4653e-04 - val_mae: 0.0212\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0010 - mae: 0.0219 - val_loss: 6.4316e-04 - val_mae: 0.0190\n",
      "Epoch 60/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0010 - mae: 0.0221 - val_loss: 5.8857e-04 - val_mae: 0.0179\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0010 - mae: 0.0217 - val_loss: 5.7056e-04 - val_mae: 0.0176\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.9576e-04 - mae: 0.0216 - val_loss: 5.6395e-04 - val_mae: 0.0173\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.7050e-04 - mae: 0.0212 - val_loss: 6.2019e-04 - val_mae: 0.0189\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.6174e-04 - mae: 0.0212 - val_loss: 6.2695e-04 - val_mae: 0.0194\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.7295e-04 - mae: 0.0214 - val_loss: 5.7016e-04 - val_mae: 0.0176\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.4690e-04 - mae: 0.0210 - val_loss: 5.6547e-04 - val_mae: 0.0178\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.5009e-04 - mae: 0.0211 - val_loss: 5.5138e-04 - val_mae: 0.0174\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.3246e-04 - mae: 0.0209 - val_loss: 6.2335e-04 - val_mae: 0.0193\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.5876e-04 - mae: 0.0213 - val_loss: 5.1214e-04 - val_mae: 0.0167\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.3985e-04 - mae: 0.0209 - val_loss: 5.4548e-04 - val_mae: 0.0171\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.2274e-04 - mae: 0.0207 - val_loss: 5.9834e-04 - val_mae: 0.0188\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.1612e-04 - mae: 0.0206 - val_loss: 5.1277e-04 - val_mae: 0.0169\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.1972e-04 - mae: 0.0208 - val_loss: 6.2238e-04 - val_mae: 0.0196\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.1423e-04 - mae: 0.0206 - val_loss: 5.0992e-04 - val_mae: 0.0167\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.1163e-04 - mae: 0.0207 - val_loss: 5.5310e-04 - val_mae: 0.0177\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.0030e-04 - mae: 0.0205 - val_loss: 5.6597e-04 - val_mae: 0.0176\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.9678e-04 - mae: 0.0205 - val_loss: 5.1631e-04 - val_mae: 0.0173\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.9461e-04 - mae: 0.0204 - val_loss: 5.0820e-04 - val_mae: 0.0168\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.9154e-04 - mae: 0.0204 - val_loss: 5.1355e-04 - val_mae: 0.0172\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.9524e-04 - mae: 0.0204 - val_loss: 6.1367e-04 - val_mae: 0.0189\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.7655e-04 - mae: 0.0201 - val_loss: 5.0846e-04 - val_mae: 0.0171\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.9022e-04 - mae: 0.0204 - val_loss: 5.2058e-04 - val_mae: 0.0171\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.7558e-04 - mae: 0.0202 - val_loss: 5.5942e-04 - val_mae: 0.0182\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.8203e-04 - mae: 0.0203 - val_loss: 5.2677e-04 - val_mae: 0.0173\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.6094e-04 - mae: 0.0199 - val_loss: 6.4451e-04 - val_mae: 0.0201\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.8556e-04 - mae: 0.0204 - val_loss: 6.3512e-04 - val_mae: 0.0196\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.8646e-04 - mae: 0.0204 - val_loss: 4.7260e-04 - val_mae: 0.0162\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.5541e-04 - mae: 0.0200 - val_loss: 5.0978e-04 - val_mae: 0.0173\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.5470e-04 - mae: 0.0200 - val_loss: 5.8910e-04 - val_mae: 0.0187\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.5107e-04 - mae: 0.0199 - val_loss: 4.7848e-04 - val_mae: 0.0164\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.4543e-04 - mae: 0.0199 - val_loss: 5.2728e-04 - val_mae: 0.0175\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.3979e-04 - mae: 0.0198 - val_loss: 4.8832e-04 - val_mae: 0.0166\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.3262e-04 - mae: 0.0196 - val_loss: 4.7346e-04 - val_mae: 0.0162\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.3288e-04 - mae: 0.0198 - val_loss: 4.7008e-04 - val_mae: 0.0161\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.3718e-04 - mae: 0.0199 - val_loss: 5.4381e-04 - val_mae: 0.0180\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.3668e-04 - mae: 0.0199 - val_loss: 4.7131e-04 - val_mae: 0.0161\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 8.1525e-04 - mae: 0.0195 - val_loss: 4.5094e-04 - val_mae: 0.0156\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.3642e-04 - mae: 0.0199 - val_loss: 5.5711e-04 - val_mae: 0.0185\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.1262e-04 - mae: 0.0195 - val_loss: 4.8830e-04 - val_mae: 0.0166\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.0868e-04 - mae: 0.0194 - val_loss: 5.3268e-04 - val_mae: 0.0178\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.0421e-04 - mae: 0.0194 - val_loss: 5.2086e-04 - val_mae: 0.0173\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.0823e-04 - mae: 0.0195 - val_loss: 5.2599e-04 - val_mae: 0.0177\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.9730e-04 - mae: 0.0193 - val_loss: 5.6248e-04 - val_mae: 0.0180\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.9420e-04 - mae: 0.0193 - val_loss: 5.2387e-04 - val_mae: 0.0170\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.0213e-04 - mae: 0.0194 - val_loss: 4.4051e-04 - val_mae: 0.0155\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.9372e-04 - mae: 0.0193 - val_loss: 5.2250e-04 - val_mae: 0.0174\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 7.7905e-04 - mae: 0.0191 - val_loss: 4.7251e-04 - val_mae: 0.0161\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 7.9006e-04 - mae: 0.0193 - val_loss: 4.8167e-04 - val_mae: 0.0163\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.9080e-04 - mae: 0.0193 - val_loss: 4.9932e-04 - val_mae: 0.0170\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.7494e-04 - mae: 0.0191 - val_loss: 4.7662e-04 - val_mae: 0.0164\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.7681e-04 - mae: 0.0192 - val_loss: 5.0065e-04 - val_mae: 0.0170\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.5264e-04 - mae: 0.0188 - val_loss: 4.9321e-04 - val_mae: 0.0164\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.6744e-04 - mae: 0.0190 - val_loss: 4.7278e-04 - val_mae: 0.0165\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.5707e-04 - mae: 0.0189 - val_loss: 4.9451e-04 - val_mae: 0.0169\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.5455e-04 - mae: 0.0189 - val_loss: 5.1536e-04 - val_mae: 0.0170\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.4853e-04 - mae: 0.0187 - val_loss: 4.5463e-04 - val_mae: 0.0159\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 6s 26ms/step - loss: 7.4697e-04 - mae: 0.0187 - val_loss: 4.7457e-04 - val_mae: 0.0162\n",
      "Epoch 118/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.4423e-04 - mae: 0.0188 - val_loss: 4.8742e-04 - val_mae: 0.0167\n",
      "Epoch 119/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.2822e-04 - mae: 0.0184 - val_loss: 5.0125e-04 - val_mae: 0.0171\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.4187e-04 - mae: 0.0187 - val_loss: 4.6274e-04 - val_mae: 0.0159\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.1486e-04 - mae: 0.0182 - val_loss: 5.1415e-04 - val_mae: 0.0175\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.1060e-04 - mae: 0.0183 - val_loss: 4.4506e-04 - val_mae: 0.0157\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.2081e-04 - mae: 0.0184 - val_loss: 5.6579e-04 - val_mae: 0.0183\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.3078e-04 - mae: 0.0186 - val_loss: 4.7345e-04 - val_mae: 0.0161\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.2263e-04 - mae: 0.0184 - val_loss: 4.5284e-04 - val_mae: 0.0159\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.0222e-04 - mae: 0.0182 - val_loss: 4.3397e-04 - val_mae: 0.0155\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.9286e-04 - mae: 0.0180 - val_loss: 4.7946e-04 - val_mae: 0.0162\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.1663e-04 - mae: 0.0184 - val_loss: 4.5690e-04 - val_mae: 0.0160\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.8565e-04 - mae: 0.0179 - val_loss: 4.5092e-04 - val_mae: 0.0159\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.9759e-04 - mae: 0.0181 - val_loss: 4.6759e-04 - val_mae: 0.0163\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.9053e-04 - mae: 0.0180 - val_loss: 4.9909e-04 - val_mae: 0.0168\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.8155e-04 - mae: 0.0179 - val_loss: 4.6146e-04 - val_mae: 0.0158\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.7368e-04 - mae: 0.0178 - val_loss: 4.8487e-04 - val_mae: 0.0166\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.6432e-04 - mae: 0.0176 - val_loss: 4.4663e-04 - val_mae: 0.0156\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.0497e-04 - mae: 0.0184 - val_loss: 4.9574e-04 - val_mae: 0.0169\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.6189e-04 - mae: 0.0177 - val_loss: 4.5384e-04 - val_mae: 0.0159\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.6697e-04 - mae: 0.0178 - val_loss: 5.4247e-04 - val_mae: 0.0179\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 6.5597e-04 - mae: 0.0176 - val_loss: 4.4187e-04 - val_mae: 0.0155\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.4548e-04 - mae: 0.0175 - val_loss: 6.0588e-04 - val_mae: 0.0191\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.6376e-04 - mae: 0.0177 - val_loss: 5.0056e-04 - val_mae: 0.0164\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.5224e-04 - mae: 0.0176 - val_loss: 4.4429e-04 - val_mae: 0.0155\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.3800e-04 - mae: 0.0173 - val_loss: 4.7800e-04 - val_mae: 0.0166\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.4938e-04 - mae: 0.0176 - val_loss: 4.4700e-04 - val_mae: 0.0156\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.3730e-04 - mae: 0.0175 - val_loss: 5.0003e-04 - val_mae: 0.0169\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.2631e-04 - mae: 0.0172 - val_loss: 5.3681e-04 - val_mae: 0.0178\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.1423e-04 - mae: 0.0171 - val_loss: 4.4513e-04 - val_mae: 0.0155\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.2133e-04 - mae: 0.0172 - val_loss: 4.3176e-04 - val_mae: 0.0153\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.2206e-04 - mae: 0.0172 - val_loss: 4.5517e-04 - val_mae: 0.0156\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.2000e-04 - mae: 0.0172 - val_loss: 4.9212e-04 - val_mae: 0.0169\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.0626e-04 - mae: 0.0170 - val_loss: 5.1948e-04 - val_mae: 0.0170\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.0799e-04 - mae: 0.0170 - val_loss: 6.3051e-04 - val_mae: 0.0199\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.1002e-04 - mae: 0.0170 - val_loss: 4.7263e-04 - val_mae: 0.0164\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.0654e-04 - mae: 0.0170 - val_loss: 4.6998e-04 - val_mae: 0.0162\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.9320e-04 - mae: 0.0168 - val_loss: 4.9083e-04 - val_mae: 0.0168\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.0072e-04 - mae: 0.0169 - val_loss: 4.7911e-04 - val_mae: 0.0164\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 5.8685e-04 - mae: 0.0168 - val_loss: 4.7240e-04 - val_mae: 0.0160\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.7934e-04 - mae: 0.0166 - val_loss: 4.8703e-04 - val_mae: 0.0163\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.8349e-04 - mae: 0.0167 - val_loss: 4.3594e-04 - val_mae: 0.0155\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 5.7796e-04 - mae: 0.0167 - val_loss: 4.5358e-04 - val_mae: 0.0157\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.6913e-04 - mae: 0.0165 - val_loss: 5.1768e-04 - val_mae: 0.0174\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 5.7059e-04 - mae: 0.0165 - val_loss: 4.5667e-04 - val_mae: 0.0159\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.6990e-04 - mae: 0.0166 - val_loss: 4.3687e-04 - val_mae: 0.0154\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.6371e-04 - mae: 0.0164 - val_loss: 5.3658e-04 - val_mae: 0.0179\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.6671e-04 - mae: 0.0165 - val_loss: 4.6206e-04 - val_mae: 0.0160\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.6568e-04 - mae: 0.0166 - val_loss: 4.9720e-04 - val_mae: 0.0167\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.4895e-04 - mae: 0.0164 - val_loss: 4.5042e-04 - val_mae: 0.0158\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.4821e-04 - mae: 0.0162 - val_loss: 4.8937e-04 - val_mae: 0.0162\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.4672e-04 - mae: 0.0163 - val_loss: 4.5134e-04 - val_mae: 0.0157\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.4967e-04 - mae: 0.0162 - val_loss: 4.6137e-04 - val_mae: 0.0158\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.4039e-04 - mae: 0.0163 - val_loss: 5.0745e-04 - val_mae: 0.0174\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.2673e-04 - mae: 0.0159 - val_loss: 4.3502e-04 - val_mae: 0.0154\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.2874e-04 - mae: 0.0160 - val_loss: 4.6466e-04 - val_mae: 0.0163\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.2619e-04 - mae: 0.0160 - val_loss: 4.6828e-04 - val_mae: 0.0158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.2792e-04 - mae: 0.0160 - val_loss: 4.8338e-04 - val_mae: 0.0165\n",
      "Epoch 175/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.1806e-04 - mae: 0.0159 - val_loss: 5.7835e-04 - val_mae: 0.0184\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.1508e-04 - mae: 0.0159 - val_loss: 4.6533e-04 - val_mae: 0.0159\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.0047e-04 - mae: 0.0156 - val_loss: 5.3472e-04 - val_mae: 0.0174\n",
      "Epoch 178/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.9805e-04 - mae: 0.0157 - val_loss: 5.1514e-04 - val_mae: 0.0170\n",
      "Epoch 179/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.1483e-04 - mae: 0.0159 - val_loss: 5.7880e-04 - val_mae: 0.0188\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.9777e-04 - mae: 0.0156 - val_loss: 4.8466e-04 - val_mae: 0.0160\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.8861e-04 - mae: 0.0155 - val_loss: 4.5854e-04 - val_mae: 0.0158\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.8756e-04 - mae: 0.0155 - val_loss: 4.9151e-04 - val_mae: 0.0164\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.8079e-04 - mae: 0.0153 - val_loss: 4.8830e-04 - val_mae: 0.0164\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.8441e-04 - mae: 0.0154 - val_loss: 4.9412e-04 - val_mae: 0.0170\n",
      "Epoch 185/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.8211e-04 - mae: 0.0154 - val_loss: 5.1818e-04 - val_mae: 0.0171\n",
      "Epoch 186/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.8096e-04 - mae: 0.0154 - val_loss: 4.5703e-04 - val_mae: 0.0157\n",
      "Epoch 187/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.6536e-04 - mae: 0.0152 - val_loss: 4.9508e-04 - val_mae: 0.0167\n",
      "Epoch 188/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.7365e-04 - mae: 0.0153 - val_loss: 4.9914e-04 - val_mae: 0.0160\n",
      "Epoch 189/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.7992e-04 - mae: 0.0155 - val_loss: 4.4913e-04 - val_mae: 0.0158\n",
      "Epoch 190/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.6782e-04 - mae: 0.0152 - val_loss: 4.9380e-04 - val_mae: 0.0167\n",
      "Epoch 191/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.5908e-04 - mae: 0.0150 - val_loss: 4.8577e-04 - val_mae: 0.0161\n",
      "Epoch 192/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.4985e-04 - mae: 0.0149 - val_loss: 5.0691e-04 - val_mae: 0.0163\n",
      "Epoch 193/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.6468e-04 - mae: 0.0153 - val_loss: 4.7638e-04 - val_mae: 0.0160\n",
      "Epoch 194/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.5213e-04 - mae: 0.0150 - val_loss: 4.9790e-04 - val_mae: 0.0169\n",
      "Epoch 195/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.4543e-04 - mae: 0.0148 - val_loss: 5.1177e-04 - val_mae: 0.0165\n",
      "Epoch 196/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.4354e-04 - mae: 0.0149 - val_loss: 4.7645e-04 - val_mae: 0.0161\n",
      "Epoch 197/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.5615e-04 - mae: 0.0151 - val_loss: 4.8747e-04 - val_mae: 0.0163\n",
      "Epoch 198/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.5342e-04 - mae: 0.0151 - val_loss: 6.0849e-04 - val_mae: 0.0187\n",
      "Epoch 199/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.3414e-04 - mae: 0.0146 - val_loss: 4.6479e-04 - val_mae: 0.0160\n",
      "Epoch 200/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.2332e-04 - mae: 0.0145 - val_loss: 5.1963e-04 - val_mae: 0.0170\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAArqUlEQVR4nO3de3hV1Z3/8fc3JxAg4RoCIiAJFUEuyiVirVphvBSrI1WxhR+tMvrzbh3t41g7M7U+Vmf0N87YccbW0WJrHafo2NHiiNLR1lq1FQKCgIATESUgt8glXBJI8v39sXbgnOSEnEBusD+v59nP2WedtfdZe+dkf/d37Zu5OyIiEj9Z7d0AERFpHwoAIiIxpQAgIhJTCgAiIjGlACAiElPZ7d2A5ujbt68XFha2dzNERI4qixYt2uruBfXLj6oAUFhYSElJSXs3Q0TkqGJmn6QrVxeQiEhMKQCIiMSUAoCISEwdVccARKRt7N+/n7KyMiorK9u7KdIMXbp0YdCgQXTq1Cmj+goAItJAWVkZ3bt3p7CwEDNr7+ZIBtyd8vJyysrKKCoqymgadQGJSAOVlZXk5+dr438UMTPy8/OblbUpAIhIWtr4H32a+zeLRwD4l3+BOXPauxUiIh1KPALAY4/Br37V3q0QkQyVl5czduxYxo4dy3HHHcfAgQMPvN+3b98hpy0pKeHWW29t8ju+9KUvtUhb33jjDS6++OIWmVdbi8dB4KwsqKlp71aISIby8/NZsmQJAPfccw95eXnccccdBz6vrq4mOzv95qu4uJji4uImv+Odd95pkbYezTLKAMxsipmtNrNSM7srzec5ZvZs9Pm7ZlYYlU80syXRsNTMLk2aZq2ZLYs+a937OyQSUFvbql8hIq1r1qxZ3HDDDZx++unceeedLFiwgDPOOINx48bxpS99idWrVwOpe+T33HMPV199NZMmTWLo0KE88sgjB+aXl5d3oP6kSZOYNm0aI0aMYObMmdQ9KXHevHmMGDGCCRMmcOuttzZrT/+Xv/wlY8aMYfTo0Xz3u98FoKamhlmzZjF69GjGjBnDww8/DMAjjzzCyJEjOeWUU5g+ffqRr6wMNZkBmFkCeBQ4HygDFprZXHf/IKnaNcA2dz/RzKYDDwLfAJYDxe5ebWYDgKVm9pK7V0fTTXb3rS25QGkpAxA5fLfdBtHeeIsZOxZ+9KNmT1ZWVsY777xDIpFg586d/OEPfyA7O5vXXnuNv/7rv+ZXabp6V61axe9+9zsqKioYPnw4N954Y4Pz5N977z1WrFjB8ccfz5lnnsnbb79NcXEx119/PW+++SZFRUXMmDEj43Zu2LCB7373uyxatIjevXtzwQUX8OKLLzJ48GDWr1/P8uXLAdi+fTsADzzwAB9//DE5OTkHytpCJhnARKDU3de4+z5gDjC1Xp2pwFPR+PPAuWZm7r4naWPfBWifBxArAxA5JlxxxRUkEgkAduzYwRVXXMHo0aO5/fbbWbFiRdppLrroInJycujbty/9+vVj06ZNDepMnDiRQYMGkZWVxdixY1m7di2rVq1i6NChB86pb04AWLhwIZMmTaKgoIDs7GxmzpzJm2++ydChQ1mzZg3f/va3efXVV+nRowcAp5xyCjNnzuTf//3fG+3aag2ZfNNAYF3S+zLg9MbqRHv7O4B8YKuZnQ48CQwBvpUUEBz4jZk58G/u/ni6Lzez64DrAE444YSMFqoBZQAih+8w9tRbS25u7oHx73//+0yePJkXXniBtWvXMmnSpLTT5OTkHBhPJBJUV1cfVp2W0Lt3b5YuXcr8+fN57LHHeO6553jyySd5+eWXefPNN3nppZe4//77WbZsWZsEglY/C8jd33X3UcBpwPfMrEv00VnuPh64ELjZzL7cyPSPu3uxuxcXFDS4nXVmlAGIHHN27NjBwIEDAfj5z3/e4vMfPnw4a9asYe3atQA8++yzGU87ceJEfv/737N161Zqamr45S9/yTnnnMPWrVupra3l8ssv57777mPx4sXU1taybt06Jk+ezIMPPsiOHTvYtWtXiy9POpmEmPXA4KT3g6KydHXKzCwb6AmUJ1dw95VmtgsYDZS4+/qofLOZvUDoanrzsJaiKcoARI45d955J1dddRX33XcfF110UYvPv2vXrvz4xz9mypQp5ObmctpppzVa9/XXX2fQoEEH3v/nf/4nDzzwAJMnT8bdueiii5g6dSpLly7lL/7iL6iNdkj//u//npqaGr75zW+yY8cO3J1bb72VXr16tfjypGN1R7sbrRA26B8C5xI29AuB/+PuK5Lq3AyMcfcbooPAl7n7182sCFgXdQsNAf4InALsBbLcvcLMcoH/Ae5191cP1Zbi4mI/rAfCnHMOmMEbbzR/WpEYWrlyJSeffHJ7N6Pd7dq1i7y8PNydm2++mWHDhnH77be3d7MOKd3fzswWuXuDc2Ob7AKK+uxvAeYDK4Hn3H2Fmd1rZpdE1WYD+WZWCnwHqDtV9CzCmT9LgBeAm6KzfvoDb5nZUmAB8HJTG/8joi4gETkMTzzxBGPHjmXUqFHs2LGD66+/vr2b1KKazAA6ksPOAM47D/buhbffbvlGiRyDlAEcvVo0AzgmKAMQEWkgHgFAB4FFRBqIRwBQBiAi0kA8AoAyABGRBuIRAJQBiBxVJk+ezPz581PKfvSjH3HjjTc2Os2kSZOoO0nkq1/9atp76txzzz089NBDh/zuF198kQ8+OHirs7vvvpvXXnutGa1PryPeNjoeAUAZgMhRZcaMGcyp9xCnOXPmZHw/nnnz5h32xVT1A8C9997Leeedd1jz6ujiEQCUAYgcVaZNm8bLL7984OEva9euZcOGDZx99tnceOONFBcXM2rUKH7wgx+knb6wsJCtW8ONhu+//35OOukkzjrrrAO3jIZwjv9pp53GqaeeyuWXX86ePXt45513mDt3Ln/1V3/F2LFj+eijj5g1axbPP/88EK74HTduHGPGjOHqq6+mqqrqwPf94Ac/YPz48YwZM4ZVq1ZlvKztedtoPRBGRA6pPe4G3adPHyZOnMgrr7zC1KlTmTNnDl//+tcxM+6//3769OlDTU0N5557Lu+//z6nnHJK2vksWrSIOXPmsGTJEqqrqxk/fjwTJkwA4LLLLuPaa68F4G//9m+ZPXs23/72t7nkkku4+OKLmTZtWsq8KisrmTVrFq+//jonnXQSV155JT/5yU+47bbbAOjbty+LFy/mxz/+MQ899BA//elPm1wP7X3baGUAItIhJXcDJXf/PPfcc4wfP55x48axYsWKlO6a+v7whz9w6aWX0q1bN3r06MEll1xy4LPly5dz9tlnM2bMGJ555plGbyddZ/Xq1RQVFXHSSScBcNVVV/HmmwdvX3bZZZcBMGHChAM3kGtKe982WhmAiBxSe90NeurUqdx+++0sXryYPXv2MGHCBD7++GMeeughFi5cSO/evZk1axaVlZWHNf9Zs2bx4osvcuqpp/Lzn/+cN47wXmF1t5RuidtJt9Vto5UBiEiHlJeXx+TJk7n66qsP7P3v3LmT3NxcevbsyaZNm3jllVcOOY8vf/nLvPjii+zdu5eKigpeeumlA59VVFQwYMAA9u/fzzPPPHOgvHv37lRUVDSY1/Dhw1m7di2lpaUAPP3005xzzjlHtIztfdtoZQAi0mHNmDGDSy+99EBX0Kmnnsq4ceMYMWIEgwcP5swzzzzk9OPHj+cb3/gGp556Kv369Uu5pfMPf/hDTj/9dAoKCjj99NMPbPSnT5/OtddeyyOPPHLg4C9Aly5d+NnPfsYVV1xBdXU1p512GjfccEOzlqej3TY6HjeDu/ZamDcP1td/jIGIpKObwR29dDO4+hIJZQAiIvXEIwCoC0hEpIF4BAAdBBZptqOpe1iC5v7N4hEAlAGINEuXLl0oLy9XEDiKuDvl5eV06dIl42nicRaQMgCRZhk0aBBlZWVs2bKlvZsizdClS5eUs4yaEo8AoAxApFk6depEUVFRezdDWllGXUBmNsXMVptZqZndlebzHDN7Nvr8XTMrjMonmtmSaFhqZpdmOs8WpQxARKSBJgOAmSWAR4ELgZHADDMbWa/aNcA2dz8ReBh4MCpfDhS7+1hgCvBvZpad4TxbjjIAEZEGMskAJgKl7r7G3fcBc4Cp9epMBZ6Kxp8HzjUzc/c97l53U4wuQN0RpUzm2XKUAYiINJBJABgIrEt6XxaVpa0TbfB3APkAZna6ma0AlgE3RJ9nMk+i6a8zsxIzKznsA1LKAEREGmj100Dd/V13HwWcBnzPzDI/RylM/7i7F7t7cUFBweE1IpGom9nhTS8icgzKJACsBwYnvR8UlaWtY2bZQE+gPLmCu68EdgGjM5xny8mKFlNZgIjIAZkEgIXAMDMrMrPOwHRgbr06c4GrovFpwG/d3aNpsgHMbAgwAlib4TxbTl0GoOMAIiIHNHkdgLtXm9ktwHwgATzp7ivM7F6gxN3nArOBp82sFPicsEEHOAu4y8z2A7XATe6+FSDdPFt42Q5SBiAi0kBGF4K5+zxgXr2yu5PGK4Er0kz3NPB0pvNsNcoAREQaiMe9gOoCgDIAEZED4hEA6rqAlAGIiBwQjwCgDEBEpIF4BABlACIiDcQjACgDEBFpIB4BQKeBiog0EI8AoNNARUQaiEcAUAYgItJAPAKAMgARkQbiEQCUAYiINBCPAKAMQESkgXgEAGUAIiINxCMAKAMQEWkgHgFAGYCISAPxCADKAEREGohXAFAGICJyQDwCgG4GJyLSQDwCgDIAEZEG4hEAlAGIiDSQUQAwsylmttrMSs3srjSf55jZs9Hn75pZYVR+vpktMrNl0eufJU3zRjTPJdHQr8WWqj5lACIiDTT5UHgzSwCPAucDZcBCM5vr7h8kVbsG2ObuJ5rZdOBB4BvAVuDP3X2DmY0G5gMDk6ab6e4lLbQsjVMGICLSQCYZwESg1N3XuPs+YA4wtV6dqcBT0fjzwLlmZu7+nrtviMpXAF3NLKclGt4sygBERBrIJAAMBNYlvS8jdS8+pY67VwM7gPx6dS4HFrt7VVLZz6Lun++bmaX7cjO7zsxKzKxky5YtGTQ3DWUAIiINtMlBYDMbRegWuj6peKa7jwHOjoZvpZvW3R9392J3Ly4oKDi8BigDEBFpIJMAsB4YnPR+UFSWto6ZZQM9gfLo/SDgBeBKd/+obgJ3Xx+9VgD/Qehqah26FYSISAOZBICFwDAzKzKzzsB0YG69OnOBq6LxacBv3d3NrBfwMnCXu79dV9nMss2sbzTeCbgYWH5ES3IouhWEiEgDTQaAqE//FsIZPCuB59x9hZnda2aXRNVmA/lmVgp8B6g7VfQW4ETg7nqne+YA883sfWAJIYN4ogWXK5UyABGRBpo8DRTA3ecB8+qV3Z00XglckWa6+4D7GpnthMybeYSUAYiINBCvK4GVAYiIHBCPAKAMQESkgXgFAGUAIiIHxCMA6EIwEZEG4hEAlAGIiDQQjwCgDEBEpIF4BABlACIiDcQjACgDEBFpIB4BQBmAiEgD8QgAygBERBqIRwBQBiAi0kA8AoAyABGRBuIRAJQBiIg0EI8AoAxARKSBeAQAZQAiIg3EIwDodtAiIg3EIwDodtAiIg3EIwAoAxARaSAeAcAsDMoAREQOyCgAmNkUM1ttZqVmdleaz3PM7Nno83fNrDAqP9/MFpnZsuj1z5KmmRCVl5rZI2ZmLbZU6SQSygBERJI0GQDMLAE8ClwIjARmmNnIetWuAba5+4nAw8CDUflW4M/dfQxwFfB00jQ/Aa4FhkXDlCNYjqZlZSkDEBFJkkkGMBEodfc17r4PmANMrVdnKvBUNP48cK6Zmbu/5+4bovIVQNcoWxgA9HD3P7m7A78AvnakC3NIygBERFJkEgAGAuuS3pdFZWnruHs1sAPIr1fncmCxu1dF9cuamCcAZnadmZWYWcmWLVsyaG4jlAGIiKRok4PAZjaK0C10fXOndffH3b3Y3YsLCgoOvxHKAEREUmQSANYDg5PeD4rK0tYxs2ygJ1AevR8EvABc6e4fJdUf1MQ8W5YyABGRFJkEgIXAMDMrMrPOwHRgbr06cwkHeQGmAb91dzezXsDLwF3u/nZdZXf/DNhpZl+Mzv65Evj1kS1KE5QBiIikaDIARH36twDzgZXAc+6+wszuNbNLomqzgXwzKwW+A9SdKnoLcCJwt5ktiYZ+0Wc3AT8FSoGPgFdaaqHSUgYgIpIiO5NK7j4PmFev7O6k8UrgijTT3Qfc18g8S4DRzWnsEVEGICKSIh5XAoMyABGReuITAJQBiIikiE8AUAYgIpIiPgFAGYCISIr4BABlACIiKeITAJQBiIikUAAQEYmp+AQAdQGJiKSITwBQBiAikiI+AUAZgIhIivgEAGUAIiIp4hMAlAGIiKSITwBQBiAikiI+AUAZgIhIivgEAGUAIiIp4hMAlAGIiKSITwBQBiAikiI+AUAZgIhIivgEAGUAIiIpMgoAZjbFzFabWamZ3ZXm8xwzezb6/F0zK4zK883sd2a2y8z+td40b0TzrP+w+NahDEBEJEWTD4U3swTwKHA+UAYsNLO57v5BUrVrgG3ufqKZTQceBL4BVALfJzz8Pd0D4GdGD4dvfcoARERSZJIBTARK3X2Nu+8D5gBT69WZCjwVjT8PnGtm5u673f0tQiBoX4mEMgARkSSZBICBwLqk92VRWdo67l4N7ADyM5j3z6Lun++bmaWrYGbXmVmJmZVs2bIlg1k2IitLGYCISJL2PAg8093HAGdHw7fSVXL3x9292N2LCwoKDv/blAGIiKTIJACsBwYnvR8UlaWtY2bZQE+g/FAzdff10WsF8B+ErqbWowxARCRFJgFgITDMzIrMrDMwHZhbr85c4KpofBrwW3f3xmZoZtlm1jca7wRcDCxvbuObRQeBRURSNHkWkLtXm9ktwHwgATzp7ivM7F6gxN3nArOBp82sFPicECQAMLO1QA+gs5l9DbgA+ASYH238E8BrwBMtuWAN6DRQEZEUTQYAAHefB8yrV3Z30nglcEUj0xY2MtsJmTWxhSgDEBFJEZ8rgZUBiIikiE8AUAYgIpIiPgFAGYCISIr4BABlACIiKeITAJQBiIikiE8AUAYgIpIiPgFAGYCISIr4BABlACIiKeIVAJQBiIgcEJ8AoJvBiYikiE8AUAYgIpIiPgFAGYCISIr4BIBEAtzDICIiMQoAWdGiqhtIRASIUwBIJMKruoFERIA4BQBlACIiKeITAJQBiIikiE8AUAYgIpIiowBgZlPMbLWZlZrZXWk+zzGzZ6PP3zWzwqg838x+Z2a7zOxf600zwcyWRdM8YmbWIkvUGGUAIiIpmgwAZpYAHgUuBEYCM8xsZL1q1wDb3P1E4GHgwai8Evg+cEeaWf8EuBYYFg1TDmcBMqYMQEQkRSYZwESg1N3XuPs+YA4wtV6dqcBT0fjzwLlmZu6+293fIgSCA8xsANDD3f/k7g78AvjaESxH05QBiIikyCQADATWJb0vi8rS1nH3amAHkN/EPMuamGfLUgYgIpKiwx8ENrPrzKzEzEq2bNly+DNSBiAikiKTALAeGJz0flBUlraOmWUDPYHyJuY5qIl5AuDuj7t7sbsXFxQUZNDcRtQFAGUAIiJAZgFgITDMzIrMrDMwHZhbr85c4KpofBrw26hvPy13/wzYaWZfjM7+uRL4dbNb3xx1XUDKAEREAMhuqoK7V5vZLcB8IAE86e4rzOxeoMTd5wKzgafNrBT4nBAkADCztUAPoLOZfQ24wN0/AG4Cfg50BV6JhtajDEBEJEWTAQDA3ecB8+qV3Z00Xglc0ci0hY2UlwCjM23oEVMGICKSosMfBG4xygBERFLEJwAoAxARSRGfAKAMQEQkRXwCgDIAEZEU8QkAygBERFLEJwD06BFeyw91fZqISHzEJwAUFYXXjz9u33aIiHQQ8QkAgwZBdjasWdPeLRER6RDiEwCys2HIEGUAIiKR+AQAgKFDlQGIiETiFQCKihQAREQisQgA5eWwcSMhA9i6FSoq2rtJIiLtLhYB4Iwz4LbbCAEAdBxARISYBIDjjkvKAEDdQCIixC0A1F0LoAAgIhKzANC7N/TsqQAgIkKMAsCOHVBZZaEb6MMP27tJIiLtLjYBAGDTJsIR4XfegX372rVNIiLtLVYBYONG4LzzYPduePfddm2TiEh7yygAmNkUM1ttZqVmdleaz3PM7Nno83fNrDDps+9F5avN7CtJ5WvNbJmZLTGzkhZZmkakBIDJk8OzAf7nf1rzK0VEOrwmA4CZJYBHgQuBkcAMMxtZr9o1wDZ3PxF4GHgwmnYkMB0YBUwBfhzNr85kdx/r7sVHvCSHkBIAevWC006D115rza8UEenwMskAJgKl7r7G3fcBc4Cp9epMBZ6Kxp8HzjUzi8rnuHuVu38MlEbza1MFBeF148ao4PzzYcGCcGRYRCSmMgkAA4F1Se/LorK0ddy9GtgB5DcxrQO/MbNFZnZdY19uZteZWYmZlWzZsiWD5jbUqRP07ZsUAL761fBoyLvvPqz5iYgcC9rzIPBZ7j6e0LV0s5l9OV0ld3/c3Yvdvbigblf+MBy4FgAO3hvikUfgpz897HmKiBzNMgkA64HBSe8HRWVp65hZNtATKD/UtO5e97oZeIFW7hpKCQAA//AP8JWvwE03wZtvtuZXi4h0SJkEgIXAMDMrMrPOhIO6c+vVmQtcFY1PA37r7h6VT4/OEioChgELzCzXzLoDmFkucAGw/MgXp3ENAkB2NsyZEy4Mu/xy+OCD1vx6EZEOp8kAEPXp3wLMB1YCz7n7CjO718wuiarNBvLNrBT4DnBXNO0K4DngA+BV4GZ3rwH6A2+Z2VJgAfCyu7/asouWqi4AuCcV9uoFc+dCIgFf/CL893+3ZhNERDoU85QtYsdWXFzsJSWHd8nAP/4j3HEHbN8ebgeUoqwMvvY1WL4cFi6EMWOOtKkiIh2GmS1Kd7p9LK4EhoPXAnz2WZoPBw2CefNCRjBjBuzZ05ZNExFpF7EJACOjS9cWLGikQr9+8ItfwIoVMH067N/fZm0TEWkPsQkAY8eGLGDevENUuuACePRReOkl+PrX4fPP26p5IiJtLjYBwAwuvBB+8xuorj5ExZtugh/9KASBUaPgv/6rrZooItKmYhMAIASAbdsO0Q1U5y//MhwMPu64cIrotGn1ziEVETn6xSoAnH9+OOPzpZcyqDxuXIgUf/d34fTQkSPDMYKj6KwpEZFDiVUA6NUrdPM/+CDceSdUVjYxQadO8L3vwZIlcPLJcNVV4T5CeqSkiBwDYhUAAJ59Fq69NtwJYsKE8FiAffvCjn2jO/cjRoTbRfzzP8Nbb4VjA3fcAR991KZtFxFpSbELAN27w7/9G7zySrgb9AUXhLLsbPjSl8LDwtJKJODWW2HlSrjsshAMTjoJZs0KGYK6hkTkKBO7AFBnyhRYtQpeeCEc8/32t8NTIq+/volt+aBB8Mwz8MkncPvtIaUYNy7cU+i55xQIROSoEZtbQWTihz8Mjwj45jfhn/7p4INkDmnLlnCQ+F/+Bd57D045Jdxl9AtfgLPOCt1FIiLtqLFbQSgAJKmthXvugQceCN1CDz0E3/pW6B6qrQ2PEm5UdTU88US4w+g77xy82ODii0MQGDLkYEA45IxERFqWAkAzrFgRuoLefhvy8mDAAFi7NpwE9JOfhIBwSNXVsH49/Oxn4YEzW7aEI81w8JnEo0aFU0vrhl69wtVqIiItTAGgmWpr4de/htdfDzeQ69o1dP2ff37o+j/rLCgvh4cfhrPPDteKNco9RJC33grD4sXh+QPJN53r3Blyc0MQGD0aiovD/Yi6d4eBA1OHfv1CvT17wqmqnTsreIhIoxQAWsBjj8F3vws7d4b3ZmHbnkiEnp8BA+CEE2Dw4EPPBwgR5tNPQ7qxcmXIEnbvDs8qXrAglHfpArt2hbJkiUT44tra8L5Xr3CqqlmIVCecELqcuncP0/fvH65q7tQppC91Q58+odH5+WF+VVXhOxVMRI4pCgAtpKoqZAXvvw9794a7R8+cGXbqIWxXr74avvMdGD68Bb6wpgY2bw5dShs2hNf10RM5e/YMWcK6dfDhh+HYwu7d4Qylzz7L/IykTp1Ct5V7yCby80NwyM0NAaVbt4ND8vu68V69QiDp2jVMM3BgaFvXrjreIdIBKAC0oq1bw9mggweHm8098UTo8h81KpwdWlQEffuGHfZ168KFxOvXh+6kb30rnDBUtxO+Zk3YFnfuHILN4MHhOESz7dsXIlRubriP0aZNIZjU1IQv2L8/NHzjxhAsOnUKG+zt20P5tm2hi2nv3vBaf3zPnoaZSTo5OemDRt1rIhECXE0N9OgRspbu3cNCJxIhgJiFIZEIK6au26tTp1DWVkOXLiEwVlaGQJubG9qak3MYfyCRtqMA0IY2b4bZs8N1BR9/HIaKivDZcceFoNC798GrkCFsD7OzD9ark0iEAFFREcb79oVhw0KvTufOYfj4Y1i2DI4/PpyFOn586EFaujT0MtU9BW3q1HB4oX//cIprRQX8/vfw+OPhuydMCIceJkwIPUjLloWHpVVVhePWQ4aE9h5/fEhG5s+r4cOV1eRl7eWbZ63l5AHbQ//Yhg1h5umCRv2AUl0dGpOdHaatqAjDrl0hYtZ1dbmHILF//8GV1gy7yCWbarpQdUR/27TqAkRWVhiSxxsrS+6K69QpBMO+fcPyVVYe/OMmB7q6YJj82qNHOCYEYf3UrSv3UCc3N9TJzQ1XPu7fHzK0zp1D3drag8G/S5cwVFeHq9y7dQs/vpycg8vYrVuYx86dYZq8vPC6b1+YV11wVzdih6IA0I7qtl2QegbR5s3hTKNPPw1DVVW4pqxbt/D/lJ0dDg+sWhV6WWprw4786tVhB72qKgzHHReed7BxY9hoV1WFaUeNgsLCsNP6ySfwxhsHDxskO++8sI1YtCjUa468vLAdr6kJ7RgxIpTl5YVt+JIloa05OWHZjj8+bB9ycsITOEtLw4XVQ4ceXNbevUPAmTs3bGtmzgzbxPXrw3cVFTpFQ2rJqq3GvJYP/xde+O9OnFRUzSUX7KVmv/NpWRZbthr9++znN2914/cLuwFw+ujd3H9jGdTW8taSXBatzOWC4s85pXAnpWU59O9RycDee+iavZ8uWfv4rLwzb6/sQ1HfCs4YtI6elZv4749O5v0tA5g4YB3Ve/bx6eYumNeSsBqqq41PdvSiU1Y1w3tu4oJBH1Bb4/zxs0KG5G6lf852Kqo6U1HVGauppk9iBxVVndm5E7J2bCOrU4KsnE5Y9X52VuVQu7+G4sR75LGLnTW57KjOZWdtHlU12QzJWkfFTueTvQWM4z0+5CTmcglf5E9M4g368DmdqKaGLNYzkAQ19GAnuexmJz3YRH+2UEAfPqeALXxOH3qxnV5sZzXDyaGKPHbxKlPoTgVjWMYP+T67yONbPM0k3qAHO9lEf/Ipp5YsVjGCDxiFd87h4h5v0jexDWprqazpxIaa/qyvHcBaK+KX+y5j+f4RfKfXbK7Ie5VaS/B5zgBqsrLpWruHLl2gJqcbVZ27MzRvM10T+9hbm8PCncPZ4105LX8N+V338Nm+fF7eOIETemxnZP4mNlT2oSBvL/2772Htnn7kdalmYM9dVJHD2op8Nu7tyYQhW+iR5+yozmVPTQ57PYeNFXm8sKSIWrKYceanfLqtO+u355LbzVn6SU/27s/m0jO3cOqwPeR0zaKsvCt7q7NxS+BZCbbv7kRVTTbjR+8jq1sX1m3LY9/eGvr3qqJoQCVrN3dj5/6uZOflsH1PDlnmHNdjD3v3Jdi+pzMVexIU9DP69g2HA2fPDueN3HZb6B1YuRL+7/9t7pbnoCMKAGY2BfhnIAH81N0fqPd5DvALYAJQDnzD3ddGn30PuAaoAW519/mZzDOdozUAtKXKyrARPfHEhl1HO3eGjeimTSH4dO8e7mbxhS8crLN168FAMGbMwc/++Mfww+zcOWQF3buHSxwKC8O8fvWrkPF89FHoHdm9O+xYjh8fdlArKsJ1clu3ho343r2ha+yEE+DVV8OGvmfPEEDWrw/fMWpUmMeSJaENvXuHwFH/ztxmcMYZITCWlx8srwtCQ4bAlVeGHdjHHjs4vVn4/uYGvaZ06xZ2og8jUTliWVlObe3Bve8ePZx9VVBZ1TJ75Lmd99Oz2z42bM8FwMxxTz/vLKulT85uHKO8MvXHeHy3bQzN3cxbW5o+UJZFDXlZe6mo7YYn3bygq+2lyjtTS6JZy5Cgmk7sp5KuKeWd2Ifh7CO1S68bu8mmmp3Uf5h46+nGbvqylU8ZcqCsfN0e+gzqdljzO+wAYGYJ4EPgfKAMWAjMcPcPkurcBJzi7jeY2XTgUnf/hpmNBH4JTASOB14DToomO+Q801EAODbVZTP9+x88s2rDhpAtQNgT6ts3BB0IwWTDhoM9HX36hGmrqsKx8K5dw7TduoUA0K3bwWPRO3eGp8L16xe6y/r2DYFp48bQtbZpUxgqK8PQoweceWYIbHXZzOmnh7LFi8O8CwtDu2tqwvf06RMyrQ8/hJdfDmXnnBMC27ZtBw9x1NaGh8716BGCX11vV93QvXsIJCUlIUDW1evZMwSztWtDz87gwaFOnz7hdORFi8JJCp9/HgJidvbBExLqeth69gzrID8/1Nu6NUy/bVuYZsSIEMC2boVzzw2vCxaEY1YDBoTMdcGCsH6PPz5M4x4uaTn55NDD99JLYV26hzrJZzIPHx6W4Xe/O3hPxd69Q1srK8MOQiIR3q9aFbox8/PDDkX37mF5N24M45ddFtbtmjXhTi2bN4fvLSwMOyIbNoSerUEDnb69a3j7LWfv7lqO77efvK61dOlUQ49u1UyauIf9lTW88kZXhg2u5MTj91BRAYMLKqndX8PvS3JZs74zlXthcN+95Hbej9XWYF5Lzy5VZNVWs6i0J1n7qzih21ZyumaxfldPPi7vQWGv7eR32sn+vdX0ztpBdW0WG6v70i27it7Zu8iz3WzansPnu3PI61rDl4eW0T17Ly8uLaLLvp1M6LyM419+4rBPqjiSAHAGcI+7fyV6/z0Ad//7pDrzozp/NLNsYCNQANyVXLeuXjTZIeeZjgKAiEjzNRYAMgknA4F1Se/LorK0ddy9GtgB5B9i2kzmWdfw68ysxMxKtmzZkkFzRUQkEx3+JG13f9zdi929uCCju7OJiEgmMgkA64Hka1sHRWVp60RdQD0JB4MbmzaTeYqISCvKJAAsBIaZWZGZdQamA3Pr1ZkLXBWNTwN+6+HgwlxgupnlmFkRMAxYkOE8RUSkFTV1X0vcvdrMbgHmE07ZfNLdV5jZvUCJu88FZgNPm1kp8Dlhg05U7zngA6AauNndawDSzbPlF09ERBqjC8FERI5xR3IWkIiIHIMUAEREYuqo6gIysy3A4V643xfY2oLNaSlqV/N11LapXc3TUdsFHbdth9uuIe7e4Dz6oyoAHAkzK0nXB9be1K7m66htU7uap6O2Czpu21q6XeoCEhGJKQUAEZGYilMAeLy9G9AItav5Omrb1K7m6ajtgo7bthZtV2yOAYiISKo4ZQAiIpJEAUBEJKaO+QBgZlPMbLWZlZrZXe3clsFm9jsz+8DMVpjZX0bl95jZejNbEg1fbYe2rTWzZdH3l0Rlfczsf8zsf6PX3m3cpuFJ62SJme00s9vaa32Z2ZNmttnMlieVpV1HFjwS/e7eN7PxbdyufzCzVdF3v2BmvaLyQjPbm7TuHmvjdjX6tzOz70Xra7WZfaWN2/VsUpvWmtmSqLwt11dj24fW+425+zE7EG409xEwFOgMLAVGtmN7BgDjo/HuhMdijiQ8Je2Odl5Xa4G+9cr+H3BXNH4X8GA7/y03AkPaa30BXwbGA8ubWkfAV4FXAAO+CLzbxu26AMiOxh9Maldhcr12WF9p/3bR/8FSIAcoiv5vE23Vrnqf/yNwdzusr8a2D632GzvWM4CJQKm7r3H3fcAcYGp7NcbdP3P3xdF4BbCSRp6E1kFMBZ6Kxp8CvtZ+TeFc4CN3b+FHuGfO3d8k3O02WWPraCrwCw/+BPQyswFt1S53/42Hp/MB/InwzI021cj6asxUYI67V7n7x0Ap4f+3TdtlZgZ8nfAs8zZ1iO1Dq/3GjvUAkPGjJ9uamRUC44B3o6JbojTuybbuaok48BszW2Rm10Vl/d39s2h8I9C/HdpVZzqp/5Ttvb7qNLaOOtJv72rCnmKdIjN7z8x+b2Znt0N70v3tOsr6OhvY5O7/m1TW5uur3vah1X5jx3oA6JDMLA/4FXCbu+8EfgJ8ARgLfEZIQdvaWe4+HrgQuNnMvpz8oYecs13OGbbw0KBLgP+MijrC+mqgPddRY8zsbwjP4ngmKvoMOMHdxwHfAf7DzHq0YZM65N8uyQxSdzTafH2l2T4c0NK/sWM9AHS4R0+aWSfCH/cZd/8vAHff5O417l4LPEErpb6H4u7ro9fNwAtRGzbVpZTR6+a2blfkQmCxu2+K2tju6ytJY+uo3X97ZjYLuBiYGW04iLpYyqPxRYS+9pPaqk2H+Nt1hPWVDVwGPFtX1tbrK932gVb8jR3rAaBDPXoy6l+cDax0939KKk/ut7sUWF5/2lZuV66Zda8bJxxAXE7qoz6vAn7dlu1KkrJX1t7rq57G1tFc4MroTI0vAjuS0vhWZ2ZTgDuBS9x9T1J5gZklovGhhMe0rmnDdjX2t2vs8bFt6TxglbuX1RW05fpqbPtAa/7G2uLodnsOhCPlHxIi99+0c1vOIqRv7wNLouGrwNPAsqh8LjCgjds1lHAGxlJgRd16AvKB14H/BV4D+rTDOssFyoGeSWXtsr4IQegzYD+hv/WaxtYR4cyMR6Pf3TKguI3bVUroH677nT0W1b08+hsvARYDf97G7Wr0bwf8TbS+VgMXtmW7ovKfAzfUq9uW66ux7UOr/cZ0KwgRkZg61ruARESkEQoAIiIxpQAgIhJTCgAiIjGlACAiElMKACIiMaUAICISU/8fLHlX4DkGNv8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error is: 6.56104040864042\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('ConvLSTM')\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_convlstm.fit(x_train_conv,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_convlstm.load_weights(filepath_simple)\n",
    "preds = simple_convlstm.predict(x_test_conv)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "atten_convlstm = keras.Sequential()\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True, \n",
    "                                            input_shape=(x_train_conv.shape[1], x_train_conv.shape[2], \n",
    "                                                         x_train_conv.shape[3], x_train_conv.shape[4])))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.ConvLSTM2D(64, kernel_size=(1,3),return_sequences=True))\n",
    "atten_convlstm.add(attention(return_sequences=True))\n",
    "atten_convlstm.add(keras.layers.Flatten())\n",
    "atten_convlstm.add(keras.layers.Dense(512, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(128, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(64, activation='relu'))\n",
    "atten_convlstm.add(keras.layers.Dense(32))\n",
    "atten_convlstm.add(keras.layers.Dense(6))\n",
    "\n",
    "atten_convlstm.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 0.0326 - mae: 0.1290 - val_loss: 0.0057 - val_mae: 0.0584\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0080 - mae: 0.0666 - val_loss: 0.0034 - val_mae: 0.0441\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0055 - mae: 0.0541 - val_loss: 0.0027 - val_mae: 0.0398\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0048 - mae: 0.0507 - val_loss: 0.0025 - val_mae: 0.0382\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0044 - mae: 0.0482 - val_loss: 0.0025 - val_mae: 0.0391\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0041 - mae: 0.0466 - val_loss: 0.0022 - val_mae: 0.0360\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0038 - mae: 0.0446 - val_loss: 0.0021 - val_mae: 0.0348\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0036 - mae: 0.0432 - val_loss: 0.0018 - val_mae: 0.0325\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0034 - mae: 0.0418 - val_loss: 0.0017 - val_mae: 0.0317\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0031 - mae: 0.0393 - val_loss: 0.0017 - val_mae: 0.0325\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0015 - val_mae: 0.0292\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0027 - mae: 0.0363 - val_loss: 0.0015 - val_mae: 0.0298\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0025 - mae: 0.0353 - val_loss: 0.0013 - val_mae: 0.0265\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0024 - mae: 0.0337 - val_loss: 0.0013 - val_mae: 0.0271\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0022 - mae: 0.0324 - val_loss: 0.0012 - val_mae: 0.0253\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0021 - mae: 0.0316 - val_loss: 0.0012 - val_mae: 0.0260\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0020 - mae: 0.0310 - val_loss: 0.0011 - val_mae: 0.0257\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0019 - mae: 0.0304 - val_loss: 0.0010 - val_mae: 0.0237\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0018 - mae: 0.0297 - val_loss: 9.2023e-04 - val_mae: 0.0223\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0018 - mae: 0.0288 - val_loss: 9.3742e-04 - val_mae: 0.0227\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0017 - mae: 0.0285 - val_loss: 9.2923e-04 - val_mae: 0.0224\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0016 - mae: 0.0278 - val_loss: 0.0012 - val_mae: 0.0274\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0016 - mae: 0.0280 - val_loss: 0.0011 - val_mae: 0.0254\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0016 - mae: 0.0271 - val_loss: 8.6178e-04 - val_mae: 0.0219\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0015 - mae: 0.0269 - val_loss: 8.3456e-04 - val_mae: 0.0214\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 0.0015 - mae: 0.0266 - val_loss: 8.3588e-04 - val_mae: 0.0215\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0015 - mae: 0.0267 - val_loss: 8.7432e-04 - val_mae: 0.0222\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0014 - mae: 0.0260 - val_loss: 7.7831e-04 - val_mae: 0.0205\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0014 - mae: 0.0258 - val_loss: 7.7611e-04 - val_mae: 0.0202\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 0.0014 - mae: 0.0257 - val_loss: 7.4115e-04 - val_mae: 0.0199\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 7.2289e-04 - val_mae: 0.0195\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 7.9618e-04 - val_mae: 0.0205\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0014 - mae: 0.0252 - val_loss: 7.6315e-04 - val_mae: 0.0203\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0013 - mae: 0.0251 - val_loss: 8.5045e-04 - val_mae: 0.0210\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0013 - mae: 0.0248 - val_loss: 9.3731e-04 - val_mae: 0.0238\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0013 - mae: 0.0245 - val_loss: 6.9105e-04 - val_mae: 0.0186\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 9.2279e-04 - val_mae: 0.0238\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 7.9748e-04 - val_mae: 0.0211\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 6.7852e-04 - val_mae: 0.0188\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 6.9967e-04 - val_mae: 0.0195\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 7.0938e-04 - val_mae: 0.0189\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 7.1428e-04 - val_mae: 0.0193\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 7.8385e-04 - val_mae: 0.0213\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 7.8647e-04 - val_mae: 0.0214\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 7.5919e-04 - val_mae: 0.0212\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 6.1827e-04 - val_mae: 0.0181\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0011 - mae: 0.0227 - val_loss: 6.0419e-04 - val_mae: 0.0180\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0011 - mae: 0.0228 - val_loss: 6.4481e-04 - val_mae: 0.0189\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 0.0011 - mae: 0.0224 - val_loss: 5.6812e-04 - val_mae: 0.0175\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0011 - mae: 0.0222 - val_loss: 5.7399e-04 - val_mae: 0.0176\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 0.0010 - mae: 0.0216 - val_loss: 5.6685e-04 - val_mae: 0.0174\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 6s 27ms/step - loss: 0.0010 - mae: 0.0218 - val_loss: 5.5875e-04 - val_mae: 0.0176\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.8644e-04 - mae: 0.0214 - val_loss: 5.5667e-04 - val_mae: 0.0175\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 9.9877e-04 - mae: 0.0217 - val_loss: 5.2630e-04 - val_mae: 0.0167\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.7127e-04 - mae: 0.0211 - val_loss: 6.7033e-04 - val_mae: 0.0201\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.7702e-04 - mae: 0.0214 - val_loss: 5.4348e-04 - val_mae: 0.0173\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.7554e-04 - mae: 0.0214 - val_loss: 6.4742e-04 - val_mae: 0.0195\n",
      "Epoch 58/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.7651e-04 - mae: 0.0215 - val_loss: 5.4867e-04 - val_mae: 0.0175\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.4458e-04 - mae: 0.0210 - val_loss: 5.2263e-04 - val_mae: 0.0168\n",
      "Epoch 60/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.5222e-04 - mae: 0.0211 - val_loss: 5.3474e-04 - val_mae: 0.0170\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.3121e-04 - mae: 0.0208 - val_loss: 5.3627e-04 - val_mae: 0.0170\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.6253e-04 - mae: 0.0212 - val_loss: 5.3754e-04 - val_mae: 0.0174\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.3095e-04 - mae: 0.0208 - val_loss: 5.6163e-04 - val_mae: 0.0179\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.5181e-04 - mae: 0.0212 - val_loss: 5.0969e-04 - val_mae: 0.0166\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.2790e-04 - mae: 0.0208 - val_loss: 6.1294e-04 - val_mae: 0.0192\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.1785e-04 - mae: 0.0206 - val_loss: 4.7495e-04 - val_mae: 0.0160\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.2471e-04 - mae: 0.0208 - val_loss: 5.1171e-04 - val_mae: 0.0167\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.1832e-04 - mae: 0.0207 - val_loss: 6.0397e-04 - val_mae: 0.0189\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.2349e-04 - mae: 0.0209 - val_loss: 5.3581e-04 - val_mae: 0.0174\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 9.0997e-04 - mae: 0.0207 - val_loss: 5.2309e-04 - val_mae: 0.0172\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.9545e-04 - mae: 0.0204 - val_loss: 5.2494e-04 - val_mae: 0.0173\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.9830e-04 - mae: 0.0204 - val_loss: 5.8302e-04 - val_mae: 0.0183\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.1508e-04 - mae: 0.0207 - val_loss: 4.8888e-04 - val_mae: 0.0163\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 9.1314e-04 - mae: 0.0208 - val_loss: 5.3052e-04 - val_mae: 0.0171\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.9680e-04 - mae: 0.0205 - val_loss: 5.2408e-04 - val_mae: 0.0173\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.8458e-04 - mae: 0.0203 - val_loss: 5.2376e-04 - val_mae: 0.0172\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.6762e-04 - mae: 0.0200 - val_loss: 5.0125e-04 - val_mae: 0.0168\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.8985e-04 - mae: 0.0205 - val_loss: 4.7956e-04 - val_mae: 0.0159\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.7576e-04 - mae: 0.0202 - val_loss: 4.9114e-04 - val_mae: 0.0163\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.5898e-04 - mae: 0.0200 - val_loss: 5.0927e-04 - val_mae: 0.0168\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.6614e-04 - mae: 0.0202 - val_loss: 5.9557e-04 - val_mae: 0.0190\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.4387e-04 - mae: 0.0197 - val_loss: 4.8304e-04 - val_mae: 0.0160\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.6229e-04 - mae: 0.0202 - val_loss: 4.9969e-04 - val_mae: 0.0165\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.3867e-04 - mae: 0.0198 - val_loss: 4.7708e-04 - val_mae: 0.0160\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.4267e-04 - mae: 0.0199 - val_loss: 5.9122e-04 - val_mae: 0.0187\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.5597e-04 - mae: 0.0200 - val_loss: 5.8695e-04 - val_mae: 0.0182\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.2338e-04 - mae: 0.0196 - val_loss: 4.7023e-04 - val_mae: 0.0160\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 8.4020e-04 - mae: 0.0199 - val_loss: 5.6510e-04 - val_mae: 0.0181\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.1774e-04 - mae: 0.0195 - val_loss: 4.6679e-04 - val_mae: 0.0159\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.3471e-04 - mae: 0.0199 - val_loss: 4.4768e-04 - val_mae: 0.0156\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 7s 28ms/step - loss: 8.0986e-04 - mae: 0.0194 - val_loss: 4.8927e-04 - val_mae: 0.0163\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.3741e-04 - mae: 0.0199 - val_loss: 5.1366e-04 - val_mae: 0.0172\n",
      "Epoch 93/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.9524e-04 - mae: 0.0192 - val_loss: 4.9514e-04 - val_mae: 0.0167\n",
      "Epoch 94/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.1219e-04 - mae: 0.0196 - val_loss: 6.7556e-04 - val_mae: 0.0204\n",
      "Epoch 95/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 8.0291e-04 - mae: 0.0194 - val_loss: 4.5048e-04 - val_mae: 0.0156\n",
      "Epoch 96/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.1540e-04 - mae: 0.0196 - val_loss: 4.4543e-04 - val_mae: 0.0157\n",
      "Epoch 97/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.7609e-04 - mae: 0.0191 - val_loss: 4.7947e-04 - val_mae: 0.0162\n",
      "Epoch 98/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.8933e-04 - mae: 0.0192 - val_loss: 5.5730e-04 - val_mae: 0.0179\n",
      "Epoch 99/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.8144e-04 - mae: 0.0192 - val_loss: 6.3247e-04 - val_mae: 0.0195\n",
      "Epoch 100/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 8.0241e-04 - mae: 0.0195 - val_loss: 5.0135e-04 - val_mae: 0.0166\n",
      "Epoch 101/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.6401e-04 - mae: 0.0189 - val_loss: 5.2500e-04 - val_mae: 0.0176\n",
      "Epoch 102/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.6125e-04 - mae: 0.0189 - val_loss: 6.2419e-04 - val_mae: 0.0193\n",
      "Epoch 103/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.5954e-04 - mae: 0.0188 - val_loss: 4.7168e-04 - val_mae: 0.0159\n",
      "Epoch 104/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.6422e-04 - mae: 0.0190 - val_loss: 4.6647e-04 - val_mae: 0.0164\n",
      "Epoch 105/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.3792e-04 - mae: 0.0185 - val_loss: 4.5935e-04 - val_mae: 0.0159\n",
      "Epoch 106/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 7.2384e-04 - mae: 0.0183 - val_loss: 4.6811e-04 - val_mae: 0.0161\n",
      "Epoch 107/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.3115e-04 - mae: 0.0185 - val_loss: 4.7011e-04 - val_mae: 0.0161\n",
      "Epoch 108/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.2123e-04 - mae: 0.0183 - val_loss: 4.5706e-04 - val_mae: 0.0160\n",
      "Epoch 109/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.2124e-04 - mae: 0.0185 - val_loss: 5.1957e-04 - val_mae: 0.0173\n",
      "Epoch 110/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 7.2637e-04 - mae: 0.0185 - val_loss: 4.5893e-04 - val_mae: 0.0160\n",
      "Epoch 111/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.1421e-04 - mae: 0.0183 - val_loss: 5.4698e-04 - val_mae: 0.0183\n",
      "Epoch 112/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.2117e-04 - mae: 0.0185 - val_loss: 4.9232e-04 - val_mae: 0.0167\n",
      "Epoch 113/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.9850e-04 - mae: 0.0181 - val_loss: 4.2435e-04 - val_mae: 0.0153\n",
      "Epoch 114/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.0343e-04 - mae: 0.0182 - val_loss: 4.6111e-04 - val_mae: 0.0160\n",
      "Epoch 115/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.8551e-04 - mae: 0.0179 - val_loss: 4.7087e-04 - val_mae: 0.0163\n",
      "Epoch 116/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 7.0051e-04 - mae: 0.0182 - val_loss: 5.9973e-04 - val_mae: 0.0192\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 6s 25ms/step - loss: 6.8980e-04 - mae: 0.0181 - val_loss: 4.7038e-04 - val_mae: 0.0161\n",
      "Epoch 118/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.8534e-04 - mae: 0.0180 - val_loss: 6.3841e-04 - val_mae: 0.0204\n",
      "Epoch 119/200\n",
      "245/245 [==============================] - 7s 28ms/step - loss: 6.8823e-04 - mae: 0.0181 - val_loss: 4.8989e-04 - val_mae: 0.0166\n",
      "Epoch 120/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 6.6431e-04 - mae: 0.0176 - val_loss: 4.6032e-04 - val_mae: 0.0157\n",
      "Epoch 121/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.7272e-04 - mae: 0.0178 - val_loss: 4.5018e-04 - val_mae: 0.0160\n",
      "Epoch 122/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.6272e-04 - mae: 0.0176 - val_loss: 4.8111e-04 - val_mae: 0.0162\n",
      "Epoch 123/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.5517e-04 - mae: 0.0175 - val_loss: 5.1477e-04 - val_mae: 0.0173\n",
      "Epoch 124/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.4802e-04 - mae: 0.0174 - val_loss: 5.4740e-04 - val_mae: 0.0178\n",
      "Epoch 125/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.5170e-04 - mae: 0.0175 - val_loss: 4.9788e-04 - val_mae: 0.0168\n",
      "Epoch 126/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.5163e-04 - mae: 0.0176 - val_loss: 5.0530e-04 - val_mae: 0.0170\n",
      "Epoch 127/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.4331e-04 - mae: 0.0175 - val_loss: 5.0460e-04 - val_mae: 0.0173\n",
      "Epoch 128/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.4052e-04 - mae: 0.0174 - val_loss: 4.4542e-04 - val_mae: 0.0154\n",
      "Epoch 129/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.3514e-04 - mae: 0.0173 - val_loss: 5.4792e-04 - val_mae: 0.0181\n",
      "Epoch 130/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.3211e-04 - mae: 0.0173 - val_loss: 4.8996e-04 - val_mae: 0.0166\n",
      "Epoch 131/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.2384e-04 - mae: 0.0173 - val_loss: 5.4448e-04 - val_mae: 0.0176\n",
      "Epoch 132/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.2691e-04 - mae: 0.0173 - val_loss: 4.5307e-04 - val_mae: 0.0158\n",
      "Epoch 133/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.2857e-04 - mae: 0.0174 - val_loss: 5.0259e-04 - val_mae: 0.0171\n",
      "Epoch 134/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 6.1132e-04 - mae: 0.0170 - val_loss: 4.4367e-04 - val_mae: 0.0154\n",
      "Epoch 135/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 5.9686e-04 - mae: 0.0168 - val_loss: 4.7009e-04 - val_mae: 0.0161\n",
      "Epoch 136/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 5.9794e-04 - mae: 0.0168 - val_loss: 4.5136e-04 - val_mae: 0.0158\n",
      "Epoch 137/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 6.0582e-04 - mae: 0.0170 - val_loss: 4.4494e-04 - val_mae: 0.0156\n",
      "Epoch 138/200\n",
      "245/245 [==============================] - 7s 28ms/step - loss: 5.8960e-04 - mae: 0.0167 - val_loss: 5.4372e-04 - val_mae: 0.0177\n",
      "Epoch 139/200\n",
      "245/245 [==============================] - 7s 27ms/step - loss: 5.9071e-04 - mae: 0.0167 - val_loss: 5.0590e-04 - val_mae: 0.0173\n",
      "Epoch 140/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 5.8920e-04 - mae: 0.0167 - val_loss: 4.9398e-04 - val_mae: 0.0167\n",
      "Epoch 141/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 5.8971e-04 - mae: 0.0169 - val_loss: 5.2227e-04 - val_mae: 0.0174\n",
      "Epoch 142/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 5.9060e-04 - mae: 0.0168 - val_loss: 4.8195e-04 - val_mae: 0.0166\n",
      "Epoch 143/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.6374e-04 - mae: 0.0164 - val_loss: 4.5033e-04 - val_mae: 0.0156\n",
      "Epoch 144/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.7135e-04 - mae: 0.0165 - val_loss: 5.1166e-04 - val_mae: 0.0170\n",
      "Epoch 145/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.6377e-04 - mae: 0.0164 - val_loss: 4.3332e-04 - val_mae: 0.0153\n",
      "Epoch 146/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.7037e-04 - mae: 0.0166 - val_loss: 5.3184e-04 - val_mae: 0.0178\n",
      "Epoch 147/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.5679e-04 - mae: 0.0163 - val_loss: 5.2609e-04 - val_mae: 0.0172\n",
      "Epoch 148/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.5139e-04 - mae: 0.0162 - val_loss: 5.1860e-04 - val_mae: 0.0168\n",
      "Epoch 149/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.5298e-04 - mae: 0.0163 - val_loss: 4.4807e-04 - val_mae: 0.0156\n",
      "Epoch 150/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.4594e-04 - mae: 0.0162 - val_loss: 4.5089e-04 - val_mae: 0.0156\n",
      "Epoch 151/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.4853e-04 - mae: 0.0163 - val_loss: 4.4000e-04 - val_mae: 0.0154\n",
      "Epoch 152/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.3373e-04 - mae: 0.0160 - val_loss: 4.3353e-04 - val_mae: 0.0152\n",
      "Epoch 153/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.3606e-04 - mae: 0.0161 - val_loss: 4.8721e-04 - val_mae: 0.0167\n",
      "Epoch 154/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.2458e-04 - mae: 0.0160 - val_loss: 4.9092e-04 - val_mae: 0.0166\n",
      "Epoch 155/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.2455e-04 - mae: 0.0159 - val_loss: 4.5276e-04 - val_mae: 0.0155\n",
      "Epoch 156/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.1835e-04 - mae: 0.0159 - val_loss: 6.0008e-04 - val_mae: 0.0189\n",
      "Epoch 157/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.1357e-04 - mae: 0.0157 - val_loss: 4.6837e-04 - val_mae: 0.0162\n",
      "Epoch 158/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.1190e-04 - mae: 0.0157 - val_loss: 4.5089e-04 - val_mae: 0.0157\n",
      "Epoch 159/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 5.0393e-04 - mae: 0.0156 - val_loss: 4.5667e-04 - val_mae: 0.0160\n",
      "Epoch 160/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 4.9758e-04 - mae: 0.0155 - val_loss: 5.1892e-04 - val_mae: 0.0169\n",
      "Epoch 161/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 5.0930e-04 - mae: 0.0157 - val_loss: 4.9819e-04 - val_mae: 0.0169\n",
      "Epoch 162/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 5.0228e-04 - mae: 0.0157 - val_loss: 4.5625e-04 - val_mae: 0.0156\n",
      "Epoch 163/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.9981e-04 - mae: 0.0156 - val_loss: 4.7436e-04 - val_mae: 0.0165\n",
      "Epoch 164/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.9315e-04 - mae: 0.0155 - val_loss: 4.5190e-04 - val_mae: 0.0157\n",
      "Epoch 165/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.7621e-04 - mae: 0.0152 - val_loss: 4.8775e-04 - val_mae: 0.0168\n",
      "Epoch 166/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.7801e-04 - mae: 0.0153 - val_loss: 4.4530e-04 - val_mae: 0.0155\n",
      "Epoch 167/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 4.9349e-04 - mae: 0.0156 - val_loss: 4.7671e-04 - val_mae: 0.0164\n",
      "Epoch 168/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.7105e-04 - mae: 0.0152 - val_loss: 4.6859e-04 - val_mae: 0.0158\n",
      "Epoch 169/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.7605e-04 - mae: 0.0153 - val_loss: 4.4399e-04 - val_mae: 0.0155\n",
      "Epoch 170/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.6899e-04 - mae: 0.0151 - val_loss: 4.8576e-04 - val_mae: 0.0164\n",
      "Epoch 171/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.6514e-04 - mae: 0.0151 - val_loss: 5.3312e-04 - val_mae: 0.0178\n",
      "Epoch 172/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.5030e-04 - mae: 0.0148 - val_loss: 4.7934e-04 - val_mae: 0.0162\n",
      "Epoch 173/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.5177e-04 - mae: 0.0148 - val_loss: 4.9128e-04 - val_mae: 0.0166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.4509e-04 - mae: 0.0148 - val_loss: 5.5367e-04 - val_mae: 0.0172\n",
      "Epoch 175/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.4877e-04 - mae: 0.0149 - val_loss: 4.7291e-04 - val_mae: 0.0159\n",
      "Epoch 176/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.5330e-04 - mae: 0.0150 - val_loss: 4.8631e-04 - val_mae: 0.0163\n",
      "Epoch 177/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.4204e-04 - mae: 0.0148 - val_loss: 4.6683e-04 - val_mae: 0.0160\n",
      "Epoch 178/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.4076e-04 - mae: 0.0147 - val_loss: 5.6421e-04 - val_mae: 0.0178\n",
      "Epoch 179/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.3785e-04 - mae: 0.0147 - val_loss: 4.6669e-04 - val_mae: 0.0161\n",
      "Epoch 180/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.4167e-04 - mae: 0.0148 - val_loss: 4.9162e-04 - val_mae: 0.0163\n",
      "Epoch 181/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.2712e-04 - mae: 0.0146 - val_loss: 4.8756e-04 - val_mae: 0.0163\n",
      "Epoch 182/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.2367e-04 - mae: 0.0144 - val_loss: 4.3667e-04 - val_mae: 0.0152\n",
      "Epoch 183/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.2280e-04 - mae: 0.0145 - val_loss: 4.4143e-04 - val_mae: 0.0153\n",
      "Epoch 184/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.3815e-04 - mae: 0.0148 - val_loss: 5.3328e-04 - val_mae: 0.0171\n",
      "Epoch 185/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.2594e-04 - mae: 0.0146 - val_loss: 4.5149e-04 - val_mae: 0.0157\n",
      "Epoch 186/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.1636e-04 - mae: 0.0144 - val_loss: 4.6795e-04 - val_mae: 0.0157\n",
      "Epoch 187/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.2011e-04 - mae: 0.0145 - val_loss: 5.9474e-04 - val_mae: 0.0182\n",
      "Epoch 188/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.1457e-04 - mae: 0.0144 - val_loss: 4.5272e-04 - val_mae: 0.0156\n",
      "Epoch 189/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 4.0473e-04 - mae: 0.0142 - val_loss: 5.6018e-04 - val_mae: 0.0174\n",
      "Epoch 190/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.0350e-04 - mae: 0.0143 - val_loss: 5.6511e-04 - val_mae: 0.0179\n",
      "Epoch 191/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.0205e-04 - mae: 0.0142 - val_loss: 4.7271e-04 - val_mae: 0.0160\n",
      "Epoch 192/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.1097e-04 - mae: 0.0143 - val_loss: 7.0376e-04 - val_mae: 0.0206\n",
      "Epoch 193/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.0127e-04 - mae: 0.0142 - val_loss: 4.6802e-04 - val_mae: 0.0160\n",
      "Epoch 194/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 4.0199e-04 - mae: 0.0142 - val_loss: 5.3394e-04 - val_mae: 0.0168\n",
      "Epoch 195/200\n",
      "245/245 [==============================] - 6s 26ms/step - loss: 4.0075e-04 - mae: 0.0142 - val_loss: 5.0773e-04 - val_mae: 0.0166\n",
      "Epoch 196/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 3.8882e-04 - mae: 0.0139 - val_loss: 4.6688e-04 - val_mae: 0.0157\n",
      "Epoch 197/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 3.8991e-04 - mae: 0.0140 - val_loss: 5.5410e-04 - val_mae: 0.0169\n",
      "Epoch 198/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 3.8345e-04 - mae: 0.0139 - val_loss: 4.9449e-04 - val_mae: 0.0166\n",
      "Epoch 199/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 3.7536e-04 - mae: 0.0137 - val_loss: 4.6486e-04 - val_mae: 0.0157\n",
      "Epoch 200/200\n",
      "245/245 [==============================] - 6s 25ms/step - loss: 3.7578e-04 - mae: 0.0137 - val_loss: 5.4052e-04 - val_mae: 0.0172\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error is: 6.031831646131014\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('ConvLSTM')\n",
    "    os.chdir(os.path.join(dest,'ConvLSTM'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = atten_convlstm.fit(x_train_conv,y_train,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_attention])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "atten_convlstm.load_weights(filepath_attention)\n",
    "preds = atten_convlstm.predict(x_test_conv)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet2.write(0, 0, 'MSE')\n",
    "        sheet2.write(0, 1, 'Hours Ahead')\n",
    "        sheet2.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet2.write(i + 1, 1, i+1)\n",
    "wk.save('ConvLSTM Results.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the prelimaries \n",
    "\n",
    "filepath_simple = 'simple_seq2seq.hdf5'\n",
    "filepath_attention = 'attention_seq2seq.hdf5'\n",
    "\n",
    "checkpoint_simple = keras.callbacks.ModelCheckpoint(filepath_simple,monitor='val_loss',save_best_only=True)\n",
    "checkpoint_attention = keras.callbacks.ModelCheckpoint(filepath_attention, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "wk=Workbook()\n",
    "sheet1 = wk.add_sheet('Simple', cell_overwrite_ok=True)\n",
    "sheet2 = wk.add_sheet('Attention', cell_overwrite_ok=True)\n",
    "sheet3 = wk.add_sheet('Predictions', cell_overwrite_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_seq = y_train.reshape(y_train.shape[0], y_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm_7/PartitionedCall:1\", shape=(None, 6, 256), dtype=float32)\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 24, 2)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 24, 256)      265216      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 24, 256)      525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 24, 256)      525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 256), (None, 525312      lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 6, 256)       0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 6, 256)       525312      repeat_vector[0][0]              \n",
      "                                                                 lstm_3[0][0]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 6, 256)       525312      lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 6, 256)       525312      lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 6, 256)       525312      lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 6, 1)         257         lstm_7[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,942,657\n",
      "Trainable params: 3,942,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "input_train = keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "output_train = keras.layers.Input(shape=(y_train_seq.shape[1], y_train_seq.shape[2]))\n",
    "\n",
    "## Encoder Section##\n",
    "encoder_first = keras.layers.LSTM(256, return_sequences=True, return_state=False)(input_train)\n",
    "encoder_second = keras.layers.LSTM(256, return_sequences=True)(encoder_first)\n",
    "encoder_third = keras.layers.LSTM(256, return_sequences=True)(encoder_second)\n",
    "encoder_fourth, encoder_fourth_s1, encoder_fourth_s2 = keras.layers.LSTM(256,return_sequences=False, return_state=True)(encoder_third)\n",
    "\n",
    "##Decorder Section##\n",
    "decoder_first = keras.layers.RepeatVector(output_train.shape[1])(encoder_fourth)\n",
    "decoder_second = keras.layers.LSTM(256, return_state=False, return_sequences=True)(decoder_first,initial_state=[encoder_fourth,encoder_fourth_s2])\n",
    "decoder_third = keras.layers.LSTM(256,return_sequences=True)(decoder_second)\n",
    "decoder_fourth = keras.layers.LSTM(256,return_sequences=True)(decoder_third)\n",
    "decoder_fifth = keras.layers.LSTM(256,return_sequences=True)(decoder_fourth)\n",
    "print(decoder_fifth)\n",
    "\n",
    "##Output Section##\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(output_train.shape[2]))(decoder_fifth)\n",
    "\n",
    "simple_seq = keras.Model(inputs=input_train, outputs=output)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "simple_seq.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "simple_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory present\n",
      "Epoch 1/200\n",
      "245/245 [==============================] - 5s 21ms/step - loss: 0.0174 - mae: 0.0915 - val_loss: 0.0036 - val_mae: 0.0485\n",
      "Epoch 2/200\n",
      "245/245 [==============================] - 4s 16ms/step - loss: 0.0059 - mae: 0.0574 - val_loss: 0.0023 - val_mae: 0.0377\n",
      "Epoch 3/200\n",
      "245/245 [==============================] - 4s 16ms/step - loss: 0.0042 - mae: 0.0478 - val_loss: 0.0018 - val_mae: 0.0337\n",
      "Epoch 4/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0027 - mae: 0.0377 - val_loss: 8.9547e-04 - val_mae: 0.0235\n",
      "Epoch 5/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0019 - mae: 0.0304 - val_loss: 7.3106e-04 - val_mae: 0.0211\n",
      "Epoch 6/200\n",
      "245/245 [==============================] - 4s 14ms/step - loss: 0.0016 - mae: 0.0281 - val_loss: 0.0010 - val_mae: 0.0256\n",
      "Epoch 7/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0014 - mae: 0.0268 - val_loss: 5.8406e-04 - val_mae: 0.0183\n",
      "Epoch 8/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 7.5055e-04 - val_mae: 0.0206\n",
      "Epoch 9/200\n",
      "245/245 [==============================] - 4s 14ms/step - loss: 0.0013 - mae: 0.0249 - val_loss: 9.1506e-04 - val_mae: 0.0225\n",
      "Epoch 10/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0012 - mae: 0.0245 - val_loss: 7.9189e-04 - val_mae: 0.0226\n",
      "Epoch 11/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0012 - mae: 0.0243 - val_loss: 7.6822e-04 - val_mae: 0.0208\n",
      "Epoch 12/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 6.1112e-04 - val_mae: 0.0195\n",
      "Epoch 13/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0011 - mae: 0.0238 - val_loss: 6.1590e-04 - val_mae: 0.0184\n",
      "Epoch 14/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 8.7152e-04 - val_mae: 0.0231\n",
      "Epoch 15/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 5.7712e-04 - val_mae: 0.0185\n",
      "Epoch 16/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0010 - mae: 0.0224 - val_loss: 5.5422e-04 - val_mae: 0.0183\n",
      "Epoch 17/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 5.5413e-04 - val_mae: 0.0177\n",
      "Epoch 18/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0010 - mae: 0.0224 - val_loss: 4.3934e-04 - val_mae: 0.0157\n",
      "Epoch 19/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0011 - mae: 0.0235 - val_loss: 4.7577e-04 - val_mae: 0.0165\n",
      "Epoch 20/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0010 - mae: 0.0223 - val_loss: 4.9681e-04 - val_mae: 0.0166\n",
      "Epoch 21/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0010 - mae: 0.0221 - val_loss: 5.2351e-04 - val_mae: 0.0167\n",
      "Epoch 22/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 0.0010 - mae: 0.0220 - val_loss: 6.0384e-04 - val_mae: 0.0180\n",
      "Epoch 23/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 9.8438e-04 - mae: 0.0217 - val_loss: 5.7150e-04 - val_mae: 0.0175\n",
      "Epoch 24/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 9.7141e-04 - mae: 0.0215 - val_loss: 3.9819e-04 - val_mae: 0.0146\n",
      "Epoch 25/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 9.2780e-04 - mae: 0.0210 - val_loss: 5.3727e-04 - val_mae: 0.0175\n",
      "Epoch 26/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 9.4594e-04 - mae: 0.0212 - val_loss: 4.2477e-04 - val_mae: 0.0149\n",
      "Epoch 27/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 9.1745e-04 - mae: 0.0208 - val_loss: 4.2575e-04 - val_mae: 0.0153\n",
      "Epoch 28/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 9.6296e-04 - mae: 0.0217 - val_loss: 4.7957e-04 - val_mae: 0.0161\n",
      "Epoch 29/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 9.3274e-04 - mae: 0.0210 - val_loss: 5.1268e-04 - val_mae: 0.0169\n",
      "Epoch 30/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 8.5587e-04 - mae: 0.0202 - val_loss: 4.4566e-04 - val_mae: 0.0165\n",
      "Epoch 31/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 8.9716e-04 - mae: 0.0208 - val_loss: 3.9590e-04 - val_mae: 0.0148\n",
      "Epoch 32/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 8.6216e-04 - mae: 0.0203 - val_loss: 4.2376e-04 - val_mae: 0.0152\n",
      "Epoch 33/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 8.3980e-04 - mae: 0.0200 - val_loss: 4.7644e-04 - val_mae: 0.0168\n",
      "Epoch 34/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 8.0904e-04 - mae: 0.0196 - val_loss: 5.0633e-04 - val_mae: 0.0168\n",
      "Epoch 35/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 8.2421e-04 - mae: 0.0199 - val_loss: 4.9567e-04 - val_mae: 0.0167\n",
      "Epoch 36/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 8.2045e-04 - mae: 0.0199 - val_loss: 4.5564e-04 - val_mae: 0.0162\n",
      "Epoch 37/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 7.7204e-04 - mae: 0.0192 - val_loss: 4.2215e-04 - val_mae: 0.0153\n",
      "Epoch 38/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 7.5526e-04 - mae: 0.0190 - val_loss: 4.6515e-04 - val_mae: 0.0161\n",
      "Epoch 39/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 8.1194e-04 - mae: 0.0201 - val_loss: 4.0589e-04 - val_mae: 0.0145\n",
      "Epoch 40/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 7.1098e-04 - mae: 0.0185 - val_loss: 4.5458e-04 - val_mae: 0.0160\n",
      "Epoch 41/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 7.1928e-04 - mae: 0.0186 - val_loss: 4.2022e-04 - val_mae: 0.0153\n",
      "Epoch 42/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 7.0924e-04 - mae: 0.0187 - val_loss: 3.9210e-04 - val_mae: 0.0142\n",
      "Epoch 43/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 6.9789e-04 - mae: 0.0183 - val_loss: 5.5043e-04 - val_mae: 0.0178\n",
      "Epoch 44/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 7.1675e-04 - mae: 0.0188 - val_loss: 5.3596e-04 - val_mae: 0.0176\n",
      "Epoch 45/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 6.7912e-04 - mae: 0.0182 - val_loss: 4.1650e-04 - val_mae: 0.0147\n",
      "Epoch 46/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 6.7136e-04 - mae: 0.0181 - val_loss: 4.0596e-04 - val_mae: 0.0150\n",
      "Epoch 47/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 6.9680e-04 - mae: 0.0187 - val_loss: 4.3258e-04 - val_mae: 0.0156\n",
      "Epoch 48/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 6.1734e-04 - mae: 0.0173 - val_loss: 5.2516e-04 - val_mae: 0.0176\n",
      "Epoch 49/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 6.0135e-04 - mae: 0.0173 - val_loss: 4.3095e-04 - val_mae: 0.0152\n",
      "Epoch 50/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 5.6061e-04 - mae: 0.0167 - val_loss: 4.6290e-04 - val_mae: 0.0162\n",
      "Epoch 51/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 5.9247e-04 - mae: 0.0173 - val_loss: 5.1220e-04 - val_mae: 0.0168\n",
      "Epoch 52/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 5.8048e-04 - mae: 0.0171 - val_loss: 5.6652e-04 - val_mae: 0.0171\n",
      "Epoch 53/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 5.3798e-04 - mae: 0.0165 - val_loss: 5.9855e-04 - val_mae: 0.0178\n",
      "Epoch 54/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 5.6933e-04 - mae: 0.0170 - val_loss: 4.4152e-04 - val_mae: 0.0149\n",
      "Epoch 55/200\n",
      "245/245 [==============================] - 4s 16ms/step - loss: 5.2312e-04 - mae: 0.0163 - val_loss: 3.8331e-04 - val_mae: 0.0143\n",
      "Epoch 56/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 5.5669e-04 - mae: 0.0168 - val_loss: 3.7771e-04 - val_mae: 0.0138\n",
      "Epoch 57/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 5.1502e-04 - mae: 0.0164 - val_loss: 6.5543e-04 - val_mae: 0.0177\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245/245 [==============================] - 4s 15ms/step - loss: 4.9400e-04 - mae: 0.0160 - val_loss: 5.0106e-04 - val_mae: 0.0149\n",
      "Epoch 59/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 4.4798e-04 - mae: 0.0153 - val_loss: 5.1593e-04 - val_mae: 0.0155\n",
      "Epoch 60/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 4.3620e-04 - mae: 0.0150 - val_loss: 4.2631e-04 - val_mae: 0.0154\n",
      "Epoch 61/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 4.6057e-04 - mae: 0.0156 - val_loss: 5.2488e-04 - val_mae: 0.0159\n",
      "Epoch 62/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 4.1057e-04 - mae: 0.0147 - val_loss: 4.8558e-04 - val_mae: 0.0160\n",
      "Epoch 63/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 4.7242e-04 - mae: 0.0157 - val_loss: 5.2065e-04 - val_mae: 0.0154\n",
      "Epoch 64/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 4.5997e-04 - mae: 0.0156 - val_loss: 4.8114e-04 - val_mae: 0.0146\n",
      "Epoch 65/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 3.8581e-04 - mae: 0.0144 - val_loss: 5.4327e-04 - val_mae: 0.0164\n",
      "Epoch 66/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 3.6551e-04 - mae: 0.0139 - val_loss: 5.6666e-04 - val_mae: 0.0164\n",
      "Epoch 67/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 3.6461e-04 - mae: 0.0140 - val_loss: 5.4489e-04 - val_mae: 0.0168\n",
      "Epoch 68/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 3.4419e-04 - mae: 0.0136 - val_loss: 5.5349e-04 - val_mae: 0.0154\n",
      "Epoch 69/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 3.2457e-04 - mae: 0.0133 - val_loss: 5.0747e-04 - val_mae: 0.0150\n",
      "Epoch 70/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 3.1999e-04 - mae: 0.0132 - val_loss: 5.0535e-04 - val_mae: 0.0164\n",
      "Epoch 71/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 3.2062e-04 - mae: 0.0133 - val_loss: 4.9261e-04 - val_mae: 0.0154\n",
      "Epoch 72/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 2.9238e-04 - mae: 0.0127 - val_loss: 4.9902e-04 - val_mae: 0.0160\n",
      "Epoch 73/200\n",
      "245/245 [==============================] - 4s 14ms/step - loss: 3.6973e-04 - mae: 0.0139 - val_loss: 5.6278e-04 - val_mae: 0.0163\n",
      "Epoch 74/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 3.0709e-04 - mae: 0.0130 - val_loss: 4.2354e-04 - val_mae: 0.0152\n",
      "Epoch 75/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 3.3289e-04 - mae: 0.0134 - val_loss: 5.5190e-04 - val_mae: 0.0165\n",
      "Epoch 76/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 2.8151e-04 - mae: 0.0125 - val_loss: 5.1682e-04 - val_mae: 0.0163\n",
      "Epoch 77/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 2.4551e-04 - mae: 0.0116 - val_loss: 4.5566e-04 - val_mae: 0.0151\n",
      "Epoch 78/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 2.3008e-04 - mae: 0.0113 - val_loss: 5.0561e-04 - val_mae: 0.0158\n",
      "Epoch 79/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 2.4479e-04 - mae: 0.0117 - val_loss: 5.1708e-04 - val_mae: 0.0161\n",
      "Epoch 80/200\n",
      "245/245 [==============================] - 4s 14ms/step - loss: 2.1875e-04 - mae: 0.0110 - val_loss: 5.5334e-04 - val_mae: 0.0167\n",
      "Epoch 81/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 2.1205e-04 - mae: 0.0109 - val_loss: 6.3366e-04 - val_mae: 0.0198\n",
      "Epoch 82/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 2.1863e-04 - mae: 0.0111 - val_loss: 5.1283e-04 - val_mae: 0.0159\n",
      "Epoch 83/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 2.1098e-04 - mae: 0.0108 - val_loss: 5.5079e-04 - val_mae: 0.0171\n",
      "Epoch 84/200\n",
      "245/245 [==============================] - 4s 14ms/step - loss: 2.1352e-04 - mae: 0.0110 - val_loss: 4.7917e-04 - val_mae: 0.0156\n",
      "Epoch 85/200\n",
      "245/245 [==============================] - 4s 14ms/step - loss: 2.0644e-04 - mae: 0.0108 - val_loss: 4.9584e-04 - val_mae: 0.0161\n",
      "Epoch 86/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 1.9276e-04 - mae: 0.0105 - val_loss: 5.2906e-04 - val_mae: 0.0166\n",
      "Epoch 87/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 1.8123e-04 - mae: 0.0101 - val_loss: 6.7162e-04 - val_mae: 0.0196\n",
      "Epoch 88/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 1.7161e-04 - mae: 0.0098 - val_loss: 5.2696e-04 - val_mae: 0.0168\n",
      "Epoch 89/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 1.7401e-04 - mae: 0.0099 - val_loss: 5.2093e-04 - val_mae: 0.0163\n",
      "Epoch 90/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 1.7289e-04 - mae: 0.0099 - val_loss: 5.1968e-04 - val_mae: 0.0165\n",
      "Epoch 91/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 1.5760e-04 - mae: 0.0094 - val_loss: 5.0754e-04 - val_mae: 0.0160\n",
      "Epoch 92/200\n",
      "245/245 [==============================] - 4s 15ms/step - loss: 1.5109e-04 - mae: 0.0092 - val_loss: 4.9296e-04 - val_mae: 0.0163\n",
      "Epoch 93/200\n",
      " 13/245 [>.............................] - ETA: 3s - loss: 1.7282e-04 - mae: 0.0098"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('Directory present')\n",
    "except FileNotFoundError:\n",
    "    print('Creating a new directory......')\n",
    "    os.chdir(os.path.join(dest))\n",
    "    os.mkdir('Seq2Seq')\n",
    "    os.chdir(os.path.join(dest,'Seq2Seq'))\n",
    "    print('New Directory Created')\n",
    "\n",
    "history = simple_seq.fit(x_train,y_train_seq,validation_split=0.1,batch_size=32,epochs=200,callbacks=[checkpoint_simple])\n",
    "\n",
    "plt.plot(history.history['loss'],'r',label='Training Loss')\n",
    "plt.plot(history.history['val_loss'],'b',label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "simple_seq.load_weights(filepath_simple)\n",
    "preds = simple_seq.predict(x_test)\n",
    "\n",
    "preds = preds.reshape(preds.shape[0],preds.shape[1])\n",
    "print(preds.shape)\n",
    "\n",
    "y_test_unscaled = scaler.inverse_transform(y_test)\n",
    "y_pred_unscaled = scaler.inverse_transform(preds)\n",
    "\n",
    "e_mse = mse(y_test_unscaled[:,5],y_pred_unscaled[:,5])\n",
    "print(f'The Mean Squared Error is: {e_mse}')\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "        sheet1.write(0, 0, 'MSE')\n",
    "        sheet1.write(0, 1, 'Hours Ahead')\n",
    "        sheet1.write(i + 1, 0, mse(y_test_unscaled[:,i],y_pred_unscaled[:,i]))\n",
    "        sheet1.write(i + 1, 1, i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
